{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from env_1 import Env1\n",
    "from algos import *\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def trajectory_optimization(env, trajectories, traj_feats, reward_param):\n",
    "    \"\"\"\n",
    "    Simulating the planning phase in the trajectory space.\n",
    "\n",
    "    This function takes a list of the all possible traj_feats and a reward parameter, calculates the reward for each trajectory,\n",
    "    and returns a sorted list of tuples containing the reward and the trajectory index, sorted by reward in descending order.\n",
    "\n",
    "    Parameters:\n",
    "    traj_feats (list of np.ndarray): A list of traj_feats, where each trajectory is represented as a NumPy array of features.\n",
    "    reward_param (any): The parameter(s) used to calculate the reward for each trajectory.\n",
    "\n",
    "    Returns:\n",
    "    list of tuple: A list of tuples where each tuple contains:\n",
    "                   - curr_reward (float): The calculated reward for the trajectory.\n",
    "                   - traj_index (int): The index of the trajectory in the original list.\n",
    "                   The list is sorted by the reward in descending order.\n",
    "    \"\"\"\n",
    "    traj_index_with_reward = []  # List of tuples (reward, trajectory index)\n",
    "    #for traj_index, traj_feature in enumerate(traj_feats):\n",
    "    for traj, traj_feature in zip(trajectories, traj_feats):\n",
    "        curr_reward = env.reward(traj_feature, reward_param)\n",
    "        #traj_index_with_reward.append((curr_reward, traj_index))\n",
    "        traj_index_with_reward.append((curr_reward, traj_feature, traj))\n",
    "\n",
    "    return sorted(traj_index_with_reward, key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_position = np.array([0.4, 0.0, 0.25])\n",
    "goal_position = np.array([0.75, 0.0, 0.1])\n",
    "xi0 = np.linspace(start_position, goal_position, 3)\n",
    "\n",
    "true_reward_param = np.array([1, 0.1, -0.01])\n",
    "\n",
    "# Define the list of alpha values\n",
    "alphas = [0.99, 0.95, 0.90, 0.85, 0.75]\n",
    "\n",
    "n_runs = 2\n",
    "n_inner_samples = 10\n",
    "n_outer_samples = 5\n",
    "n_burn = 0.1\n",
    "delta = 0.5\n",
    "normalize = False # Normalize the mcmc samples\n",
    "#num_generated_demos = 50000\n",
    "num_generated_demos = 50\n",
    "\n",
    "num_demonstrations = 10\n",
    "beta = 1\n",
    "\n",
    "step_size = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avar_all_iterations_max_norm_map_policy = []\n",
    "true_avar_all_iterations_max_norm_map_policy = []\n",
    "\n",
    "\n",
    "avar_all_iterations_max_norm_mean_policy = []\n",
    "true_avar_all_iterations_max_norm_mean_policy = []\n",
    "\n",
    "env = Env1(visualize=False)\n",
    "\n",
    "demonstration_space, xi_star, trajectory_space, f_star = human_demo_2(env, xi0, true_reward_param,\\\n",
    "                                                                           n_samples=num_generated_demos, n_demos=20)\n",
    "\n",
    "#all_demonstrations = np.array(demonstration_space)[:100]\n",
    "all_demonstrations = np.array(demonstration_space)\n",
    "trajectory_space = np.array(trajectory_space)\n",
    "\n",
    "map_trajs = []\n",
    "mean_trajs = []\n",
    "\n",
    "for experiment_num in range(n_runs):\n",
    "    print(\"=================================================  Experiment Panda Push Branch 3.0.4 %d =================================================\" %experiment_num)\n",
    "\n",
    "    #avar_bounds_random_norm = {k: {i: [] for i in range(1, 16)} for k in alphas}\n",
    "\n",
    "    #true_avar_bounds_random_norm = {i: [] for i in range(1, 16)}\n",
    "\n",
    "    avar_bounds_max_norm_map_policy = {k: {i: [] for i in range(1, num_demonstrations+1)} for k in alphas}\n",
    "\n",
    "    true_avar_bounds_max_norm_map_policy = {i: [] for i in range(1, num_demonstrations+1)}\n",
    "\n",
    "\n",
    "    avar_bounds_max_norm_mean_policy = {k: {i: [] for i in range(1, 11)} for k in alphas}\n",
    "\n",
    "    true_avar_bounds_max_norm_mean_policy = {i: [] for i in range(1, 11)}\n",
    "\n",
    "\n",
    "    random_indices = np.random.choice(len(all_demonstrations), num_demonstrations, replace=False)\n",
    "    #trajectory_space = main_trajectory_space[random_indices]\n",
    "\n",
    "    demonstrations = all_demonstrations[random_indices]\n",
    "\n",
    "    for demonstration in range(num_demonstrations):\n",
    "\n",
    "        print(f\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Demo: {demonstration + 1} <<<<<<<<<<<<<<<<<<<<<<<<<<<<\\n\")\n",
    "        \n",
    "        print(\"Running double Metropolis-Hasting...\\n\")\n",
    "\n",
    "        #all_double_mcmc_sample, map_solution = mcmc_double(env, demonstrations[:demonstration + 1], n_outer_samples, n_inner_samples, n_burn, len(true_reward_param), beta=beta)\n",
    "        all_double_mcmc_sample, map_solution, accept_ratio = mcmc_double_2(env, demonstrations[:demonstration + 1], n_outer_samples, n_inner_samples,\\\n",
    "                                                                            n_burn, len(true_reward_param), beta=beta, step_size=step_size, normalize=normalize)\n",
    "        mean_solution = np.mean(all_double_mcmc_sample, axis=0)\n",
    "        \n",
    "        if map_solution is None:\n",
    "            map_solution = np.mean(all_double_mcmc_sample, axis=0)\n",
    "            print(\"MAP solution is not available\")\n",
    "        \n",
    "        print(\"Acceptance ration: \\n\", accept_ratio)\n",
    "\n",
    "        nevd_max_norm_map_policy = [] # normalizing expected regret with respect to the max for map policy\n",
    "        nevd_max_norm_mean_policy = [] # normalizing expected regret with respect to the max for mean policy\n",
    "\n",
    "\n",
    "        _, best_traj_feat_for_map, best_traj_for_map  = \\\n",
    "            trajectory_optimization(env, demonstration_space, trajectory_space, reward_param=map_solution)[0]\n",
    "\n",
    "\n",
    "        _, best_traj_feat_for_mean, best_traj_for_mean = \\\n",
    "            trajectory_optimization(env, demonstration_space, trajectory_space, reward_param=mean_solution)[0]\n",
    "        \n",
    "        map_trajs.append(best_traj_for_map)\n",
    "        mean_trajs.append(best_traj_for_mean)\n",
    "\n",
    "        \n",
    "        for i, mcmc_sample in enumerate(all_double_mcmc_sample):\n",
    "            best_traj_reward_for_current_sample, best_traj_for_current_sample, _ = \\\n",
    "                trajectory_optimization(env, demonstration_space, trajectory_space, reward_param=mcmc_sample)[0]\n",
    "            \n",
    "            map_policy_reward_under_current_mcmc_sample = env.reward(best_traj_feat_for_map, mcmc_sample)\n",
    "\n",
    "            mean_policy_reward_under_current_mcmc_sample = env.reward(best_traj_feat_for_mean, mcmc_sample)\n",
    "\n",
    "            nevd_max_norm_map_policy.append((best_traj_reward_for_current_sample - map_policy_reward_under_current_mcmc_sample) / (2))\n",
    "            nevd_max_norm_mean_policy.append((best_traj_reward_for_current_sample - mean_policy_reward_under_current_mcmc_sample) / (2))\n",
    "\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            N_burned = len(all_double_mcmc_sample)\n",
    "            k = math.ceil(N_burned * alpha + norm.ppf(1 - delta) * np.sqrt(N_burned * alpha * (1 - alpha)) - 0.5)\n",
    "            if k >= N_burned:\n",
    "                k = N_burned - 1\n",
    "            #normalized_expected_regrets_random_normalization.sort()\n",
    "            #normalized_expected_regrets_max_normalization.sort()\n",
    "            nevd_max_norm_map_policy.sort()\n",
    "            nevd_max_norm_mean_policy.sort()\n",
    "\n",
    "            avar_bounds_max_norm_map_policy[alpha][demonstration+1].append(nevd_max_norm_map_policy[k])\n",
    "            avar_bounds_max_norm_mean_policy[alpha][demonstration+1].append(nevd_max_norm_mean_policy[k])\n",
    "\n",
    "\n",
    "            print(f\"{alpha}-VaR-max-normalization for {demonstration + 1} demonstration MAP Policy: {nevd_max_norm_map_policy[k]:.6f}\\n\")\n",
    "            print(f\"{alpha}-VaR-max-normalization for {demonstration + 1} demonstration Mean Policy: {nevd_max_norm_mean_policy[k]:.6f}\\n\")\n",
    "\n",
    "\n",
    "        #map_traj_true_reward, _ = trajectory_optimization(trajectories=trajectory_space_main, reward_param=map_solution)[0]\n",
    "        map_traj_true_reward = env.reward(best_traj_feat_for_map, true_reward_param)\n",
    "        mean_traj_true_reward = env.reward(best_traj_feat_for_mean, true_reward_param)\n",
    "        \n",
    "\n",
    "        best_traj_true_reward = env.reward(f_star, true_reward_param)\n",
    "\n",
    "        #map_traj_true_reward = np.sum(calculate_reward_per_step(trajectory_space_main[traj_index_for_map], optimal_reward_function_param))\n",
    "        # Evaluate random policy under the true reward function\n",
    "        #random_policy_true_reward = np.sum(calculate_reward_per_step(random_traj, optimal_reward_function_param))\n",
    "\n",
    "        true_avar_max_norm_map_policy = ((best_traj_true_reward - map_traj_true_reward) / (2))\n",
    "        true_avar_bounds_max_norm_map_policy[demonstration + 1].append(true_avar_max_norm_map_policy)\n",
    "        print(f\"True nEVD for {demonstration + 1} demonstration MAP Policy: {true_avar_max_norm_map_policy:.6f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        true_avar_max_norm_mean_policy = ((best_traj_true_reward - mean_traj_true_reward) / (2))\n",
    "        true_avar_bounds_max_norm_mean_policy[demonstration + 1].append(true_avar_max_norm_mean_policy)\n",
    "        print(f\"True nEVD for {demonstration + 1} demonstration MEAN Policy: {true_avar_max_norm_mean_policy:.6f}\\n\")\n",
    "        \n",
    "    \n",
    "    avar_all_iterations_max_norm_map_policy.append(avar_bounds_max_norm_map_policy)\n",
    "    true_avar_all_iterations_max_norm_map_policy.append(true_avar_bounds_max_norm_map_policy)\n",
    "\n",
    "    avar_all_iterations_max_norm_mean_policy.append(avar_bounds_max_norm_mean_policy)\n",
    "    true_avar_all_iterations_max_norm_mean_policy.append(true_avar_bounds_max_norm_mean_policy)\n",
    "\n",
    "    mean_results_map = {k: {i: [] for i in range(1, 11)} for k in alphas}\n",
    "\n",
    "    true_mean_results_map = {1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[], 10:[]}\n",
    "    \n",
    "    mean_results_mean = {k: {i: [] for i in range(1, 11)} for k in alphas}\n",
    "\n",
    "    true_mean_results_mean = {1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[], 10:[]}\n",
    "    \n",
    "\n",
    "    for data in avar_all_iterations_max_norm_map_policy:\n",
    "        for alpha, val1 in data.items():\n",
    "            for item , val2 in val1.items():\n",
    "                mean_results_map[alpha][item].append(val2[0])\n",
    "    \n",
    "    for data in true_avar_all_iterations_max_norm_map_policy:\n",
    "        for key, values in data.items():\n",
    "            true_mean_results_map[key].append(values[0])\n",
    "\n",
    "    for data in avar_all_iterations_max_norm_mean_policy:\n",
    "        for alpha, val1 in data.items():\n",
    "            for item , val2 in val1.items():\n",
    "                mean_results_mean[alpha][item].append(val2[0])\n",
    "    \n",
    "    for data in true_avar_all_iterations_max_norm_mean_policy:\n",
    "        for key, values in data.items():\n",
    "            true_mean_results_mean[key].append(values[0])\n",
    "\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        print(f\"\\nMean results - MAP Policy -alpha={alpha}:\")\n",
    "        for i in range(1, 11):\n",
    "            print(f\"Demo:{i} Mean nEVD - Normalized by Max {alpha}: {np.nanmean(mean_results_map[alpha][i]):.6f}\")\n",
    "\n",
    "    print(f\"\\nTrue Mean result - MAP Policy\")\n",
    "    for i in range(1, 11):\n",
    "        print(f\"Demo:{i} True Mean nEVD: {np.nanmean(true_mean_results_map[i]):.6f}\")\n",
    "\n",
    "    for alpha in alphas:\n",
    "        print(f\"\\nMean results - MEAN Policy -alpha={alpha}:\")\n",
    "        for i in range(1, 11):\n",
    "            print(f\"Demo:{i} Mean nEVD - Normalized by Max: {np.nanmean(mean_results_mean[alpha][i]):.6f}\")\n",
    "\n",
    "    print(f\"\\nTrue Mean result - MEAN Policy\")\n",
    "    for i in range(1, 11):\n",
    "        print(f\"Demo:{i} True Mean nEVD: {np.nanmean(true_mean_results_mean[i]):.6f}\")\n",
    "    \n",
    "    if experiment_num%20 == 0:\n",
    "        np.save('map_trajectories.npy', np.array(map_trajs))\n",
    "        print(map_trajs)\n",
    "        np.save('mean_trajectories.npy', np.array(mean_trajs))\n",
    "        print(mean_trajs)\n",
    "        np.save('optimal_trajectory', xi_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
