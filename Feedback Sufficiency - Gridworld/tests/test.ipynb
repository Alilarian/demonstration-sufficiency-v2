{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1, 1),\n",
       " (-1, 1),\n",
       " (-1, 1),\n",
       " (-1, 1),\n",
       " (-1, 1),\n",
       " (-1, 1),\n",
       " (-1, 1),\n",
       " (-1, 1),\n",
       " (-1, 1),\n",
       " (-1, 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ctrl_bounds = [(-1,1)]* 10\n",
    "state_bounds = []\n",
    "feed_bounds = state_bounds + ctrl_bounds\n",
    "feed_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * upper_input_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_input_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 10\n",
    "lower_input_bound = [x[0] for x in feed_bounds]\n",
    "upper_input_bound = [x[1] for x in feed_bounds]\n",
    "inputs_set = np.random.uniform(low=2*lower_input_bound, high=2*upper_input_bound, size=(5, 2*z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "demos = [[(0, 1), (0, 1), (0, 1)], [(2, 1), (2, 1), (2, 1)], [(4, 3), (4, 3), (4, 3)], [(3, 3), (3, 3), (3, 3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "demos_shuffled = demos.copy()\n",
    "random.shuffle(demos_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#import mdp_utils\n",
    "#import mdp_worlds\n",
    "#import bayesian_irl\n",
    "#from mdp import FeatureMDP\n",
    "import copy\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Get the parent directory\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "# Load YAML file from the parent directory (assuming it's in a 'configs' folder within the parent directory)\n",
    "yaml_file_path = os.path.join(parent_directory, 'configs', 'gridworld_config.yaml')\n",
    "\n",
    "# Check if the YAML file exists and load it\n",
    "if os.path.exists(yaml_file_path):\n",
    "    with open(yaml_file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"YAML file not found at: {yaml_file_path}\")\n",
    "\n",
    "\n",
    "from env import gridworld_env, gridworld_env2\n",
    "from agent.q_learning_agent import ValueIteration\n",
    "from data_generation.generate_data import GridWorldMDPDataGenerator\n",
    "from reward_learning.birl import BIRL\n",
    "from utils.common_helper import (calculate_percentage_optimal_actions,\n",
    "                                 compute_policy_loss_avar_bound,\n",
    "                                 calculate_expected_value_difference)\n",
    "\n",
    "rseed = 1377\n",
    "random.seed(rseed)\n",
    "np.random.seed(rseed)\n",
    "\n",
    "\n",
    "render_mode = config['env_config']['render_mode']\n",
    "size = config['env_config']['size']\n",
    "noise_prob = config['env_config']['noise_prob']\n",
    "seed = config['env_config']['seed']\n",
    "gamma = config['env_config']['gamma']\n",
    "epsilon = float(config['algorithm_config']['epsilon'])\n",
    "\n",
    "num_steps = config['bayesian_irl_config']['num_steps']\n",
    "step_stdev = config['bayesian_irl_config']['step_stdev']\n",
    "beta = config['bayesian_irl_config']['beta']\n",
    "normalize = config['bayesian_irl_config']['normalize']\n",
    "adaptive = config['bayesian_irl_config']['adaptive']\n",
    "burn_frac = config['bayesian_irl_config']['burn_frac']\n",
    "skip_rate = config['bayesian_irl_config']['skip_rate']\n",
    "\n",
    "\n",
    "\n",
    "alphas = config['suff_config']['alphas']\n",
    "delta = config['suff_config']['delta']\n",
    "optimality_threshold = config['suff_config']['optimality_threshold']\n",
    "random_normalization = config['suff_config']['random_normalization']\n",
    "thresholds = config['suff_config']['thresholds']\n",
    "\n",
    "\n",
    "num_world = config['experiments']['num_world']\n",
    "num_demonstration = config['experiments']['num_demonstration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = [([(0,1), (3,3), (4,3), (5,None)], [(0,1), (3,0), (0,1), (3,0)]), ([(0,1), (3,3), (4,3), (5,None)], [(0,3), (1,3), (2,1), (5,None)])].copy()\n",
    "\n",
    "random.shuffle(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward_for_trajectory(env, trajectory, discount_factor=None):\n",
    "    \"\"\"\n",
    "    Computes the cumulative reward for a given trajectory in the environment. If a discount factor \n",
    "    is provided, the function calculates the discounted cumulative reward, where rewards received \n",
    "    later in the trajectory are given less weight.\n",
    "\n",
    "    :param env: The environment (MDP) which provides the reward function. This should have a \n",
    "                `compute_reward(state)` method.\n",
    "    :param trajectory: List of tuples (state, action), where `state` is the current state and \n",
    "                       `action` is the action taken in that state. The action is ignored in reward \n",
    "                       computation but kept for compatibility with the trajectory format.\n",
    "    :param discount_factor: (Optional) A float representing the discount factor (gamma) for \n",
    "                            future rewards. It should be between 0 and 1. If None, no discounting \n",
    "                            is applied, and rewards are summed without any decay.\n",
    "    :return: The cumulative reward for the trajectory, either discounted or non-discounted.\n",
    "             If discount_factor is provided, it applies a discount based on the time step of \n",
    "             the trajectory.\n",
    "    \"\"\"\n",
    "    cumulative_reward = 0\n",
    "    discount = 1 if discount_factor is None else discount_factor\n",
    "    \n",
    "    for t, (state, action) in enumerate(trajectory):\n",
    "        if state is None:  # Terminal state reached\n",
    "            break\n",
    "        \n",
    "        # Compute the reward for the current state\n",
    "        reward = env.compute_reward(state)\n",
    "        \n",
    "        # If a discount factor is provided, apply it to the reward\n",
    "        if discount_factor:\n",
    "            cumulative_reward += reward * (discount_factor ** t)\n",
    "        else:\n",
    "            cumulative_reward += reward\n",
    "\n",
    "    return cumulative_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = [(21, 1), (21, 0), (16, 3), (17, 3), (18, 1), (23, 3), (24, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_to_feature_map = {\n",
    "    \"red\": [1, 0, 0],\n",
    "    \"blue\": [0, 1, 0],\n",
    "    \"black\": [0, 0, 1]  # 'black' indicates a terminal state\n",
    "}\n",
    "\n",
    "custom_grid_features = [\n",
    "    [\"blue\", \"red\", \"blue\"],\n",
    "    [\"blue\", \"blue\", \"black\"]\n",
    "]\n",
    "\n",
    "# Initialize environments\n",
    "# Define your feature weights list\n",
    "feature_weights_list = [[-1, -0.3, 1]]\n",
    "\n",
    "# Initialize environments with feature weights\n",
    "env = gridworld_env2.NoisyLinearRewardFeaturizedGridWorldEnv(gamma=gamma,\n",
    "    color_to_feature_map=color_to_feature_map,\n",
    "    grid_features=custom_grid_features,\n",
    "    custom_feature_weights=feature_weights_list[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "([(0,1), (3,3), (4,3), (5,None)], [(0,3), (1,3), (2,1), (5,None)]) , ([(0,1), (3,3), (4,3), (5,None)], [(0,1), (3,0), (0,1), (3,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RIGHT = 3\n",
    "UP = 0\n",
    "LEFT = 2\n",
    "DOWN = 1\n",
    "\n",
    "traj = [(0,3), (1,3), (2,1), (5,None)]\n",
    "\n",
    "compute_reward_for_trajectory(env, traj, discount_factor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = [(0,1), (3,3), (4,3), (5,None)]\n",
    "\n",
    "compute_reward_for_trajectory(env, traj, discount_factor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = [(0,1), (3,0), (0,1), (3,0)]\n",
    "\n",
    "compute_reward_for_trajectory(env, traj, discount_factor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "class Trajectory:\n",
    "    def __init__(self, xi, T):\n",
    "        \"\"\"\n",
    "        Initialize the trajectory object.\n",
    "        \n",
    "        Args:\n",
    "            xi: A 2D array (n x m) where n is the number of waypoints and m is the number of joints.\n",
    "            T: The total time duration for the trajectory.\n",
    "        \"\"\"\n",
    "        self.T = T\n",
    "        self.n, self.m = np.shape(xi)\n",
    "        self.traj = []\n",
    "        xi = np.asarray(xi)\n",
    "        timesteps = np.linspace(0, self.T, self.n)\n",
    "        \n",
    "        # Use CubicSpline for smoother interpolation\n",
    "        for idx in range(self.m):\n",
    "            self.traj.append(CubicSpline(timesteps, xi[:, idx]))\n",
    "        \n",
    "    def get_waypoint(self, t):\n",
    "        \"\"\"\n",
    "        Get the waypoint at time t.\n",
    "        \n",
    "        Args:\n",
    "            t: The time at which to sample the trajectory.\n",
    "            \n",
    "        Returns:\n",
    "            A 1D array containing the joint positions at time t.\n",
    "        \"\"\"\n",
    "        if t < 0.0:\n",
    "            t = 0.0\n",
    "        if t > self.T:\n",
    "            t = self.T\n",
    "        \n",
    "        waypoint = np.array([0.] * self.m)\n",
    "        for idx in range(self.m):\n",
    "            waypoint[idx] = self.traj[idx](t)\n",
    "        \n",
    "        return waypoint\n",
    "\n",
    "# Connect to PyBullet\n",
    "p.connect(p.GUI)\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "# Load the Panda robot URDF\n",
    "robot_id = p.loadURDF(\"franka_panda/panda.urdf\", useFixedBase=True)\n",
    "\n",
    "# Set gravity for the simulation\n",
    "p.setGravity(0, 0, -9.81)\n",
    "\n",
    "# Number of joints (typically 7 for Panda arm)\n",
    "num_joints = 7\n",
    "\n",
    "# Replace this with your actual sequence of joint positions for each joint (each row should have 7 values)\n",
    "joint_positions_sequence = [\n",
    " [-2.75771,0.879738,-0.203636,0.867466,-0.5184,-0.02646,3.04725],\n",
    " [-3.0323,1.3127,-0.859796,0.333257,0.234699,1.03275,3.04725],\n",
    "[-2.75771,0.879738,-0.203636,0.867466,-0.518486,-0.0264612,3.04725],\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Create a Trajectory instance for generating waypoints\n",
    "T = 2.0  # Total duration in seconds\n",
    "trajectory = Trajectory(joint_positions_sequence, T)\n",
    "\n",
    "# Set up the camera for a zoomed-in view\n",
    "cameraDistance = 1.0  # Smaller values for closer zoom\n",
    "cameraYaw = 50        # Horizontal rotation of the camera\n",
    "cameraPitch = -30     # Vertical angle of the camera\n",
    "cameraTargetPosition = [0, 0, 0.5]  # Center point the camera looks at\n",
    "\n",
    "p.resetDebugVisualizerCamera(cameraDistance, cameraYaw, cameraPitch, cameraTargetPosition)\n",
    "\n",
    "# Simulation loop to visualize the joint movements\n",
    "dt = 0.5  # Time step for visualization\n",
    "current_time = 0.0\n",
    "\n",
    "while current_time <= T:\n",
    "    joint_positions = trajectory.get_waypoint(current_time)\n",
    "    for joint_index in range(num_joints):\n",
    "        p.setJointMotorControl2(\n",
    "            bodyIndex=robot_id,\n",
    "            jointIndex=joint_index,\n",
    "            controlMode=p.POSITION_CONTROL,\n",
    "            targetPosition=joint_positions[joint_index]\n",
    "        )\n",
    "    # Step the simulation\n",
    "    p.stepSimulation()\n",
    "    # Add a delay for visualization\n",
    "    time.sleep(dt)\n",
    "    current_time += dt\n",
    "\n",
    "# Keep the simulation window open until closed by the user\n",
    "input(\"Press Enter to exit...\")\n",
    "\n",
    "# Disconnect from PyBullet\n",
    "#p.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_estop(env, beta, trajs):\n",
    "    \"\"\"\n",
    "    Generates E-stops for random trajectories using the human likelihood model.\n",
    "    \n",
    "    :param beta: Sensitivity parameter for human decision-making.\n",
    "    :param num_trajs: Number of trajectories to generate.\n",
    "    :return: List of E-stop events (trajectories with stopping point t).\n",
    "    \"\"\"\n",
    "    # Generate random trajectories first\n",
    "\n",
    "    estop_trajectories = []\n",
    "\n",
    "    # Iterate over each trajectory\n",
    "    for trajectory in trajs:\n",
    "        T = len(trajectory)  # Length of the trajectory\n",
    "        stop_probs = []\n",
    "        \n",
    "        # Calculate stop probabilities for each possible stopping point t\n",
    "        for t in range(T):\n",
    "            # Sub-trajectory Î¾_0:t (use cumulative rewards up to point t)\n",
    "            reward_up_to_t = sum(env.compute_reward(s) for s, _ in trajectory[:t+1])\n",
    "            stop_prob_numerator = np.exp(beta * reward_up_to_t)\n",
    "            \n",
    "            # Compute denominator (normalization factor for the entire trajectory)\n",
    "            stop_prob_denominator = sum(np.exp(beta * sum(env.compute_reward(s) for s, _ in trajectory[:k+1])) for k in range(T))\n",
    "            \n",
    "            # Calculate stop probability for stopping at time t\n",
    "            stop_prob = stop_prob_numerator / stop_prob_denominator\n",
    "            stop_probs.append(stop_prob)\n",
    "        \n",
    "        # Select stopping point t based on the calculated probabilities\n",
    "        #stop_point = np.random.choice(range(T), p=stop_probs)\n",
    "        #print(stop_prob)\n",
    "        stop_point = np.argmax(stop_probs)\n",
    "        \n",
    "        # Append the trajectory with its stopping point to the result list\n",
    "        estop_trajectories.append((trajectory, stop_point))\n",
    "\n",
    "    return estop_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(3):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_estop(env, beta, trajs):\n",
    "    \"\"\"\n",
    "    Generates E-stops for random trajectories using the human likelihood model with discounted rewards.\n",
    "\n",
    "    :param env: The environment that provides rewards for states.\n",
    "    :param beta: Sensitivity parameter for human decision-making.\n",
    "    :param trajs: List of trajectories to generate E-stops for.\n",
    "    :return: List of E-stop events (trajectories with stopping point t).\n",
    "    \"\"\"\n",
    "    estop_trajectories = []\n",
    "    discount_factor = 1\n",
    "\n",
    "    # Iterate over each trajectory\n",
    "    for trajectory in trajs:\n",
    "        T = len(trajectory)  # Length of the trajectory\n",
    "        stop_probs = []\n",
    "\n",
    "        # Calculate stop probabilities for each possible stopping point t\n",
    "        for t in range(T):\n",
    "            # Calculate discounted cumulative reward up to point t\n",
    "            reward_up_to_t = sum(\n",
    "                (discount_factor ** i) * env.compute_reward(s) for i, (s, _) in enumerate(trajectory[:t+1])\n",
    "            )\n",
    "            stop_prob_numerator = np.exp(beta * reward_up_to_t)\n",
    "\n",
    "            # Compute denominator (normalization factor for the entire trajectory)\n",
    "            stop_prob_denominator = sum(\n",
    "                np.exp(beta * sum((discount_factor ** i) * env.compute_reward(s) for i, (s, _) in enumerate(trajectory[:k+1])))\n",
    "                for k in range(T)\n",
    "            )\n",
    "\n",
    "            # Calculate stop probability for stopping at time t\n",
    "            stop_prob = stop_prob_numerator / stop_prob_denominator\n",
    "            stop_probs.append(stop_prob)\n",
    "\n",
    "        # Select stopping point t with the highest stop probability\n",
    "        stop_point = np.argmax(stop_probs)\n",
    "\n",
    "        # Append the trajectory with its stopping point to the result list\n",
    "        estop_trajectories.append((trajectory, stop_point))\n",
    "\n",
    "    return estop_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.89087081  0.08908708  0.4454354 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "reward_param = [-1, 0.1, 0.5]\n",
    "reward_param = np.array(reward_param)/np.linalg.norm(reward_param)\n",
    "print(reward_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_to_feature_map = {\n",
    "    \"red\": [1, 0, 0],\n",
    "    \"blue\": [0, 1, 0],\n",
    "    \"black\": [0, 0, 1]  # 'black' indicates a terminal state\n",
    "}\n",
    "\n",
    "custom_grid_features = [\n",
    "    [\"blue\", \"red\", \"blue\"],\n",
    "    [\"blue\", \"blue\", \"black\"]\n",
    "]\n",
    "\n",
    "# Initialize environments\n",
    "# Define your feature weights list\n",
    "feature_weights_list = list(reward_param)\n",
    "# Initialize environments with feature weights\n",
    "env = gridworld_env2.NoisyLinearRewardFeaturizedGridWorldEnv(gamma=gamma,\n",
    "    color_to_feature_map=color_to_feature_map,\n",
    "    grid_features=custom_grid_features,\n",
    "    custom_feature_weights=feature_weights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0, 1), (3, 3), (4, 0), (1, 3), (2, 1), (5, None)], 2),\n",
       " ([(0, 1), (3, 3), (4, 0), (1, 1), (4, 3), (5, None)], 2),\n",
       " ([(0, 1), (3, 3), (4, 1), (4, 0), (1, 3), (2, 1), (5, None)], 3),\n",
       " ([(0, 1), (3, 3), (4, 3), (5, None)], 3),\n",
       " ([(3, 3), (4, 0), (1, 1), (4, 3), (5, None)], 1)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RIGHT = 3\n",
    "UP = 0\n",
    "LEFT = 2\n",
    "DOWN = 1\n",
    "\n",
    "generate_estop(env, 10, [\n",
    "    [(0,1), (3,3), (4,0), (1,3), (2,1) ,(5,None)],\n",
    "    [(0,1), (3,3), (4,0), (1,1), (4,3) ,(5,None)],\n",
    "    [(0,1), (3,3), (4,1), (4,0), (1,3), (2,1) ,(5,None)],\n",
    "    [(0,1), (3,3), (4,3), (5,None)],\n",
    "    [(3,3), (4,0), (1,1), (4,3) ,(5,None)],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
