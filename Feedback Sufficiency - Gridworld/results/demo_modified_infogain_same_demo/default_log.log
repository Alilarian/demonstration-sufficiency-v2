Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Same Demos: [(2, 1), (2, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [(2, 1), (4, 3), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4966
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.53389203  0.33154673  0.77784064]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4892
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.24468057 -0.48505476  0.83955542]
True reward weights: [-0.54665947 -0.41797193  0.72557763]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123742

Running PBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3012
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.70882167 -0.00349632  0.70537906]
True reward weights: [-0.85898087 -0.47493149  0.19129022]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.192984

Running experiment 2/50...
Shuffled Demos: [(2, 1), (4, 3), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4812
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.22000064 -0.94885333 -0.22644442]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.24687146 -0.32342944  0.91348119]
True reward weights: [-0.83425396 -0.01097193  0.55127121]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123772

Running PBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2936
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.89670999 -0.35281845  0.26726454]
True reward weights: [-0.19660284 -0.69008272  0.69651501]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.190578

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Shuffled Demos: [(0, 1), (2, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3008
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.60012655  0.19445944  0.77590828]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124145

Running PBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90553041 -0.17724182  0.38548674]
True reward weights: [-0.95958698  0.03667436  0.27901223]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.129086

Running PBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.59901411  0.01415988  0.80061326]
True reward weights: [-0.82470552 -0.51080285  0.24277821]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127910

Running experiment 4/50...
Shuffled Demos: [(2, 1), (0, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4846
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.02489125 -0.83988118  0.54219925]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.2942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.59971828  0.1484883   0.78631369]
True reward weights: [-0.79243115  0.14031549  0.59360293]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.170821

Running PBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.2948
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.27303184  0.45675945  0.84665484]
True reward weights: [-0.91303752 -0.0704301   0.40174879]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.153601

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Shuffled Demos: [(2, 1), (4, 3), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4782
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.01623019 -0.87783413  0.4786897 ]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4822
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.39857999  0.12966633  0.90792105]
True reward weights: [0.10463252 0.58506889 0.80420546]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123887

Running PBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2940
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.89983543 -0.03051984  0.43516059]
True reward weights: [-0.46067935 -0.01240676  0.88747992]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.190174

Running experiment 6/50...
Shuffled Demos: [(4, 3), (0, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.25544547 0.24342625 0.93567691]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.2952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.45756123  0.34034276  0.82146487]
True reward weights: [-0.7211465  -0.61310368 -0.3225703 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.167873

Running PBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.2898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.34199915  0.29394624  0.89254254]
True reward weights: [0.31069612 0.48819683 0.81555611]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151139

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Shuffled Demos: [(2, 1), (0, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4870
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.15748112 -0.78514452  0.59895558]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3060
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.52925063  0.16651669  0.83196512]
True reward weights: [-0.97184178  0.13075713  0.19602584]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.169011

Running PBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.2952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.8763822  -0.19148092  0.44191548]
True reward weights: [-0.81283829  0.1959728   0.54853311]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151758

Running experiment 8/50...
Shuffled Demos: [(0, 1), (2, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.2940
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.95358952 -0.19035861  0.23330373]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123866

Running PBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.2998
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.27727059  0.36684718  0.88800009]
True reward weights: [-0.85627065  0.14941785  0.49444401]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126436

Running PBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.2970
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.90176646 -0.16898366  0.39782129]
True reward weights: [-0.88122155 -0.44937746 -0.14665771]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.125539

Saving results to files...
Results saved successfully.

Running experiment 9/50...
Shuffled Demos: [(4, 3), (0, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.27427262 -0.50840025  0.81627429]
True reward weights: [-0.0505111  -0.01531811  0.99860602]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.2928
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.89581589 -0.17536307  0.40836465]
True reward weights: [-0.56414205 -0.56804557  0.59922281]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.168750

Running PBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.2942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.21974693  0.4618716   0.85929384]
True reward weights: [-0.57416028  0.25853534  0.77685227]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152661

Running experiment 10/50...
Shuffled Demos: [(0, 1), (2, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.2998
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.84628678 -0.12891597  0.51689396]
True reward weights: [-0.76518493 -0.64328377  0.02603879]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123839

Running PBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.2946
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.32957299  0.2647236   0.90625772]
True reward weights: [-0.73375074 -0.55441991  0.39271925]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.125526

Running PBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.2976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.79043469 -0.17897922  0.5858152 ]
True reward weights: [-0.03920133  0.68912566  0.72358073]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.126347

Saving results to files...
Results saved successfully.

Running experiment 11/50...
Shuffled Demos: [(2, 1), (4, 3), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4870
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.61122629 -0.44123487  0.65704963]
True reward weights: [-0.87829137 -0.24439244  0.41094598]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4756
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.19430147 -0.12490662  0.97295698]
True reward weights: [-0.88294225 -0.29520901  0.36505428]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123730

Running PBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3110
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.57917289  0.07384559  0.81185319]
True reward weights: [ 0.44799256 -0.65960401  0.60351074]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.188823

Running experiment 12/50...
Shuffled Demos: [(0, 1), (4, 3), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.2980
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.35854235  0.24434446  0.90096791]
True reward weights: [-0.8497926  -0.50850291  0.13884283]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124177

Running PBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3058
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.82223325  0.02675222  0.56852159]
True reward weights: [-0.93072045 -0.35262732  0.0970228 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.129090

Running PBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.2954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.93705072 -0.23732746  0.25614766]
True reward weights: [-0.7196645  -0.12201172  0.68351748]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127093

Saving results to files...
Results saved successfully.

Running experiment 13/50...
Shuffled Demos: [(4, 3), (2, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4754
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.89641181 -0.16129625  0.41283093]
True reward weights: [-0.34477193 -0.32328333  0.88126058]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4850
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.42384866 -0.53379516  0.7317206 ]
True reward weights: [ 0.18336749 -0.42420492  0.88680694]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123703

Running PBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.2968
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.91660037 -0.06823365  0.393939  ]
True reward weights: [-0.4144459  -0.80816875 -0.41844698]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.191474

Running experiment 14/50...
Shuffled Demos: [(2, 1), (4, 3), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4810
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.31256146 -0.93012705 -0.19279267]
True reward weights: [-0.43088031 -0.17122291  0.88601629]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4798
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.21873123 -0.57364277  0.78936089]
True reward weights: [-0.47128032  0.16709499  0.86601046]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123737

Running PBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.3048
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.83814703 -0.03235985  0.54448361]
True reward weights: [ 0.7136685  -0.22495807  0.66337858]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.190153

Saving results to files...
Results saved successfully.

Running experiment 15/50...
Shuffled Demos: [(0, 1), (2, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.2830
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.87735179 -0.31738458  0.35989007]
True reward weights: [-0.75433776 -0.02805593  0.65588674]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124424

Running PBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.2952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.35762961  0.42575139  0.83116594]
True reward weights: [-0.40153462  0.36639223  0.83936088]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.130743

Running PBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.2986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.82752086  0.06781182  0.55732466]
True reward weights: [-0.82024557 -0.53311336 -0.20733393]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.128468

Running experiment 16/50...
Shuffled Demos: [(2, 1), (0, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4818
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.90303395 -0.26967332  0.33437402]
True reward weights: [-0.92032338 -0.11683579  0.37330186]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.2928
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90230464 -0.2080747   0.37755961]
True reward weights: [0.48671328 0.18131342 0.85453825]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.169314

Running PBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.2922
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.06214571  0.4939805   0.8672492 ]
True reward weights: [-0.82657965 -0.41952902  0.37518193]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152641

Saving results to files...
Results saved successfully.

Running experiment 17/50...
Shuffled Demos: [(4, 3), (0, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.4898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.9227676   0.01507049  0.38506212]
True reward weights: [-0.76208548 -0.28664819  0.58056743]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.2854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.7659994  -0.02396895  0.64239427]
True reward weights: [ 0.28917225 -0.9083199   0.30221578]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.170882

Running PBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.3022
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.92329195  0.01909848  0.38362379]
True reward weights: [-0.46165325  0.41548523  0.78373994]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.153506

Running experiment 18/50...
Shuffled Demos: [(0, 1), (4, 3), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.2978
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.46958648  0.28579367  0.83535054]
True reward weights: [-0.39174849 -0.30341429  0.86860399]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124083

Running PBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.2994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.94105062 -0.15275395  0.30181113]
True reward weights: [-0.9010061  -0.35136759  0.25441858]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.128262

Running PBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.2958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.88184631 -0.21844165  0.41788794]
True reward weights: [-0.76438787 -0.21381796  0.60827055]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127148

Saving results to files...
Results saved successfully.

Running experiment 19/50...
Shuffled Demos: [(2, 1), (4, 3), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4846
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.03615634 -0.99508154  0.09222495]
True reward weights: [-0.92648321 -0.16748955  0.33701054]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4820
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.04822718 -0.47370136  0.87936407]
True reward weights: [-0.7252661  -0.57211807  0.38297518]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123712

Running PBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.2988
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.51835176  0.15684428  0.84066124]
True reward weights: [0.26338015 0.46060098 0.8476306 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.189358

Running experiment 20/50...
Shuffled Demos: [(4, 3), (2, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.4822
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.22342572 -0.95128016  0.21247824]
True reward weights: [-0.61012931 -0.3129562   0.72787406]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.4876
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.42227543 -0.90582895 -0.0340202 ]
True reward weights: [-0.80831688 -0.41754299  0.41506828]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123578

Running PBIRL with 3 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.3064
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.51299061  0.40509699  0.75679394]
True reward weights: [-0.47022652  0.19342007  0.86108983]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.190891

Saving results to files...
Results saved successfully.

Running experiment 21/50...
Shuffled Demos: [(0, 1), (4, 3), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.2876
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.87259494 -0.17709829  0.45520795]
True reward weights: [-0.3738588  -0.3262014   0.86822937]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123939

Running PBIRL with 2 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.2984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.96600877 -0.19168197  0.17345048]
True reward weights: [0.04694989 0.59836526 0.79984669]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.127681

Running PBIRL with 3 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.3056
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.957148   -0.10166279  0.27116853]
True reward weights: [-0.68780683  0.12664115  0.71476135]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.126009

Running experiment 22/50...
Shuffled Demos: [(4, 3), (0, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.4774
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.12572065 -0.54646815  0.82798966]
True reward weights: [-0.70965126 -0.6051058   0.36089066]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.2998
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.84487589 -0.11232548  0.52303701]
True reward weights: [ 0.19059754 -0.9374519   0.2913014 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.171695

Running PBIRL with 3 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.2992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.48262516  0.19846799  0.85304362]
True reward weights: [-0.04709904  0.31613859  0.94754318]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152983

Saving results to files...
Results saved successfully.

Running experiment 23/50...
Shuffled Demos: [(4, 3), (2, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.4766
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.02748738 -0.68733677  0.72581858]
True reward weights: [-0.74528522 -0.55533186  0.36899385]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.4834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.08791924 -0.98964911  0.11342328]
True reward weights: [-0.42747628 -0.82880234 -0.36104114]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124039

Running PBIRL with 3 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.2900
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.7228821   0.16341783  0.67136882]
True reward weights: [-0.66583297 -0.74431193 -0.05163541]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.189369

Running experiment 24/50...
Shuffled Demos: [(0, 1), (2, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.2922
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.6555666   0.25799899  0.70969638]
True reward weights: [-0.68144434 -0.29187893  0.6711485 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123958

Running PBIRL with 2 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.3044
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.68800288  0.24125355  0.68443317]
True reward weights: [0.25277846 0.39192762 0.88458792]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.127507

Running PBIRL with 3 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.2986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.65786521  0.19922263  0.72630828]
True reward weights: [-0.53853151  0.37178733  0.7561468 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.126460

Saving results to files...
Results saved successfully.

Running experiment 25/50...
Shuffled Demos: [(2, 1), (0, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.4804
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.44646705 -0.21100061  0.86956651]
True reward weights: [-0.44714936 -0.30119858  0.84222139]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.3006
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.44378376  0.36675501  0.81764707]
True reward weights: [-0.6822693  -0.63347045  0.36499835]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.167538

Running PBIRL with 3 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.2964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.65588024  0.20286553  0.72709469]
True reward weights: [-0.77628296 -0.29906273  0.55492905]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151166

Running experiment 26/50...
Shuffled Demos: [(2, 1), (0, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.4778
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.87107458 -0.25811857  0.4178563 ]
True reward weights: [-0.7333898  -0.47830978  0.48307263]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.3004
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.84165567 -0.1137626   0.52789564]
True reward weights: [-0.82970303  0.29374656  0.47466392]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.171292

Running PBIRL with 3 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.2972
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.9016667  -0.36070521  0.23851399]
True reward weights: [0.29962645 0.49848976 0.81346908]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.153615

Saving results to files...
Results saved successfully.

Running experiment 27/50...
Shuffled Demos: [(4, 3), (0, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.4832
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.50621804 -0.42658433  0.74951258]
True reward weights: [-0.65557811 -0.01374154  0.75500233]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.3106
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.9352099  -0.33295149  0.12052281]
True reward weights: [-0.48980638  0.58385731  0.64745683]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.169955

Running PBIRL with 3 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.2908
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.4766007   0.36096271  0.80159697]
True reward weights: [-0.92896137  0.09700309  0.35724105]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152725

Running experiment 28/50...
Shuffled Demos: [(2, 1), (4, 3), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.4888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.2762086  -0.79376893  0.54188532]
True reward weights: [-0.866301   -0.34967581  0.35672034]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.4828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.83478489 -0.46936415  0.28780458]
True reward weights: [-0.24955343  0.48041205  0.84078971]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123586

Running PBIRL with 3 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.2984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.91108107 -0.31755538  0.26284952]
True reward weights: [0.32275558 0.1391919  0.93619146]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.186500

Saving results to files...
Results saved successfully.

Running experiment 29/50...
Shuffled Demos: [(0, 1), (4, 3), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.2902
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.44974734  0.37731059  0.80954558]
True reward weights: [-0.6535586  -0.23072892  0.72085041]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124374

Running PBIRL with 2 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.2990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.70294716 -0.09543562  0.70481014]
True reward weights: [ 0.69307488  0.69529833 -0.19028255]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.129958

Running PBIRL with 3 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.2966
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.75907336  0.18885515  0.62300993]
True reward weights: [-0.85701532 -0.24827574  0.45153505]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.128594

Running experiment 30/50...
Shuffled Demos: [(0, 1), (4, 3), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.2954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.91956508 -0.08213733  0.38425711]
True reward weights: [-0.81326386 -0.04441834  0.5801973 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123932

Running PBIRL with 2 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.2998
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.70379481 -0.03790695  0.70939124]
True reward weights: [0.65096735 0.72500374 0.22496907]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.127093

Running PBIRL with 3 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.2912
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.39766519  0.19851475  0.89579813]
True reward weights: [-0.65484292 -0.07808058  0.75172081]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.126297

Saving results to files...
Results saved successfully.

Running experiment 31/50...
Shuffled Demos: [(2, 1), (0, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.4862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.84428243 -0.31227212  0.43551498]
True reward weights: [-0.27534761 -0.19938474  0.94044108]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.3034
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.92673235 -0.15659496  0.34153355]
True reward weights: [-0.98648415 -0.06135252  0.15193712]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.168984

Running PBIRL with 3 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.2994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.03700105 0.51351506 0.85728245]
True reward weights: [-0.93561375 -0.08794737  0.34189497]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152470

Running experiment 32/50...
Shuffled Demos: [(2, 1), (0, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.4864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.90664238 -0.25886229  0.33315149]
True reward weights: [-0.90753571 -0.40234227  0.1204144 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.2984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.82379154 -0.18706944  0.53513786]
True reward weights: [ 0.35398767 -0.92801639  0.11609614]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.169230

Running PBIRL with 3 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.2974
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.68059204  0.2347269   0.69404449]
True reward weights: [-0.74461029 -0.53831554 -0.39467948]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152451

Saving results to files...
Results saved successfully.

Running experiment 33/50...
Shuffled Demos: [(4, 3), (2, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.4738
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.97323584 -0.16913865  0.15557674]
True reward weights: [-0.32556312 -0.30396656  0.89532842]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.4808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.70353243 0.26972062 0.65748985]
True reward weights: [ 0.5220968  -0.71158455  0.4701727 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123660

Running PBIRL with 3 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.2906
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.62967305  0.11116774  0.76886513]
True reward weights: [-0.82995602  0.00934302  0.55775058]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.192662

Running experiment 34/50...
Shuffled Demos: [(2, 1), (0, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.4848
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.63574431 0.32946468 0.69805601]
True reward weights: [-0.839382   -0.393236    0.37523766]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.3090
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.74267435 -0.09139904  0.66338604]
True reward weights: [-0.82281732 -0.19368045  0.53428414]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.168946

Running PBIRL with 3 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.3076
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.18369362  0.44478547  0.87659714]
True reward weights: [-0.88031493 -0.45426221  0.13671675]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151534

Saving results to files...
Results saved successfully.

Running experiment 35/50...
Shuffled Demos: [(0, 1), (2, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.2992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.61651409  0.19772961  0.76211113]
True reward weights: [-0.94121751 -0.05112011  0.33391067]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123867

Running PBIRL with 2 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.2898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.20452208  0.45036768  0.8691028 ]
True reward weights: [-0.80875395  0.104684    0.57875583]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126561

Running PBIRL with 3 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.3030
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.94958972 -0.08925566  0.30052088]
True reward weights: [0.25638341 0.42524406 0.86800636]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.125483

Running experiment 36/50...
Shuffled Demos: [(4, 3), (2, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.4808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.32395331 -0.90586573 -0.2728764 ]
True reward weights: [-0.05456508 -0.040557    0.99768621]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.4810
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.01027688 0.28856272 0.95740584]
True reward weights: [-0.0083331   0.37920312  0.92527593]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123963

Running PBIRL with 3 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.3060
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.71165723  0.14196453  0.68803347]
True reward weights: [-0.30426333 -0.81110536  0.49953169]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.191012

Saving results to files...
Results saved successfully.

Running experiment 37/50...
Shuffled Demos: [(4, 3), (0, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.4810
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.32935737 -0.90766079  0.26014536]
True reward weights: [-0.75478167 -0.33513042  0.563908  ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.2972
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.40525444  0.24980203  0.87941332]
True reward weights: [-0.9609334  -0.05622448  0.27100887]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.169139

Running PBIRL with 3 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.2936
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.94376486 -0.31426336  0.10269584]
True reward weights: [-0.82240409  0.02672177  0.56827586]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151873

Running experiment 38/50...
Shuffled Demos: [(0, 1), (2, 1), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.2994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.71914754  0.17116744  0.67344527]
True reward weights: [-0.49248773 -0.22865786  0.83974486]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123935

Running PBIRL with 2 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.2978
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.87142382 -0.06042597  0.48679485]
True reward weights: [-0.54540047 -0.17032143  0.82068809]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126966

Running PBIRL with 3 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.2964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.10922386  0.48589829  0.86716377]
True reward weights: [-0.35756304  0.02271839  0.93361263]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.125979

Saving results to files...
Results saved successfully.

Running experiment 39/50...
Shuffled Demos: [(4, 3), (0, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.4826
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.82848747 -0.19264566  0.52582903]
True reward weights: [-0.56481614 -0.46100063  0.68444222]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.88858871 -0.09313968  0.44914931]
True reward weights: [-0.21722155  0.52345014  0.82390215]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.168300

Running PBIRL with 3 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2928
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.41281655  0.15231544  0.89798803]
True reward weights: [-0.72656793 -0.20617298  0.65543248]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152020

Running experiment 40/50...
Shuffled Demos: [(4, 3), (2, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.4794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.62438781  0.08190547  0.77680844]
True reward weights: [-0.6667631  -0.57474284  0.47444455]
MAP Policy for current environment:
