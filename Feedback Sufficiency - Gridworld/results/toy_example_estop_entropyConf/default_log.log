Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [([(0, 1), (3, 1), (3, 2), (3, 3), (0, 1), (3, 2), (3, 2), (3, 1), (3, 0), (0, 0)], 0), ([(0, 0), (0, 2), (0, 0), (0, 3), (1, 3), (2, 3), (2, 0), (2, 1), (5, None)], 0), ([(0, 3), (1, 0), (1, 2), (0, 2), (3, 3), (4, 2), (4, 3), (5, None)], 0), ([(0, 2), (0, 0), (0, 2), (0, 1), (3, 3), (3, 1), (3, 3), (4, 2), (3, 3), (4, 2)], 0), ([(0, 3), (1, 3), (2, 2), (1, 0), (1, 0), (1, 1), (4, 3), (5, None)], 0), ([(0, 2), (0, 3), (1, 2), (0, 2), (0, 1), (1, 3), (2, 0), (2, 0), (2, 0), (2, 0)], 0)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [([(0, 1), (3, 1), (3, 2), (3, 3), (0, 1), (3, 2), (3, 2), (3, 1), (3, 0), (0, 0)], 0), ([(0, 0), (0, 2), (0, 0), (0, 3), (1, 3), (2, 3), (2, 0), (2, 1), (5, None)], 0), ([(0, 3), (1, 0), (1, 2), (0, 2), (3, 3), (4, 2), (4, 3), (5, None)], 0), ([(0, 2), (0, 0), (0, 2), (0, 1), (3, 3), (3, 1), (3, 3), (4, 2), (3, 3), (4, 2)], 0), ([(0, 3), (1, 3), (2, 2), (1, 0), (1, 0), (1, 1), (4, 3), (5, None)], 0), ([(0, 2), (0, 3), (1, 2), (0, 2), (0, 1), (1, 3), (2, 0), (2, 0), (2, 0), (2, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.1108282  -0.96242967  0.24788351]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5874
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.14786828  0.98063069  0.12844616]
True reward weights: [ 0.04034363 -0.80649057  0.58986893]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182407

Running EBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5696
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.39286738 0.91331576 0.10728257]
True reward weights: [-0.44998656  0.89260982  0.02756464]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.174828

Running EBIRL with 4 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5646
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.56157724  0.82428609 -0.07199619]
True reward weights: [-0.4026063   0.44341063  0.80080908]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.222380

Running EBIRL with 5 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5516
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.1494253   0.84747367 -0.5093726 ]
True reward weights: [-0.97280972 -0.2211973  -0.06865139]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.223377

Running EBIRL with 6 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5318
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.46250555 0.83489659 0.29838951]
True reward weights: [-0.85381007  0.18378192 -0.48706527]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.226920

Running experiment 2/50...
Shuffled Demos: [([(0, 3), (1, 0), (1, 3), (2, 2), (1, 0), (1, 1), (0, 0), (0, 1), (0, 1), (3, 2)], 0), ([(0, 1), (1, 3), (2, 1), (5, None)], 3), ([(0, 3), (1, 0), (1, 2), (4, 1), (5, None)], 4), ([(0, 2), (0, 1), (1, 1), (4, 2), (3, 0), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)], 9), ([(0, 2), (0, 2), (0, 2), (0, 0), (0, 0), (1, 3), (2, 0), (2, 1), (5, None)], 8), ([(0, 0), (0, 1), (3, 0), (0, 3), (0, 0), (0, 0), (0, 2), (0, 2), (0, 3), (1, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5940
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.21207031  0.79733906 -0.56504566]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running EBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5868
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.55503076  0.7994232   0.22992044]
True reward weights: [-0.91114478 -0.36391382 -0.19334404]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147617

Running EBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.4784413   0.52755373  0.70198361]
True reward weights: [-0.84769882  0.43713952 -0.30052578]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.202521

Running EBIRL with 4 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5832
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.18018348 0.88986328 0.41913871]
True reward weights: [0.14844945 0.65834773 0.73793023]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.272865

Running EBIRL with 5 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5856
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.25134034 0.68422141 0.68459411]
True reward weights: [-0.82978358  0.2512667  -0.49832144]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.350906

Running EBIRL with 6 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5918
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.23534199 0.95681246 0.17065775]
True reward weights: [-0.78831537 -0.5274164  -0.31684511]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.351379

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Shuffled Demos: [([(0, 0), (0, 3), (1, 1), (4, 1), (3, 0), (0, 3), (1, 0), (2, 2), (1, 1), (0, 0)], 0), ([(0, 1), (1, 3), (2, 2), (1, 3), (1, 1), (2, 0), (1, 2), (0, 0), (0, 1), (3, 1)], 0), ([(0, 0), (0, 0), (0, 2), (3, 1), (3, 2), (0, 3), (1, 0), (1, 0), (1, 1), (4, 3)], 0), ([(0, 2), (0, 1), (1, 3), (2, 1), (1, 2), (0, 1), (3, 2), (0, 1), (3, 3), (0, 0)], 0), ([(0, 2), (0, 3), (1, 2), (0, 1), (3, 2), (0, 2), (0, 1), (3, 0), (0, 1), (3, 3)], 0), ([(0, 0), (0, 2), (0, 1), (3, 0), (0, 1), (3, 3), (3, 2), (3, 3), (0, 0), (0, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5770
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.98615609 -0.12764805 -0.10584016]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5810
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.55578462 -0.14987353 -0.81770495]
True reward weights: [-0.75457871 -0.50469822 -0.41939323]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123706

Running EBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5750
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.48604907  0.26148755  0.83389482]
True reward weights: [-0.78881071  0.31194488 -0.52959235]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123886

Running EBIRL with 4 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.85548218 -0.13695117  0.49939425]
True reward weights: [-0.21544032  0.85081564 -0.47926841]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.123808

Running EBIRL with 5 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5778
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.34532731  0.5820407  -0.73619133]
True reward weights: [-0.4955739   0.85123808  0.17262743]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.123923

Running EBIRL with 6 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5806
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.22886062  0.89155758  0.39082976]
True reward weights: [-0.6838848   0.39508293  0.61336046]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.147940

Running experiment 4/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 2), (3, 2), (3, 0), (0, 1), (3, 0), (0, 0), (0, 2), (0, 3)], 2), ([(0, 2), (0, 2), (0, 2), (3, 0), (0, 2), (0, 3), (1, 3), (2, 3), (2, 0), (2, 3)], 0), ([(0, 3), (1, 0), (1, 3), (1, 1), (4, 0), (1, 3), (2, 2), (2, 1), (5, None)], 0), ([(0, 2), (0, 0), (0, 2), (3, 3), (4, 3), (5, None)], 5), ([(0, 1), (3, 3), (4, 3), (5, None)], 3), ([(0, 0), (0, 0), (0, 0), (1, 2), (0, 2), (0, 3), (1, 2), (1, 2), (0, 1), (1, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.43662139 -0.89703339  0.06850445]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5820
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.10345784  0.9946311  -0.00233481]
True reward weights: [-0.78787567  0.46887575  0.39925863]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182804

Running EBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5678
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.34938688  0.93089302 -0.10661607]
True reward weights: [-0.49867587  0.86320718  0.07871301]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.175457

Running EBIRL with 4 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5722
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.21682026  0.9665171  -0.13723585]
True reward weights: [ 0.10803573  0.31213263 -0.94387579]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.222947

Running EBIRL with 5 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.11608872 0.93227429 0.34261942]
True reward weights: [ 0.39215314  0.90214013 -0.17988634]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.289039

Running EBIRL with 6 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5722
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.48167565  0.87115745 -0.09525366]
True reward weights: [-0.58493439  0.4565261  -0.67039964]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.286364

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Shuffled Demos: [([(0, 3), (1, 1), (2, 3), (2, 3), (2, 0), (2, 2), (1, 2), (0, 0), (0, 1), (3, 0)], 0), ([(0, 0), (0, 2), (3, 3), (4, 2), (1, 0), (1, 1), (2, 2), (1, 2), (0, 1), (0, 0)], 0), ([(0, 1), (3, 1), (3, 3), (4, 0), (1, 1), (4, 2), (3, 3), (4, 0), (1, 0), (2, 2)], 0), ([(0, 3), (1, 0), (1, 3), (1, 2), (0, 2), (0, 3), (0, 1), (1, 3), (2, 1), (5, None)], 0), ([(0, 2), (0, 3), (1, 3), (2, 2), (1, 0), (1, 2), (0, 2), (0, 3), (1, 0), (1, 3)], 0), ([(0, 3), (1, 0), (1, 1), (0, 2), (0, 3), (1, 3), (2, 1), (5, None)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.31915771  0.87954024 -0.35291264]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5856
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.6825035   0.0154088   0.73071988]
True reward weights: [ 0.61646526  0.78258009 -0.08682732]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123739

Running EBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5816
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.84728093  0.29937308  0.43873772]
True reward weights: [-0.77122016  0.59494261 -0.2264128 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123762

Running EBIRL with 4 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5708
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.88396511 -0.28785508 -0.36843608]
True reward weights: [-0.09708078  0.94298504 -0.31836228]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.129490

Running EBIRL with 5 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5550
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.1666144   0.98391984 -0.06435367]
True reward weights: [-0.78672986 -0.59718021  0.15630712]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.128315

Running EBIRL with 6 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5496
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.74017462 -0.19238525 -0.6443054 ]
True reward weights: [ 0.50043163  0.81926173 -0.27996142]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.129481

Running experiment 6/50...
Shuffled Demos: [([(0, 1), (3, 1), (3, 3), (4, 1), (4, 3), (5, None)], 5), ([(0, 3), (1, 0), (0, 0), (0, 2), (0, 1), (3, 3), (3, 3), (4, 2), (3, 3), (4, 1)], 0), ([(0, 2), (0, 1), (3, 3), (4, 0), (1, 0), (1, 1), (4, 3), (5, None)], 7), ([(0, 3), (0, 0), (0, 3), (1, 2), (1, 3), (2, 2), (1, 2), (0, 2), (0, 1), (3, 2)], 0), ([(0, 2), (0, 0), (0, 1), (3, 0), (4, 0), (1, 2), (0, 2), (0, 3), (1, 0), (2, 1)], 0), ([(0, 2), (0, 0), (0, 1), (3, 2), (3, 2), (3, 1), (3, 1), (3, 2), (3, 0), (0, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.10357442  0.32337396  0.94058579]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5912
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.85591838  0.51635739 -0.02790655]
True reward weights: [-0.30691335  0.49573968  0.81243238]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.184549

Running EBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.37001779 0.91714627 0.14808633]
True reward weights: [-0.74153929 -0.58008463 -0.33707759]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.226095

Running EBIRL with 4 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5840
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.28785719 0.95189043 0.10508498]
True reward weights: [-0.81545226 -0.31132126 -0.48797202]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.220072

Running EBIRL with 5 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.4264777  0.78276645 0.45320354]
True reward weights: [-0.81323547  0.06613603  0.57816441]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.216539

Running EBIRL with 6 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5820
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.48018623  0.8650753  -0.14514099]
True reward weights: [ 0.07334567  0.28703712 -0.95510738]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.284431

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Shuffled Demos: [([(0, 2), (0, 0), (0, 3), (1, 2), (0, 0), (0, 2), (0, 1), (1, 3), (2, 1), (5, None)], 9), ([(0, 0), (0, 2), (0, 3), (0, 1), (3, 0), (4, 3), (5, None)], 6), ([(0, 2), (0, 0), (1, 0), (1, 3), (2, 2), (5, None)], 5), ([(0, 3), (3, 0), (0, 3), (1, 1), (2, 2), (1, 1), (4, 0), (1, 0), (1, 1), (4, 3)], 0), ([(0, 0), (0, 3), (1, 0), (2, 3), (2, 1), (5, None)], 5), ([(0, 2), (0, 1), (1, 3), (2, 2), (1, 2), (0, 3), (1, 3), (4, 0), (1, 2), (0, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.05060207 0.98885722 0.14000294]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.00460517 0.87127509 0.49077337]
True reward weights: [ 0.42148204 -0.52774444 -0.7374542 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.25170658  0.91742711  0.30817414]
True reward weights: [-0.33287979 -0.89197684 -0.30588947]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.5854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.03891445  0.96441565 -0.26151123]
True reward weights: [-0.30906799  0.00905659 -0.95099682]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.331440

Running EBIRL with 5 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.5930
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.24483593  0.87611348  0.41530776]
True reward weights: [-0.6594808   0.72816246  0.18672039]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.395311

Running EBIRL with 6 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.5860
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.07366594  0.97382975 -0.21500918]
True reward weights: [-0.71892337  0.52840147  0.45159835]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.386815

Running experiment 8/50...
Shuffled Demos: [([(0, 3), (3, 1), (3, 0), (0, 3), (1, 3), (2, 2), (1, 3), (2, 0), (2, 1), (5, None)], 9), ([(0, 3), (1, 3), (2, 1), (2, 0), (2, 3), (2, 1), (5, None)], 6), ([(0, 2), (0, 1), (3, 3), (3, 1), (3, 1), (3, 3), (4, 0), (1, 2), (0, 3), (1, 3)], 0), ([(0, 2), (0, 0), (0, 2), (0, 3), (1, 0), (1, 2), (1, 1), (4, 1), (4, 0), (1, 1)], 0), ([(0, 1), (3, 2), (3, 1), (3, 2), (0, 2), (0, 0), (0, 2), (0, 3), (1, 0), (2, 2)], 0), ([(0, 2), (0, 0), (1, 1), (4, 0), (1, 0), (1, 0), (1, 0), (2, 0), (2, 2), (1, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.22831809 -0.62750032 -0.74438847]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.26997956 0.95272507 0.13937709]
True reward weights: [-0.32540768 -0.94538088  0.01910067]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.5966
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.16601226 0.96697194 0.19340422]
True reward weights: [-0.15836636  0.96898605  0.18970011]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.253465

Running EBIRL with 4 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.5896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.27620178 0.91809345 0.28428328]
True reward weights: [-0.45254373  0.796768   -0.40045589]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.238859

Running EBIRL with 5 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.5852
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.16010552 0.96605853 0.2027243 ]
True reward weights: [-0.07834324  0.89247225 -0.44424725]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.231005

Running EBIRL with 6 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.5822
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.25430368 0.93925604 0.23049452]
True reward weights: [-0.81079252  0.43955332 -0.38653379]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.225864

Saving results to files...
Results saved successfully.

Running experiment 9/50...
Shuffled Demos: [([(0, 1), (0, 0), (0, 0), (0, 0), (1, 1), (4, 2), (3, 1), (3, 3), (3, 2), (3, 1)], 0), ([(0, 2), (0, 1), (3, 3), (4, 2), (3, 2), (3, 1), (3, 3), (4, 2), (3, 3), (4, 0)], 7), ([(0, 2), (0, 2), (0, 2), (0, 0), (0, 3), (1, 1), (4, 3), (5, None)], 7), ([(0, 3), (1, 2), (0, 2), (0, 1), (3, 0), (0, 3), (1, 2), (0, 2), (0, 3), (1, 3)], 0), ([(0, 3), (1, 1), (4, 0), (1, 2), (0, 3), (1, 2), (0, 1), (3, 0), (0, 0), (0, 1)], 0), ([(0, 0), (0, 2), (0, 2), (0, 3), (1, 3), (2, 3), (2, 1), (5, None)], 7)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5928
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.40894801 0.79709798 0.44429306]
True reward weights: [-0.0505111  -0.01531811  0.99860602]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123672

Running EBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5730
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.00085523  0.84268214 -0.5384107 ]
True reward weights: [-0.22000473  0.28607953  0.93260732]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148243

Running EBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.02427274  0.94571687  0.32408398]
True reward weights: [-0.91362681  0.35949554 -0.18986577]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.203112

Running EBIRL with 4 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5830
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.14479582  0.98923656 -0.0210996 ]
True reward weights: [-0.01360559  0.3169756  -0.9483361 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.203249

Running EBIRL with 5 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5980
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.13071201  0.98605365  0.10301735]
True reward weights: [-0.75691194  0.59343458  0.27371466]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.203180

Running EBIRL with 6 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5764
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.10787227  0.992681    0.05429547]
True reward weights: [-0.97479636  0.21837119 -0.04567361]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.273401

Running experiment 10/50...
Shuffled Demos: [([(0, 2), (0, 0), (0, 2), (0, 0), (0, 3), (1, 1), (4, 3), (5, None)], 7), ([(0, 3), (3, 0), (3, 3), (4, 3), (5, None)], 4), ([(0, 3), (1, 3), (2, 3), (2, 2), (1, 0), (2, 2), (1, 3), (2, 2), (2, 1), (5, None)], 9), ([(0, 2), (0, 2), (0, 2), (0, 0), (0, 1), (3, 0), (4, 1), (4, 1), (4, 0), (1, 0)], 0), ([(0, 0), (0, 0), (0, 3), (1, 0), (1, 1), (4, 2), (3, 3), (4, 3), (5, None)], 8), ([(0, 0), (0, 1), (3, 2), (3, 0), (0, 3), (1, 1), (4, 2), (3, 0), (0, 2), (0, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.11940662 -0.94653962 -0.29967449]
True reward weights: [-0.76518493 -0.64328377  0.02603879]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.58691328 -0.70352177 -0.40073672]
True reward weights: [-0.92455625 -0.165401   -0.34327577]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.5728838  -0.7590141  -0.30935699]
True reward weights: [-0.38040767 -0.43684977  0.81513943]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.05634157  0.99765089 -0.03896588]
True reward weights: [ 0.07032498  0.08820126 -0.9936171 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.329693

Running EBIRL with 5 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.04622665 0.7348084  0.67669765]
True reward weights: [-0.27290166  0.37311716 -0.88674025]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.394331

Running EBIRL with 6 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.17275373  0.98489349 -0.01187287]
True reward weights: [-0.76635561  0.06167065 -0.63944962]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.386590

Saving results to files...
Results saved successfully.

Running experiment 11/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (4, 1), (3, 0), (0, 3), (1, 2), (0, 1), (3, 1), (3, 1)], 0), ([(0, 0), (0, 3), (0, 3), (1, 2), (0, 3), (3, 2), (0, 0), (0, 1), (3, 3), (4, 0)], 0), ([(0, 1), (3, 1), (3, 0), (0, 0), (0, 1), (3, 2), (3, 0), (4, 0), (3, 3), (4, 1)], 0), ([(0, 2), (0, 3), (0, 0), (0, 3), (3, 2), (3, 3), (4, 0), (1, 3), (2, 0), (2, 0)], 0), ([(0, 3), (1, 2), (0, 3), (0, 1), (3, 0), (0, 2), (0, 1), (0, 2), (0, 0), (1, 1)], 0), ([(0, 2), (0, 1), (3, 0), (0, 2), (3, 1), (3, 0), (3, 2), (3, 3), (0, 2), (0, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.7149234   0.19235932  0.67222201]
True reward weights: [-0.87829137 -0.24439244  0.41094598]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running EBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5792
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.17936033  0.41820392  0.89046918]
True reward weights: [-0.4583204   0.67715249 -0.57567952]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123888

Running EBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5890
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.21045391  0.94768229  0.24001549]
True reward weights: [-0.91706682  0.3862315  -0.09906399]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148130

Running EBIRL with 4 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5764
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.29063612  0.95609318 -0.03763612]
True reward weights: [ 0.12847325  0.55526481 -0.8216907 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.148182

Running EBIRL with 5 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5814
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.0345595   0.97862833 -0.20271218]
True reward weights: [ 0.40919766  0.64488994 -0.6455031 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.148218

Running EBIRL with 6 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5846
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.22437514  0.95519189 -0.19303949]
True reward weights: [0.46944235 0.65992981 0.58661446]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.203104

Running experiment 12/50...
Shuffled Demos: [([(0, 0), (1, 1), (4, 0), (1, 0), (1, 3), (2, 1), (2, 2), (2, 1), (1, 2), (0, 0)], 0), ([(0, 0), (0, 3), (1, 1), (4, 1), (4, 0), (3, 2), (3, 3), (4, 3), (4, 2), (3, 1)], 0), ([(0, 0), (1, 3), (1, 1), (4, 2), (4, 0), (1, 2), (4, 3), (1, 1), (4, 0), (3, 3)], 0), ([(0, 0), (0, 3), (1, 1), (4, 2), (3, 1), (3, 1), (3, 1), (3, 3), (4, 1), (4, 0)], 0), ([(0, 1), (3, 2), (3, 2), (3, 2), (3, 2), (3, 3), (4, 0), (1, 3), (2, 1), (5, None)], 9), ([(0, 1), (3, 3), (4, 3), (5, None)], 3)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5916
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.74651275  0.6551468  -0.11619549]
True reward weights: [-0.8497926  -0.50850291  0.13884283]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running EBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5822
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.22092146 0.78967896 0.57236426]
True reward weights: [-0.41689526 -0.3642133  -0.8327947 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123757

Running EBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.53974447  0.81476703  0.21173237]
True reward weights: [-0.76254227  0.33575995 -0.55298693]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123934

Running EBIRL with 4 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5892
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.98147317 -0.07490278 -0.17635191]
True reward weights: [-0.2462676  -0.04946923 -0.96793856]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.124150

Running EBIRL with 5 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.34275283  0.86218737 -0.37303277]
True reward weights: [-0.73256051 -0.17584278 -0.65759746]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.148145

Running EBIRL with 6 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.25647586  0.93427452 -0.24769187]
True reward weights: [-0.57707205  0.52047016  0.6293637 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.202983

Saving results to files...
Results saved successfully.

Running experiment 13/50...
Shuffled Demos: [([(0, 0), (0, 2), (0, 0), (1, 3), (2, 0), (2, 2), (1, 2), (0, 0), (0, 2), (0, 1)], 0), ([(0, 1), (0, 3), (3, 0), (0, 3), (1, 3), (2, 0), (2, 1), (5, None)], 7), ([(0, 2), (0, 0), (0, 0), (0, 2), (0, 1), (3, 3), (0, 1), (1, 2), (0, 3), (1, 0)], 0), ([(0, 1), (3, 2), (3, 0), (0, 2), (0, 3), (1, 1), (4, 0), (1, 2), (0, 1), (3, 2)], 0), ([(0, 2), (0, 2), (0, 0), (1, 2), (0, 1), (3, 3), (4, 2), (3, 1), (3, 3), (0, 1)], 0), ([(0, 0), (0, 3), (3, 1), (4, 2), (3, 1), (3, 1), (3, 0), (0, 3), (0, 2), (0, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.47288218 -0.11235805 -0.87393255]
True reward weights: [-0.34477193 -0.32328333  0.88126058]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.00411458  0.95685908 -0.29052328]
True reward weights: [ 0.25767954  0.33915495 -0.90475144]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147775

Running EBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5868
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.44996194 0.68617331 0.57157716]
True reward weights: [-0.74865328 -0.5211919   0.40972828]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.147921

Running EBIRL with 4 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.3563022  0.69622455 0.62315337]
True reward weights: [0.50460379 0.58501012 0.63493163]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.147962

Running EBIRL with 5 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5856
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.31369501  0.93084781 -0.18739743]
True reward weights: [0.09237224 0.99451798 0.04900366]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.148080

Running EBIRL with 6 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5906
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.18288306  0.87123947  0.45551682]
True reward weights: [0.13934785 0.55252781 0.82176347]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.202983

Running experiment 14/50...
Shuffled Demos: [([(0, 0), (1, 3), (4, 1), (4, 1), (4, 1), (5, None)], 5), ([(0, 0), (0, 2), (0, 3), (1, 1), (4, 2), (3, 0), (0, 1), (3, 2), (3, 1), (3, 0)], 0), ([(0, 2), (0, 1), (3, 1), (3, 1), (3, 1), (3, 0), (0, 0), (0, 2), (0, 2), (0, 0)], 1), ([(0, 0), (0, 1), (1, 1), (4, 0), (1, 2), (0, 1), (3, 0), (0, 3), (1, 3), (2, 3)], 0), ([(0, 2), (0, 1), (3, 0), (0, 0), (0, 3), (1, 3), (2, 2), (1, 1), (4, 2), (3, 3)], 0), ([(0, 1), (3, 3), (4, 1), (5, None)], 3)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.62856654  0.72974037  0.26904107]
True reward weights: [-0.43088031 -0.17122291  0.88601629]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.60825179  0.69470235  0.38395104]
True reward weights: [-0.74401442  0.48471161 -0.45988824]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183101

Running EBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5744
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.05825491 0.78195046 0.62061247]
True reward weights: [-0.35106982 -0.15946971  0.92266971]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.225350

Running EBIRL with 4 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5766
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.18871738 0.75892974 0.6232266 ]
True reward weights: [-0.82988389  0.55746462 -0.02293294]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.219618

Running EBIRL with 5 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.54727944  0.77496821  0.31608462]
True reward weights: [-0.7336902  -0.16809334  0.65836412]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.216177

Running EBIRL with 6 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5912
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.26435349 0.78848748 0.55534199]
True reward weights: [-0.58964885 -0.44818267 -0.6718977 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.284150

Saving results to files...
Results saved successfully.

Running experiment 15/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 2), (3, 0), (0, 0), (0, 1), (3, 1), (3, 1), (3, 1), (3, 3)], 0), ([(0, 0), (0, 0), (0, 1), (1, 2), (0, 3), (1, 0), (1, 1), (2, 1), (5, None)], 0), ([(0, 3), (1, 2), (0, 3), (1, 3), (2, 1), (1, 0), (0, 2), (0, 0), (0, 3), (1, 3)], 0), ([(0, 0), (0, 0), (0, 3), (1, 0), (0, 0), (0, 1), (1, 0), (1, 0), (1, 0), (0, 3)], 0), ([(0, 2), (0, 2), (0, 3), (1, 3), (2, 1), (1, 1), (4, 3), (5, None)], 0), ([(0, 0), (0, 3), (3, 0), (0, 3), (3, 1), (3, 0), (0, 3), (1, 2), (4, 2), (3, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.11135383 -0.95686773  0.26833651]
True reward weights: [-0.75433776 -0.02805593  0.65588674]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.48320314  0.83805933 -0.25332052]
True reward weights: [0.16143355 0.39582068 0.90402721]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183255

Running EBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5606
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.4158858   0.86623467 -0.27690521]
True reward weights: [0.29709832 0.58902445 0.75152031]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.175914

Running EBIRL with 4 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5628
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.5100359  0.81861273 0.26407685]
True reward weights: [ 0.35803024  0.78994364 -0.49779855]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.168363

Running EBIRL with 5 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5460
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.17254645 0.98379848 0.04866492]
True reward weights: [0.44153865 0.89393349 0.07698402]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.166543

Running EBIRL with 6 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5500
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.08255347  0.97956144  0.18342386]
True reward weights: [-0.48025196  0.5625731  -0.67295584]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.163381

Running experiment 16/50...
Shuffled Demos: [([(0, 1), (3, 3), (3, 1), (3, 0), (0, 2), (0, 0), (0, 2), (0, 3), (1, 2), (1, 1)], 0), ([(0, 3), (1, 0), (1, 1), (4, 1), (4, 0), (5, None)], 0), ([(0, 1), (3, 3), (4, 2), (3, 0), (0, 1), (1, 2), (0, 1), (3, 0), (3, 0), (0, 1)], 0), ([(0, 0), (0, 1), (1, 2), (0, 1), (3, 2), (3, 2), (3, 3), (4, 1), (4, 1), (3, 1)], 0), ([(0, 0), (0, 3), (1, 2), (0, 2), (0, 3), (1, 1), (4, 0), (1, 1), (4, 2), (4, 3)], 0), ([(0, 3), (1, 3), (2, 0), (2, 2), (2, 1), (5, None)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.48619616 -0.13307473 -0.86365758]
True reward weights: [-0.92032338 -0.11683579  0.37330186]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running EBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5604
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.68455629 -0.04604063  0.7275046 ]
True reward weights: [-0.9341559  -0.23611771 -0.26758398]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.129460

Running EBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5462
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.51553307  0.19460962  0.83447753]
True reward weights: [-0.65444256  0.60747524  0.45019858]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127757

Running EBIRL with 4 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5504
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.8943406   0.24107433  0.37687936]
True reward weights: [-0.47766798  0.55690112 -0.67948101]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.126788

Running EBIRL with 5 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5456
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.40078941  0.54820275  0.7340583 ]
True reward weights: [-0.32935198  0.78827038 -0.51976637]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.126500

Running EBIRL with 6 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5320
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.89007829  0.27986549  0.35977207]
True reward weights: [-0.30192382  0.73249803 -0.61015461]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.132537

Saving results to files...
Results saved successfully.

Running experiment 17/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 3), (1, 3), (2, 2), (1, 2), (0, 0), (0, 2), (0, 0), (0, 0)], 0), ([(0, 1), (3, 3), (4, 0), (1, 3), (2, 2), (1, 1), (4, 1), (5, None)], 0), ([(0, 3), (1, 0), (0, 1), (3, 0), (0, 3), (1, 2), (0, 3), (1, 3), (2, 0), (2, 3)], 0), ([(0, 2), (0, 1), (3, 0), (0, 1), (0, 0), (0, 3), (1, 1), (4, 1), (4, 2), (3, 0)], 0), ([(0, 3), (1, 3), (2, 2), (1, 1), (4, 1), (4, 0), (1, 1), (4, 2), (3, 3), (4, 1)], 0), ([(0, 2), (0, 3), (0, 0), (0, 0), (0, 3), (3, 1), (3, 1), (3, 0), (0, 2), (0, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.5888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.42242356  0.07252262 -0.90349256]
True reward weights: [-0.76208548 -0.28664819  0.58056743]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.5528
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.92133501 -0.34318724  0.18265903]
True reward weights: [-0.50619714 -0.42719583  0.74917834]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.129504

Running EBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.5514
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.57541443 0.76202101 0.29702224]
True reward weights: [-0.85093559 -0.51669416  0.0945292 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127760

Running EBIRL with 4 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.5496
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.34903144  0.61127736 -0.71029363]
True reward weights: [-0.84816187  0.26541496 -0.45844993]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.126826

Running EBIRL with 5 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.5422
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.18090107  0.93592132 -0.30220206]
True reward weights: [-0.9979901   0.02348536  0.05885749]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.126194

Running EBIRL with 6 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.5462
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.01591931  0.99197473 -0.12543013]
True reward weights: [ 0.39531786  0.916939   -0.05428326]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.150008

Running experiment 18/50...
Shuffled Demos: [([(0, 0), (0, 3), (0, 1), (3, 3), (0, 1), (3, 2), (3, 1), (3, 1), (3, 2), (3, 2)], 2), ([(0, 3), (1, 3), (2, 2), (5, None)], 3), ([(0, 2), (0, 3), (1, 2), (0, 2), (0, 3), (1, 1), (4, 0), (1, 1), (2, 1), (5, None)], 9), ([(0, 0), (0, 3), (0, 3), (1, 2), (4, 1), (4, 2), (3, 3), (3, 3), (4, 2), (1, 1)], 0), ([(0, 1), (3, 3), (4, 3), (5, None)], 3), ([(0, 2), (0, 3), (1, 0), (1, 3), (2, 1), (5, None)], 5)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.27252782 -0.88915862  0.36759425]
True reward weights: [-0.39174849 -0.30341429  0.86860399]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.12350208  0.84025281 -0.52794171]
True reward weights: [-0.99557604  0.05268622 -0.07779788]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.03953787  0.97595048  0.21437679]
True reward weights: [-0.10626275 -0.53794352 -0.83625654]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.5900
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.13446951 0.98422545 0.11497052]
True reward weights: [-0.44069194  0.75956649  0.47838202]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.332160

Running EBIRL with 5 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.5900
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.06587079  0.96491781 -0.25415481]
True reward weights: [-0.47916237  0.30499524 -0.82303179]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.395813

Running EBIRL with 6 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.5844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.04685793  0.99028908 -0.13088879]
True reward weights: [-0.87595924  0.48223088  0.01219799]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.469041

Saving results to files...
Results saved successfully.

Running experiment 19/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 2), (0, 3), (1, 3), (2, 3), (2, 2), (5, None)], 0), ([(0, 1), (1, 3), (2, 0), (2, 2), (1, 2), (0, 0), (0, 1), (3, 1), (3, 0), (3, 1)], 0), ([(0, 0), (0, 3), (3, 2), (3, 2), (3, 1), (3, 3), (4, 0), (1, 3), (2, 2), (1, 0)], 0), ([(0, 0), (0, 1), (3, 0), (3, 3), (4, 3), (5, None)], 5), ([(0, 2), (0, 3), (1, 3), (2, 2), (2, 0), (2, 2), (1, 0), (1, 3), (4, 2), (3, 1)], 0), ([(0, 2), (0, 1), (0, 0), (0, 2), (0, 0), (0, 1), (0, 0), (0, 1), (3, 3), (4, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5914
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.56228066 -0.10882503 -0.81975458]
True reward weights: [-0.92648321 -0.16748955  0.33701054]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5246
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.89310935  0.36026584  0.26937746]
True reward weights: [ 0.03633363  0.32176051 -0.94612369]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.135227

Running EBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5252
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.72297835  0.60832077  0.32748763]
True reward weights: [-0.93574754  0.32784648 -0.12997393]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.131807

Running EBIRL with 4 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5278
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.430888    0.63886322 -0.63732983]
True reward weights: [-0.38847744  0.11223165 -0.91459791]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.153634

Running EBIRL with 5 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5152
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.07870748  0.66087393 -0.74635835]
True reward weights: [-0.14344239  0.93754277 -0.31691929]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.152496

Running EBIRL with 6 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5224
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.50036147  0.82962611 -0.24770731]
True reward weights: [-0.93853415  0.11779408  0.32446603]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.206662

Running experiment 20/50...
Shuffled Demos: [([(0, 0), (0, 1), (3, 0), (0, 3), (1, 0), (1, 1), (4, 0), (1, 1), (4, 1), (3, 3)], 0), ([(0, 0), (1, 1), (4, 2), (3, 1), (3, 0), (0, 1), (3, 2), (3, 3), (4, 3), (5, None)], 9), ([(0, 1), (3, 3), (4, 3), (5, None)], 3), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], 4), ([(0, 3), (1, 1), (4, 3), (5, None)], 3), ([(0, 1), (3, 0), (0, 1), (0, 0), (0, 3), (1, 2), (4, 3), (5, None)], 7)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5894
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.82193266 -0.38766466  0.41730422]
True reward weights: [-0.61012931 -0.3129562   0.72787406]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.44312278  0.88939976  0.11229544]
True reward weights: [-0.15365242  0.97503222 -0.1603219 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147797

Running EBIRL with 3 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.58736781 0.75341489 0.29557582]
True reward weights: [-0.54818407  0.24045465  0.80104668]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.202594

Running EBIRL with 4 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5748
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.57792425 0.73511981 0.35440433]
True reward weights: [-0.37811179  0.29760916  0.87661865]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.272888

Running EBIRL with 5 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.32541198  0.61224162  0.720602  ]
True reward weights: [-0.89453183  0.13481214 -0.42619068]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.350890

Running EBIRL with 6 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5890
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.23269159  0.93826722 -0.25594774]
True reward weights: [-0.54983527  0.65199858  0.52209102]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.432726

Saving results to files...
Results saved successfully.

Running experiment 21/50...
Shuffled Demos: [([(0, 0), (1, 1), (4, 2), (3, 2), (3, 0), (0, 0), (0, 1), (0, 0), (0, 3), (1, 2)], 0), ([(0, 0), (0, 1), (3, 1), (4, 2), (3, 1), (3, 3), (4, 0), (1, 2), (1, 2), (4, 3)], 0), ([(0, 1), (3, 0), (0, 1), (1, 1), (4, 0), (3, 0), (0, 2), (0, 3), (1, 1), (4, 2)], 0), ([(0, 2), (0, 1), (3, 2), (3, 3), (4, 0), (1, 1), (4, 3), (4, 3), (5, None)], 8), ([(0, 0), (1, 1), (4, 0), (1, 1), (4, 2), (3, 1), (3, 2), (3, 0), (4, 2), (1, 3)], 0), ([(0, 1), (3, 0), (3, 3), (4, 2), (3, 1), (3, 2), (3, 3), (3, 0), (0, 2), (0, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5922
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.12581325  0.13420518 -0.98293438]
True reward weights: [-0.3738588  -0.3262014   0.86822937]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.04110645  0.54186726  0.83945824]
True reward weights: [0.03714823 0.37821422 0.92497244]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123833

Running EBIRL with 3 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.18003405 0.76678015 0.61614604]
True reward weights: [ 0.36572463  0.44622899 -0.81677731]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123812

Running EBIRL with 4 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5722
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.44506821  0.86830727 -0.21899037]
True reward weights: [-0.7818568  -0.08963808  0.61698052]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.147856

Running EBIRL with 5 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5832
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.22120894  0.78883179  0.57342045]
True reward weights: [-0.61633423  0.14722171 -0.7736006 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.147999

Running EBIRL with 6 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5866
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.01927211  0.89329906  0.44904941]
True reward weights: [-0.76896561 -0.18245322 -0.61270116]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.202767

Running experiment 22/50...
Shuffled Demos: [([(0, 0), (0, 2), (0, 0), (0, 0), (0, 0), (0, 2), (0, 1), (1, 2), (0, 1), (3, 1)], 0), ([(0, 2), (0, 0), (0, 1), (3, 1), (3, 2), (3, 2), (3, 2), (3, 2), (3, 1), (3, 2)], 0), ([(0, 3), (1, 1), (4, 3), (4, 0), (1, 1), (4, 1), (4, 1), (3, 2), (3, 2), (3, 2)], 0), ([(0, 1), (0, 2), (0, 0), (0, 3), (1, 0), (2, 1), (5, None)], 6), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], 4), ([(0, 0), (0, 0), (0, 0), (0, 1), (3, 3), (4, 1), (4, 0), (1, 2), (0, 3), (3, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5892
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.52680705  0.79846601 -0.29142129]
True reward weights: [-0.70965126 -0.6051058   0.36089066]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running EBIRL with 2 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.06847524 0.95474144 0.28944762]
True reward weights: [-0.99822789  0.00888378  0.05884007]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147950

Running EBIRL with 3 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5830
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.19566141  0.96373878 -0.18145022]
True reward weights: [-0.82190139 -0.22592536  0.52291093]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148121

Running EBIRL with 4 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5788
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.31418577 0.91367495 0.2578476 ]
True reward weights: [-0.87990372  0.38519054  0.27820439]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202981

Running EBIRL with 5 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.09126716 0.97560737 0.1996511 ]
True reward weights: [-0.5426727   0.32607058 -0.77406997]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.273227

Running EBIRL with 6 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5826
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.04538    0.98599096 0.16050692]
True reward weights: [-0.56942147  0.68866365  0.44888926]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.273442

Saving results to files...
Results saved successfully.

Running experiment 23/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 0), (0, 0), (0, 1), (3, 3), (4, 0), (5, None)], 7), ([(0, 3), (0, 2), (0, 1), (3, 2), (3, 1), (3, 2), (3, 3), (4, 3), (5, None)], 8), ([(0, 1), (3, 0), (0, 0), (0, 3), (0, 2), (0, 1), (3, 2), (3, 0), (0, 3), (1, 2)], 0), ([(0, 0), (0, 2), (3, 2), (3, 1), (3, 2), (3, 2), (0, 2), (0, 1), (3, 3), (4, 2)], 0), ([(0, 3), (1, 0), (1, 2), (0, 2), (0, 2), (0, 0), (0, 3), (1, 1), (4, 0), (1, 0)], 0), ([(0, 2), (0, 3), (1, 0), (1, 2), (0, 0), (0, 2), (0, 3), (1, 2), (0, 0), (0, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 23
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.10166521  0.98874812 -0.10973302]
True reward weights: [-0.74528522 -0.55533186  0.36899385]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 23
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.62515083 -0.77863664 -0.05395752]
True reward weights: [ 0.04689145  0.58315244 -0.81100828]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.5960
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.30605037  0.76880051  0.56149706]
True reward weights: [ 0.39517233  0.57081197 -0.71973087]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.252768

Running EBIRL with 4 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.5866
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.48053068  0.87647187 -0.02978819]
True reward weights: [ 0.11236281  0.25615084 -0.96008403]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.308715

Running EBIRL with 5 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.5888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.39245482  0.84755635 -0.35724983]
True reward weights: [0.06904745 0.2032095  0.97669768]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.300994

Running EBIRL with 6 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.5814
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.27686056  0.95902406 -0.06017545]
True reward weights: [-0.55506112  0.29957799  0.77598981]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.295939

Running experiment 24/50...
Shuffled Demos: [([(0, 0), (1, 1), (0, 2), (0, 0), (0, 1), (3, 2), (3, 1), (4, 3), (5, None)], 8), ([(0, 2), (0, 3), (1, 0), (1, 1), (4, 2), (1, 0), (1, 3), (2, 2), (1, 0), (1, 0)], 0), ([(0, 0), (0, 2), (0, 3), (1, 0), (1, 0), (2, 3), (2, 0), (2, 2), (1, 3), (2, 1)], 0), ([(0, 3), (1, 3), (1, 0), (1, 2), (0, 3), (1, 2), (0, 2), (0, 1), (1, 3), (2, 0)], 0), ([(0, 3), (1, 2), (0, 3), (1, 0), (1, 0), (1, 0), (0, 1), (3, 0), (0, 1), (3, 3)], 0), ([(0, 1), (3, 0), (0, 1), (3, 1), (4, 3), (4, 0), (1, 3), (1, 1), (4, 3), (5, None)], 9)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 24
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.11425378 -0.83428533  0.5393645 ]
True reward weights: [-0.68144434 -0.29187893  0.6711485 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.5854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.17028542  0.85339466 -0.49266665]
True reward weights: [-0.63997009  0.16690252  0.75005455]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182171

Running EBIRL with 3 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.5904
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.40765482  0.89216375 -0.19458006]
True reward weights: [-0.81718399  0.3214315  -0.47842671]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.169616

Running EBIRL with 4 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.5636
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.29410275  0.86621655  0.4039461 ]
True reward weights: [-0.55643907 -0.34691205  0.75500171]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.163783

Running EBIRL with 5 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.5894
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.50070232 0.62662246 0.59719467]
True reward weights: [-0.51505771 -0.4899396  -0.70333118]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.160431

Running EBIRL with 6 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.5964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.2174101   0.77249328  0.59664645]
True reward weights: [-0.92992967 -0.35832108  0.08268506]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.213057

Saving results to files...
Results saved successfully.

Running experiment 25/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 1), (3, 2), (3, 0), (0, 1), (3, 1), (3, 3), (4, 3), (5, None)], 9), ([(0, 0), (0, 2), (0, 2), (0, 1), (3, 0), (0, 3), (1, 2), (0, 2), (0, 2), (0, 0)], 0), ([(0, 2), (0, 3), (1, 2), (0, 3), (1, 2), (0, 3), (1, 1), (4, 0), (1, 1), (4, 1)], 0), ([(0, 2), (3, 2), (3, 3), (0, 3), (1, 3), (2, 1), (5, None)], 6), ([(0, 1), (3, 2), (3, 1), (3, 0), (0, 1), (3, 0), (0, 0), (0, 3), (3, 3), (4, 2)], 2), ([(0, 1), (3, 0), (0, 1), (3, 0), (0, 3), (3, 3), (4, 0), (5, None)], 7)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 25
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.17457367  0.97926195 -0.10281083]
True reward weights: [-0.44714936 -0.30119858  0.84222139]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5820
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.35424825  0.92834867 -0.1125919 ]
True reward weights: [-0.64145367  0.47721293  0.60067046]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183159

Running EBIRL with 3 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5956
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.36188926 0.88564245 0.29098731]
True reward weights: [-0.77477136  0.62930567  0.06085822]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170574

Running EBIRL with 4 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5776
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.27396678 0.95746887 0.09052938]
True reward weights: [-0.38519273  0.55462395 -0.73757632]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.219406

Running EBIRL with 5 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5872
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.3674255   0.91706765  0.15487228]
True reward weights: [-0.49405176 -0.30331642 -0.81480796]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.286206

Running EBIRL with 6 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.32720642 0.92945019 0.17046496]
True reward weights: [-0.30113993  0.88338516 -0.35908967]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.361882

Running experiment 26/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 2), (0, 3), (0, 0), (0, 0), (0, 3), (3, 2), (3, 1), (3, 1)], 0), ([(0, 3), (1, 0), (1, 2), (0, 2), (0, 1), (3, 1), (4, 2), (3, 1), (3, 0), (0, 1)], 0), ([(0, 3), (1, 1), (4, 3), (5, None)], 3), ([(0, 0), (0, 1), (3, 3), (4, 1), (3, 0), (0, 2), (0, 3), (1, 3), (4, 2), (3, 0)], 0), ([(0, 2), (0, 3), (1, 3), (2, 2), (1, 2), (0, 1), (3, 2), (3, 1), (3, 2), (3, 2)], 0), ([(0, 3), (1, 1), (4, 2), (3, 1), (3, 2), (3, 0), (4, 2), (3, 2), (3, 1), (4, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 26
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.28220173 -0.93653639  0.20799464]
True reward weights: [-0.7333898  -0.47830978  0.48307263]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.5774
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.26164252  0.96516103  0.00271754]
True reward weights: [ 0.65298248  0.38063901 -0.65477311]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.184156

Running EBIRL with 3 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.5908
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.04432408  0.85246745 -0.5208979 ]
True reward weights: [-0.50617186  0.40326134 -0.76234529]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.225746

Running EBIRL with 4 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.5840
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.46153024  0.88649479 -0.03341887]
True reward weights: [-0.45964624  0.81750911  0.34699306]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.219932

Running EBIRL with 5 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.5958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.38215854  0.85320808 -0.35495184]
True reward weights: [-0.79826925 -0.0320413  -0.60144789]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.216445

Running EBIRL with 6 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.5812
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.36164666  0.87665432 -0.31731513]
True reward weights: [-0.02014499  0.08681607 -0.99602066]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.214310

Saving results to files...
Results saved successfully.

Running experiment 27/50...
Shuffled Demos: [([(0, 3), (1, 2), (0, 3), (1, 0), (1, 1), (4, 0), (1, 3), (2, 1), (5, None)], 0), ([(0, 1), (3, 1), (3, 1), (4, 2), (3, 1), (3, 3), (4, 0), (1, 3), (2, 3), (2, 3)], 0), ([(0, 3), (1, 2), (0, 0), (0, 0), (0, 1), (3, 1), (3, 2), (3, 3), (4, 2), (1, 2)], 0), ([(0, 3), (1, 2), (4, 2), (4, 3), (5, None)], 4), ([(0, 2), (0, 1), (3, 2), (3, 0), (0, 0), (0, 3), (1, 0), (2, 1), (5, None)], 8), ([(0, 1), (3, 3), (4, 3), (5, None)], 3)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5826
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.44481689  0.78890332  0.42399232]
True reward weights: [-0.65557811 -0.01374154  0.75500233]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5626
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.62523469  0.75812781  0.18526688]
True reward weights: [-0.78490952  0.25746669  0.5635849 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.127168

Running EBIRL with 3 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5676
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.6113558   0.17269236  0.77228326]
True reward weights: [-0.59243235  0.8035667  -0.05748456]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.126100

Running EBIRL with 4 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5586
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.03068067  0.94787215 -0.31717043]
True reward weights: [0.56543206 0.67702118 0.47109331]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.149803

Running EBIRL with 5 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5522
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.16236033  0.96867174 -0.1879207 ]
True reward weights: [0.33115839 0.94268667 0.04093857]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.204234

Running EBIRL with 6 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5606
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.30489742  0.87599596 -0.37372267]
True reward weights: [ 0.57408316  0.76553434 -0.29049216]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.274286

Running experiment 28/50...
Shuffled Demos: [([(0, 0), (0, 2), (0, 2), (0, 3), (1, 3), (2, 0), (2, 0), (2, 1), (5, None)], 8), ([(0, 2), (0, 3), (1, 3), (2, 2), (2, 1), (5, None)], 5), ([(0, 0), (1, 3), (2, 1), (2, 0), (2, 0), (2, 0), (2, 2), (1, 1), (4, 1), (4, 1)], 0), ([(0, 0), (0, 2), (0, 0), (0, 0), (0, 3), (1, 1), (4, 1), (4, 2), (3, 3), (4, 3)], 0), ([(0, 2), (0, 1), (0, 0), (0, 1), (3, 0), (0, 1), (3, 2), (3, 2), (3, 2), (3, 1)], 0), ([(0, 3), (0, 0), (0, 3), (1, 2), (0, 1), (3, 1), (3, 2), (3, 3), (4, 1), (4, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 28
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.18868402 -0.61215795 -0.76789387]
True reward weights: [-0.866301   -0.34967581  0.35672034]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 28
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.42659046  0.7864844   0.44661267]
True reward weights: [0.53776759 0.68273782 0.49464643]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.5874
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.35546968 0.79209082 0.49621915]
True reward weights: [ 0.13998691 -0.75141362 -0.64481101]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.251945

Running EBIRL with 4 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.5862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.28412227 0.95154955 0.11759248]
True reward weights: [-0.20410916  0.10247595  0.97356979]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.238222

Running EBIRL with 5 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.5930
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.34431113  0.84722607 -0.40454646]
True reward weights: [-0.55325869 -0.31782612  0.7699944 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.300636

Running EBIRL with 6 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.5830
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.43757583  0.85592991  0.27552021]
True reward weights: [-0.55414858  0.82581196 -0.10466117]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.295902

Saving results to files...
Results saved successfully.

Running experiment 29/50...
Shuffled Demos: [([(0, 2), (0, 0), (0, 3), (1, 1), (0, 3), (1, 2), (0, 1), (3, 0), (0, 1), (3, 2)], 0), ([(0, 0), (0, 1), (3, 2), (0, 0), (0, 2), (0, 1), (3, 3), (4, 2), (3, 2), (0, 0)], 0), ([(0, 2), (0, 0), (0, 0), (0, 1), (3, 1), (3, 0), (0, 1), (3, 0), (4, 2), (3, 2)], 0), ([(0, 0), (1, 0), (1, 1), (4, 0), (1, 1), (4, 3), (5, None)], 0), ([(0, 1), (3, 2), (0, 0), (0, 1), (3, 3), (4, 1), (4, 2), (1, 2), (1, 0), (1, 1)], 0), ([(0, 0), (0, 3), (3, 1), (3, 3), (4, 0), (1, 2), (4, 1), (4, 1), (4, 3), (5, None)], 9)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5788
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.22145614  0.43045705 -0.87502223]
True reward weights: [-0.6535586  -0.23072892  0.72085041]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running EBIRL with 2 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.53380911  0.82374899  0.1910116 ]
True reward weights: [-0.88303427  0.44276984 -0.15558064]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147621

Running EBIRL with 3 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5798
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.28179907  0.81933388  0.49928076]
True reward weights: [-0.84968097 -0.52649773  0.02902407]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.202542

Running EBIRL with 4 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5516
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.40923843  0.84556334 -0.34285063]
True reward weights: [-0.78841033 -0.37359361 -0.4887095 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.209606

Running EBIRL with 5 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5570
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.04415856 0.92290606 0.38248454]
True reward weights: [-0.83365743  0.38229526 -0.3985795 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.208303

Running EBIRL with 6 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5592
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.49159239 0.85366509 0.1720257 ]
True reward weights: [0.26711625 0.85467437 0.44517482]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.277624

Running experiment 30/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 2), (0, 3), (1, 3), (2, 0), (2, 2), (1, 3), (2, 2), (1, 1)], 0), ([(0, 1), (3, 3), (4, 0), (1, 0), (1, 1), (4, 3), (5, None)], 0), ([(0, 1), (3, 0), (0, 1), (3, 1), (3, 3), (4, 1), (3, 1), (4, 2), (3, 0), (0, 0)], 0), ([(0, 0), (0, 0), (0, 2), (0, 0), (0, 0), (0, 3), (1, 1), (4, 0), (5, None)], 0), ([(0, 1), (3, 1), (4, 1), (3, 0), (0, 3), (1, 0), (1, 1), (4, 2), (3, 2), (3, 2)], 0), ([(0, 3), (1, 1), (4, 2), (3, 2), (3, 0), (0, 0), (0, 3), (1, 2), (0, 1), (3, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5858
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.11974361 0.66067123 0.74106342]
True reward weights: [-0.81326386 -0.04441834  0.5801973 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5508
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.43956437 0.79828372 0.41173569]
True reward weights: [-0.8080496  -0.32654085 -0.49033348]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.130224

Running EBIRL with 3 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5446
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.21937434  0.97142082  0.09064488]
True reward weights: [0.27653602 0.87459657 0.39825704]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152054

Running EBIRL with 4 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5340
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.52848777 0.8456268  0.07493987]
True reward weights: [-0.25501016  0.00710111 -0.9669123 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.156283

Running EBIRL with 5 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5336
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.1398223   0.91610824 -0.37575979]
True reward weights: [-0.98622088 -0.10321972  0.12928286]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.154527

Running EBIRL with 6 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5308
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.37265379  0.90922525 -0.18557641]
True reward weights: [-0.3415739  -0.01574866 -0.93972296]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.153573

Saving results to files...
Results saved successfully.

Running experiment 31/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 1), (3, 0), (4, 1), (3, 3), (3, 2), (0, 0), (0, 1), (1, 0)], 0), ([(0, 3), (1, 2), (0, 2), (0, 3), (3, 1), (3, 1), (3, 0), (0, 2), (0, 3), (1, 1)], 0), ([(0, 0), (0, 2), (3, 1), (3, 1), (3, 0), (0, 3), (0, 1), (3, 3), (4, 2), (4, 3)], 0), ([(0, 2), (0, 1), (3, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 2), (0, 3), (0, 1)], 0), ([(0, 1), (3, 0), (0, 2), (0, 0), (0, 1), (3, 2), (3, 0), (0, 0), (0, 0), (0, 3)], 0), ([(0, 2), (0, 1), (3, 0), (0, 3), (0, 0), (0, 2), (3, 1), (3, 0), (0, 0), (0, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.5922
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.84390748  0.26578149 -0.46602614]
True reward weights: [-0.27534761 -0.19938474  0.94044108]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running EBIRL with 2 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.5912
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90143617  0.03258812 -0.43168373]
True reward weights: [-0.30172279  0.11093009  0.9469202 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123865

Running EBIRL with 3 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.5862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.15565979  0.81606668 -0.55660148]
True reward weights: [-0.1537059   0.75138303  0.64171492]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148038

Running EBIRL with 4 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.5834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.0954461   0.93468704 -0.34241813]
True reward weights: [-0.79132898 -0.53860658  0.28931192]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202861

Running EBIRL with 5 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.5872
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.32399644  0.94008698 -0.10612621]
True reward weights: [-0.9258923   0.36567017  0.09491451]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.273170

Running EBIRL with 6 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.5862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.03698913  0.95986443 -0.27801454]
True reward weights: [-0.24622404  0.44447795 -0.86128571]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.351209

Running experiment 32/50...
Shuffled Demos: [([(0, 2), (0, 3), (3, 1), (3, 1), (3, 0), (0, 2), (0, 2), (0, 0), (0, 2), (3, 1)], 0), ([(0, 1), (3, 3), (4, 1), (4, 2), (3, 0), (0, 3), (1, 2), (4, 2), (3, 3), (4, 0)], 0), ([(0, 1), (0, 1), (3, 0), (4, 1), (4, 2), (3, 3), (3, 2), (3, 2), (3, 0), (0, 1)], 0), ([(0, 3), (1, 1), (4, 2), (3, 1), (3, 0), (0, 2), (0, 0), (0, 3), (1, 1), (0, 3)], 0), ([(0, 2), (0, 1), (0, 2), (0, 1), (3, 2), (3, 0), (3, 2), (3, 1), (3, 3), (0, 0)], 0), ([(0, 0), (0, 3), (3, 1), (3, 1), (3, 2), (3, 2), (3, 2), (3, 2), (3, 1), (3, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 32
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.04821193  0.96723531 -0.24926183]
True reward weights: [-0.90753571 -0.40234227  0.1204144 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5916
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.02534915  0.85277732  0.52165915]
True reward weights: [0.74237965 0.63565643 0.21169165]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183720

Running EBIRL with 3 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5934
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.25155942  0.96019456 -0.12142594]
True reward weights: [-0.73186497  0.6267396  -0.26752782]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.225650

Running EBIRL with 4 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5884
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.53480468  0.81272675 -0.23121243]
True reward weights: [ 0.25090678  0.9067324  -0.33894268]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.219759

Running EBIRL with 5 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.14818301  0.91198067 -0.38253505]
True reward weights: [0.38431227 0.5165042  0.76519768]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.286450

Running EBIRL with 6 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5816
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.1317364   0.96326458 -0.23402323]
True reward weights: [-0.16345145  0.20635135 -0.96472936]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.362160

Saving results to files...
Results saved successfully.

Running experiment 33/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 3), (0, 2), (0, 2), (0, 0), (0, 2), (0, 3), (3, 2), (0, 2)], 0), ([(0, 3), (1, 1), (4, 0), (1, 0), (1, 3), (2, 1), (5, None)], 6), ([(0, 2), (0, 3), (1, 0), (0, 3), (3, 1), (3, 1), (3, 0), (0, 2), (3, 3), (4, 2)], 0), ([(0, 3), (1, 3), (1, 2), (0, 3), (3, 0), (0, 2), (0, 1), (0, 1), (0, 3), (1, 3)], 0), ([(0, 3), (1, 0), (1, 1), (4, 0), (1, 1), (4, 1), (5, None)], 6), ([(0, 2), (0, 1), (3, 3), (4, 0), (5, None)], 4)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 33
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.5465349  -0.80526318  0.22989306]
True reward weights: [-0.32556312 -0.30396656  0.89532842]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 33
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.16035606 0.87755882 0.45185888]
True reward weights: [-0.45224471  0.68930065  0.56598528]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.5812
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.14177826  0.98881933 -0.04620883]
True reward weights: [ 0.12873018  0.30012085 -0.94517512]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.253314

Running EBIRL with 4 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.5932
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.0377275  0.92798434 0.37070433]
True reward weights: [-0.01859901  0.87043015  0.49194048]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.238969

Running EBIRL with 5 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.5794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.23402506  0.8811596   0.41084064]
True reward weights: [-0.62747759 -0.17218072  0.75935873]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.301186

Running EBIRL with 6 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.5800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.1626202   0.98115938  0.10431178]
True reward weights: [ 0.32234381  0.81193722 -0.48667485]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.374056

Running experiment 34/50...
Shuffled Demos: [([(0, 0), (1, 1), (4, 0), (1, 1), (4, 3), (5, None)], 0), ([(0, 3), (1, 1), (4, 0), (1, 0), (1, 2), (0, 0), (0, 3), (1, 0), (1, 3), (2, 1)], 0), ([(0, 3), (1, 2), (0, 2), (0, 0), (0, 1), (1, 2), (1, 0), (1, 2), (0, 2), (0, 0)], 0), ([(0, 0), (1, 0), (1, 3), (1, 3), (2, 2), (1, 2), (4, 2), (1, 3), (2, 3), (5, None)], 0), ([(0, 3), (1, 0), (1, 0), (2, 2), (1, 0), (1, 2), (0, 1), (3, 0), (0, 1), (3, 3)], 0), ([(0, 2), (3, 1), (3, 2), (3, 2), (0, 2), (0, 2), (0, 2), (0, 1), (3, 3), (4, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.96656386  0.14527457  0.21130453]
True reward weights: [-0.839382   -0.393236    0.37523766]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5494
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.07154753  0.98530741 -0.15508143]
True reward weights: [0.46006193 0.79280345 0.39975707]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.130559

Running EBIRL with 3 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5360
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.91900472 -0.32421454 -0.22431064]
True reward weights: [-0.02172628  0.85312053 -0.52126129]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.128293

Running EBIRL with 4 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5536
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.55187127  0.67882008  0.48439798]
True reward weights: [-0.51317201  0.78029332 -0.35748682]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.127061

Running EBIRL with 5 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5560
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.86045787  0.28673914 -0.42118038]
True reward weights: [-0.90607425 -0.35573895  0.22908352]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.126619

Running EBIRL with 6 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5472
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.10770039  0.98607157  0.12674183]
True reward weights: [-0.63476503 -0.023913   -0.77233511]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.149985

Saving results to files...
Results saved successfully.

Running experiment 35/50...
Shuffled Demos: [([(0, 0), (0, 2), (0, 1), (3, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 3), (2, 0)], 0), ([(0, 0), (0, 3), (3, 2), (0, 3), (1, 1), (4, 1), (4, 1), (4, 3), (5, None)], 0), ([(0, 0), (0, 0), (0, 0), (0, 1), (3, 3), (4, 2), (3, 2), (3, 1), (4, 1), (4, 1)], 6), ([(0, 0), (0, 2), (0, 1), (3, 2), (3, 2), (0, 0), (0, 2), (0, 1), (3, 0), (3, 3)], 6), ([(0, 2), (0, 2), (3, 3), (3, 3), (4, 3), (5, None)], 5), ([(0, 3), (1, 1), (4, 2), (3, 0), (0, 0), (0, 2), (0, 0), (0, 1), (3, 0), (0, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5860
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.16851079  0.62450343 -0.76262676]
True reward weights: [-0.94121751 -0.05112011  0.33391067]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5268
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.08550288  0.89764974  0.43233575]
True reward weights: [-0.94957849  0.31089514 -0.04055737]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.133968

Running EBIRL with 3 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5270
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.29711627  0.72560912  0.62065556]
True reward weights: [-0.9571097  -0.2175488   0.19134667]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154837

Running EBIRL with 4 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5164
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.30113324  0.92408635  0.23533634]
True reward weights: [-0.9945342  -0.09196196  0.04944423]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.207988

Running EBIRL with 5 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5214
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.2642724   0.90385418 -0.33646355]
True reward weights: [0.07879753 0.94043698 0.33071021]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.277329

Running EBIRL with 6 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5334
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.26202678  0.87997671  0.39621075]
True reward weights: [-0.42250339  0.89530641 -0.1411287 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.276851

Running experiment 36/50...
Shuffled Demos: [([(0, 2), (0, 0), (0, 0), (1, 2), (0, 1), (0, 0), (0, 3), (1, 1), (4, 1), (4, 2)], 0), ([(0, 0), (0, 0), (0, 1), (3, 1), (3, 1), (3, 2), (3, 3), (4, 0), (1, 2), (0, 2)], 0), ([(0, 0), (0, 1), (1, 0), (1, 3), (4, 1), (4, 0), (1, 0), (1, 3), (2, 3), (2, 0)], 0), ([(0, 2), (0, 3), (1, 3), (4, 1), (3, 1), (3, 3), (4, 3), (4, 1), (4, 1), (4, 3)], 0), ([(0, 3), (1, 1), (4, 0), (1, 1), (4, 2), (3, 0), (0, 2), (0, 3), (1, 2), (0, 3)], 0), ([(0, 0), (0, 3), (1, 2), (0, 0), (0, 2), (0, 3), (1, 1), (4, 2), (3, 0), (0, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.94515114  0.30501368  0.1168588 ]
True reward weights: [-0.05456508 -0.040557    0.99768621]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running EBIRL with 2 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.16882161  0.9834698   0.06547065]
True reward weights: [-0.95383472 -0.15598286 -0.25664895]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123956

Running EBIRL with 3 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5746
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.41409323  0.86254053  0.29077591]
True reward weights: [-0.93358385 -0.31759649 -0.16599295]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123879

Running EBIRL with 4 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.46829801  0.39475703  0.79048331]
True reward weights: [-0.89917477 -0.4349619  -0.04788413]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.124046

Running EBIRL with 5 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.97994767 -0.19855874 -0.01664326]
True reward weights: [-0.15551096  0.18358874 -0.97062429]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.123877

Running EBIRL with 6 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.31709068  0.51493452  0.79642699]
True reward weights: [ 0.36906392  0.75154775 -0.54677948]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.123985

Saving results to files...
Results saved successfully.

Running experiment 37/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], 3), ([(0, 0), (0, 0), (0, 1), (3, 0), (0, 0), (0, 0), (0, 0), (0, 3), (1, 3), (4, 2)], 0), ([(0, 1), (3, 2), (3, 3), (4, 0), (1, 0), (2, 0), (2, 2), (5, None)], 7), ([(0, 0), (0, 2), (0, 3), (1, 0), (1, 2), (0, 3), (1, 2), (0, 1), (3, 1), (3, 0)], 0), ([(0, 3), (1, 2), (0, 3), (3, 1), (3, 0), (0, 0), (0, 2), (0, 0), (0, 3), (1, 1)], 0), ([(0, 1), (3, 2), (3, 2), (3, 1), (3, 3), (4, 0), (1, 0), (1, 1), (4, 2), (3, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 37
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.44666171 0.64488343 0.62017633]
True reward weights: [-0.75478167 -0.33513042  0.563908  ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.5880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.09884259 0.58451446 0.80534029]
True reward weights: [-0.70496276 -0.10347087 -0.7016561 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.184868

Running EBIRL with 3 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.5762
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.32279684  0.93360104 -0.15553554]
True reward weights: [0.09020899 0.08712281 0.99210481]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.226312

Running EBIRL with 4 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.5670
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.19890373  0.9418661  -0.27078691]
True reward weights: [-0.06366039  0.90052488  0.43011893]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.220263

Running EBIRL with 5 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.5892
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.36253089  0.9297142  -0.06482945]
True reward weights: [-0.37127269  0.66104365 -0.65205666]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.216671

Running EBIRL with 6 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.5874
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.39716293  0.90426112 -0.15675916]
True reward weights: [-0.97032232  0.22540888  0.08755245]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.214262

Running experiment 38/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 0), (5, None)], 3), ([(0, 2), (0, 0), (0, 2), (0, 0), (0, 1), (3, 2), (3, 2), (3, 0), (0, 0), (0, 3)], 1), ([(0, 3), (1, 1), (4, 3), (5, None)], 3), ([(0, 2), (0, 3), (1, 3), (2, 3), (2, 1), (1, 2), (0, 2), (0, 2), (0, 0), (0, 3)], 0), ([(0, 0), (0, 1), (1, 1), (2, 2), (1, 0), (1, 2), (0, 2), (0, 2), (0, 0), (0, 2)], 0), ([(0, 2), (0, 3), (1, 2), (0, 2), (0, 3), (0, 2), (0, 3), (1, 3), (1, 0), (1, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 38
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.55878534  0.82529557  0.08152397]
True reward weights: [-0.49248773 -0.22865786  0.83974486]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 38
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.23690342 -0.90312256 -0.35811507]
True reward weights: [-0.42500184  0.89645469  0.12546882]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 38
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.60763596 -0.66986287  0.42668757]
True reward weights: [ 0.66756368 -0.22936649 -0.70834296]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.5752
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.04840554 0.96167079 0.26990033]
True reward weights: [ 0.11709637 -0.96757314 -0.22380941]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.329852

Running EBIRL with 5 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.5796
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.3001878   0.8354344   0.46036577]
True reward weights: [-0.07437564  0.55685845  0.82727077]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.316376

Running EBIRL with 6 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.5952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.50944758  0.86008933 -0.02663668]
True reward weights: [-0.0070016   0.96781895 -0.2515501 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.308004

Saving results to files...
Results saved successfully.

Running experiment 39/50...
Shuffled Demos: [([(0, 3), (1, 0), (1, 1), (2, 3), (2, 3), (2, 3), (2, 2), (1, 0), (1, 2), (1, 3)], 0), ([(0, 2), (0, 0), (0, 1), (0, 2), (0, 1), (3, 3), (4, 1), (4, 3), (5, None)], 8), ([(0, 3), (1, 1), (4, 3), (5, None)], 3), ([(0, 3), (1, 0), (1, 1), (4, 2), (3, 0), (0, 3), (1, 2), (0, 3), (1, 1), (4, 1)], 0), ([(0, 2), (0, 1), (3, 1), (4, 0), (1, 1), (0, 3), (1, 3), (4, 1), (4, 2), (3, 3)], 0), ([(0, 2), (0, 1), (3, 2), (3, 0), (3, 2), (3, 2), (3, 1), (3, 2), (3, 2), (3, 3)], 7)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.5852
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.85313816 -0.39857546  0.33659008]
True reward weights: [-0.56481614 -0.46100063  0.68444222]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running EBIRL with 2 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.5802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.22236387  0.92461175  0.30926916]
True reward weights: [-0.97389497  0.2019839   0.10359096]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147666

Running EBIRL with 3 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.5870
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.74069957  0.6287659   0.23668035]
True reward weights: [-0.91196057 -0.17128148 -0.3728144 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.202554

Running EBIRL with 4 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.5864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.22784991  0.91631248 -0.32932638]
True reward weights: [-0.24603806  0.80165427  0.54480795]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202619

Running EBIRL with 5 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.5848
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.70844976  0.70288702 -0.06362999]
True reward weights: [-0.03202773  0.09752403  0.99471769]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.202739

Running EBIRL with 6 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.5888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.23742636  0.9115435   0.33573378]
True reward weights: [ 0.00358858  0.39447396 -0.91890011]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.273012

Running experiment 40/50...
Shuffled Demos: [([(0, 3), (1, 1), (4, 2), (3, 3), (0, 0), (0, 3), (1, 2), (0, 1), (1, 0), (1, 1)], 0), ([(0, 0), (0, 3), (0, 2), (0, 2), (0, 0), (0, 1), (3, 0), (0, 3), (1, 0), (1, 2)], 0), ([(0, 0), (0, 1), (3, 2), (3, 2), (3, 3), (4, 0), (5, None)], 6), ([(0, 2), (0, 1), (3, 0), (4, 3), (5, None)], 4), ([(0, 1), (3, 2), (3, 1), (3, 3), (4, 2), (3, 3), (4, 1), (4, 3), (5, None)], 8), ([(0, 2), (0, 1), (0, 3), (1, 0), (1, 1), (4, 0), (1, 2), (0, 2), (0, 2), (0, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.5902
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.89743649  0.29748142  0.32574921]
True reward weights: [-0.6667631  -0.57474284  0.47444455]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.5754
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.41086953 -0.08367258  0.90784642]
True reward weights: [-0.643003   -0.53978435 -0.5433047 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123761

Running EBIRL with 3 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.5904
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.01547197  0.87947091  0.47570111]
True reward weights: [-0.5072488   0.5272155  -0.68172023]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.147737

Running EBIRL with 4 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.5750
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.01157543 0.83693106 0.54718591]
True reward weights: [-0.29594776  0.51489734 -0.80454686]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202601

Running EBIRL with 5 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.5898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.34063807  0.79309202  0.50494629]
True reward weights: [-0.53754488 -0.06573283  0.84066919]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.272865

Running EBIRL with 6 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.5686
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.34199222 0.84686754 0.40725507]
True reward weights: [ 0.16166474  0.62515519 -0.76357416]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.272932

Saving results to files...
Results saved successfully.

Running experiment 41/50...
Shuffled Demos: [([(0, 0), (1, 3), (2, 3), (2, 0), (2, 1), (5, None)], 5), ([(0, 1), (3, 1), (3, 3), (4, 2), (3, 1), (3, 0), (0, 3), (1, 1), (2, 0), (1, 0)], 0), ([(0, 0), (0, 0), (1, 2), (0, 3), (1, 1), (4, 0), (1, 0), (1, 2), (0, 2), (3, 1)], 0), ([(0, 1), (3, 1), (3, 0), (4, 3), (4, 2), (1, 3), (2, 2), (2, 1), (5, None)], 8), ([(0, 3), (1, 0), (2, 1), (2, 0), (2, 2), (1, 3), (2, 1), (5, None)], 7), ([(0, 2), (0, 3), (1, 2), (0, 3), (1, 2), (0, 1), (3, 2), (3, 2), (3, 1), (3, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 41
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.55293672  0.64597737  0.5262834 ]
True reward weights: [-0.46247666 -0.0267168   0.88622884]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.22346654  0.57285493 -0.78860632]
True reward weights: [-0.03875153  0.67312633 -0.73851152]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183279

Running EBIRL with 3 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.8816288   0.28476653  0.37634915]
True reward weights: [-0.77773683  0.62749228  0.03713316]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170290

Running EBIRL with 4 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.3648462   0.78606403 -0.49898958]
True reward weights: [-0.69405387  0.53229808  0.48471433]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.219086

Running EBIRL with 5 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.27094224 0.94410341 0.18777396]
True reward weights: [-0.25868247  0.965836   -0.01562709]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.285855

Running EBIRL with 6 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5840
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.51489431 0.85325937 0.08265772]
True reward weights: [-0.24159145  0.63367957  0.73490392]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.283677

Running experiment 42/50...
Shuffled Demos: [([(0, 3), (1, 3), (2, 3), (2, 0), (1, 2), (0, 3), (1, 2), (0, 1), (3, 1), (4, 2)], 0), ([(0, 1), (3, 2), (3, 3), (0, 2), (0, 3), (0, 0), (0, 3), (1, 0), (1, 2), (0, 1)], 0), ([(0, 2), (3, 3), (0, 3), (1, 1), (4, 3), (1, 0), (1, 3), (2, 1), (5, None)], 8), ([(0, 1), (3, 1), (3, 1), (3, 3), (4, 2), (3, 0), (0, 2), (0, 3), (1, 1), (4, 0)], 0), ([(0, 0), (0, 1), (3, 3), (4, 2), (3, 3), (4, 2), (3, 1), (3, 0), (0, 2), (0, 3)], 0), ([(0, 3), (1, 2), (0, 1), (3, 1), (3, 2), (3, 3), (4, 2), (3, 3), (4, 0), (1, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5832
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.71511889  0.43259087 -0.54906294]
True reward weights: [-0.71505276 -0.68463496  0.14133129]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5798
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.34943819  0.12413286 -0.92870016]
True reward weights: [-0.35476081 -0.10038374 -0.92955251]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123675

Running EBIRL with 3 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5960
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.43713348 0.7963082  0.41809997]
True reward weights: [0.40195151 0.71226954 0.57541905]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.147789

Running EBIRL with 4 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5770
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.28612044 0.94356892 0.16677166]
True reward weights: [-0.11110142 -0.07830562 -0.99071929]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.148163

Running EBIRL with 5 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5832
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.05384048 0.97851081 0.19904218]
True reward weights: [-0.68896514  0.29895383  0.66026786]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.203010

Running EBIRL with 6 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5806
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.41879585 0.80648628 0.41736066]
True reward weights: [-0.02641096  0.57382749  0.81855023]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.203071

Saving results to files...
Results saved successfully.

Running experiment 43/50...
Shuffled Demos: [([(0, 2), (0, 2), (0, 0), (0, 3), (1, 0), (2, 2), (5, None)], 0), ([(0, 2), (0, 0), (0, 3), (1, 2), (0, 3), (1, 3), (1, 3), (2, 2), (5, None)], 0), ([(0, 1), (3, 1), (3, 2), (3, 1), (3, 3), (4, 3), (5, None)], 6), ([(0, 2), (0, 0), (0, 1), (3, 3), (4, 0), (1, 3), (2, 2), (1, 1), (4, 2), (4, 3)], 0), ([(0, 3), (1, 1), (4, 3), (5, None)], 0), ([(0, 2), (0, 2), (0, 0), (0, 0), (0, 2), (0, 2), (3, 3), (4, 1), (5, None)], 8)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.25134449  0.96408191  0.08586042]
True reward weights: [-0.94655708 -0.29794542  0.12352418]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5478
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.65582116  0.69210897 -0.30146937]
True reward weights: [ 0.14908472  0.98823989 -0.033995  ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.129589

Running EBIRL with 3 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5538
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.08808861  0.97182248  0.21863548]
True reward weights: [ 0.47801909  0.4839688  -0.73298837]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151720

Running EBIRL with 4 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5212
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.4673127   0.88264297  0.0505986 ]
True reward weights: [ 0.47276928  0.71400279 -0.51641961]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.158493

Running EBIRL with 5 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5148
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.10037032  0.82897952  0.55019884]
True reward weights: [ 0.42140605  0.87879169 -0.22392432]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.156471

Running EBIRL with 6 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5318
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.07220392  0.98371462 -0.16459692]
True reward weights: [-0.55262293  0.43802655  0.70904206]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.209900

Running experiment 44/50...
Shuffled Demos: [([(0, 1), (3, 2), (3, 0), (0, 1), (3, 3), (4, 3), (4, 2), (3, 1), (3, 2), (3, 2)], 0), ([(0, 1), (0, 2), (0, 1), (3, 0), (0, 1), (3, 1), (3, 3), (4, 1), (4, 3), (5, None)], 9), ([(0, 1), (0, 2), (0, 1), (3, 2), (3, 0), (0, 2), (0, 1), (3, 3), (4, 1), (4, 3)], 0), ([(0, 0), (0, 0), (0, 3), (1, 1), (4, 0), (3, 2), (3, 0), (0, 3), (1, 2), (0, 3)], 0), ([(0, 2), (0, 0), (0, 1), (1, 1), (4, 2), (3, 1), (3, 3), (4, 3), (4, 3), (4, 0)], 0), ([(0, 3), (1, 2), (0, 3), (1, 0), (1, 2), (0, 1), (3, 2), (3, 0), (0, 2), (0, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 44
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.16983605 -0.98117526  0.09192833]
True reward weights: [-0.88315502 -0.32822573  0.33511952]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 44
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.537885   -0.80770787 -0.24142852]
True reward weights: [ 0.88775531 -0.35070734  0.29815241]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 44
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.10812153 -0.85449796  0.50807772]
True reward weights: [0.30345881 0.40749355 0.86131397]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.2717911   0.89387404  0.35653723]
True reward weights: [ 0.0748785  -0.83891559  0.53908612]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.330686

Running EBIRL with 5 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5876
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.02301107  0.97188243  0.23433957]
True reward weights: [-0.35133399  0.13951763  0.92579655]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.317259

Running EBIRL with 6 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5822
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.06137801  0.99801737  0.0139305 ]
True reward weights: [-0.34146649 -0.10809028  0.93365793]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.308807

Saving results to files...
Results saved successfully.

Running experiment 45/50...
Shuffled Demos: [([(0, 1), (1, 1), (4, 0), (1, 2), (0, 1), (3, 1), (3, 1), (3, 2), (3, 3), (3, 3)], 0), ([(0, 0), (1, 3), (2, 0), (2, 1), (1, 1), (0, 1), (3, 0), (0, 1), (3, 2), (3, 0)], 0), ([(0, 1), (3, 1), (3, 2), (0, 2), (0, 2), (0, 2), (0, 3), (1, 3), (2, 2), (1, 3)], 0), ([(0, 0), (0, 2), (0, 2), (0, 1), (3, 2), (0, 1), (3, 0), (0, 3), (1, 3), (2, 2)], 0), ([(0, 2), (0, 0), (0, 3), (1, 1), (0, 2), (0, 1), (3, 0), (0, 0), (0, 3), (1, 1)], 0), ([(0, 0), (0, 2), (0, 0), (0, 2), (0, 3), (1, 3), (1, 0), (1, 0), (1, 2), (0, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.5866
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.9183874   0.03309631 -0.39429586]
True reward weights: [-0.83381491 -0.37890594  0.40147601]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.5960
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.07122069  0.72227017 -0.68793417]
True reward weights: [-0.71749835  0.68741337 -0.11251216]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123585

Running EBIRL with 3 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.5714
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.38993233  0.90473945  0.17146225]
True reward weights: [ 0.3974078   0.72659171 -0.56047438]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123804

Running EBIRL with 4 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.5864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.13067503  0.58562595 -0.79997893]
True reward weights: [-0.45991119  0.61201637 -0.64336433]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.123943

Running EBIRL with 5 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.5864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.58297373  0.00092525  0.81249048]
True reward weights: [-0.73484773  0.17874937  0.65425337]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.123822

Running EBIRL with 6 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.5862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.78821528 -0.07787601  0.61045229]
True reward weights: [-0.85460857  0.50879699  0.10377771]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.123877

Running experiment 46/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 1), (4, 1), (4, 2), (3, 2), (3, 0), (3, 2), (3, 2), (3, 2)], 2), ([(0, 0), (0, 3), (1, 2), (0, 1), (3, 3), (4, 2), (1, 3), (2, 3), (2, 1), (2, 1)], 0), ([(0, 3), (0, 0), (0, 3), (1, 2), (0, 1), (3, 3), (4, 2), (3, 3), (4, 2), (3, 1)], 0), ([(0, 3), (1, 2), (0, 3), (1, 2), (0, 3), (1, 1), (4, 3), (5, None)], 0), ([(0, 3), (0, 3), (1, 0), (1, 3), (1, 3), (2, 2), (5, None)], 0), ([(0, 3), (0, 3), (1, 0), (1, 3), (4, 1), (5, None)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 46
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.11371465 -0.90062209 -0.41946255]
True reward weights: [-0.82098277 -0.18440172  0.54035479]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5886
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.07957533  0.86152966 -0.50143237]
True reward weights: [-0.0173729   0.05006751 -0.99859473]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183033

Running EBIRL with 3 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5806
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.18229585  0.980555   -0.07266436]
True reward weights: [-0.29768342  0.94418904  0.14103768]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170359

Running EBIRL with 4 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5548
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.0997371   0.91607256 -0.38841161]
True reward weights: [-0.8997309   0.43428511 -0.04336764]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.171123

Running EBIRL with 5 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5436
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.34662721  0.82921006  0.43847492]
True reward weights: [-0.66062897 -0.45323519 -0.59845403]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.166256

Running EBIRL with 6 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5482
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.10904663  0.97580297 -0.18951883]
True reward weights: [0.39767629 0.66087175 0.63647631]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.165953

Saving results to files...
Results saved successfully.

Running experiment 47/50...
Shuffled Demos: [([(0, 0), (0, 3), (0, 1), (3, 3), (4, 3), (4, 2), (3, 1), (3, 0), (0, 3), (0, 2)], 0), ([(0, 2), (0, 0), (0, 1), (1, 1), (2, 2), (1, 2), (1, 1), (2, 0), (2, 1), (5, None)], 0), ([(0, 2), (0, 1), (3, 1), (3, 3), (4, 2), (3, 2), (3, 3), (4, 0), (1, 1), (2, 2)], 0), ([(0, 2), (0, 2), (0, 1), (3, 1), (3, 0), (0, 1), (0, 3), (1, 2), (0, 1), (3, 2)], 0), ([(0, 3), (1, 1), (4, 0), (1, 1), (2, 2), (5, None)], 0), ([(0, 2), (0, 0), (0, 1), (3, 0), (3, 3), (4, 3), (5, None)], 6)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 47
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.45436833 0.88591604 0.09328555]
True reward weights: [-0.73857451 -0.23755048  0.63093381]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5924
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.33053991  0.94372009 -0.01165185]
True reward weights: [0.50340296 0.86405001 0.00174234]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183692

Running EBIRL with 3 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5626
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.32102245 0.82737825 0.46085771]
True reward weights: [-0.83397724 -0.54749174 -0.0688096 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.176904

Running EBIRL with 4 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5582
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.28339331 0.80942447 0.51431532]
True reward weights: [-0.68846616  0.35023403  0.63509879]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.169112

Running EBIRL with 5 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5502
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.17184085  0.97918775 -0.1079911 ]
True reward weights: [-0.07227262  0.32490338  0.94298169]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.167279

Running EBIRL with 6 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5498
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.1768194   0.91567656 -0.36093121]
True reward weights: [-0.58250161  0.49646924 -0.64359162]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.218776

Running experiment 48/50...
Shuffled Demos: [([(0, 3), (1, 3), (2, 1), (5, None)], 0), ([(0, 0), (0, 0), (0, 0), (0, 2), (0, 2), (0, 2), (0, 1), (3, 3), (4, 2), (3, 2)], 0), ([(0, 2), (3, 1), (3, 3), (4, 3), (4, 3), (5, None)], 5), ([(0, 2), (0, 1), (3, 2), (3, 1), (3, 2), (3, 3), (4, 2), (3, 3), (0, 1), (3, 0)], 0), ([(0, 1), (3, 1), (3, 0), (3, 1), (3, 0), (0, 2), (0, 2), (0, 1), (3, 1), (3, 1)], 0), ([(0, 1), (0, 1), (3, 1), (3, 3), (4, 0), (1, 1), (4, 1), (4, 2), (4, 3), (5, None)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.37472532  0.61847636 -0.69070104]
True reward weights: [-0.95541883 -0.18852986  0.22722531]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5924
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.35388777 0.87900096 0.31956339]
True reward weights: [0.86353481 0.50189654 0.04906632]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147820

Running EBIRL with 3 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.5514655  0.83131909 0.06924147]
True reward weights: [ 0.17317773  0.98413227 -0.0386413 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.202761

Running EBIRL with 4 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.23624946  0.84728674  0.47570093]
True reward weights: [0.61093992 0.74390502 0.27084633]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.273113

Running EBIRL with 5 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5698
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.0759073   0.90195116 -0.42511433]
True reward weights: [-0.66701088  0.68565395 -0.29150498]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.351183

Running EBIRL with 6 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.11443268  0.97838275  0.17225665]
True reward weights: [-0.35069806  0.75427452 -0.55505029]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.351443

Saving results to files...
Results saved successfully.

Running experiment 49/50...
Shuffled Demos: [([(0, 0), (0, 2), (0, 0), (0, 2), (0, 3), (1, 2), (0, 1), (3, 3), (4, 0), (1, 0)], 0), ([(0, 1), (3, 1), (3, 2), (3, 1), (3, 0), (0, 3), (1, 3), (2, 1), (5, None)], 0), ([(0, 0), (0, 2), (0, 0), (0, 2), (0, 1), (3, 1), (3, 3), (4, 3), (5, None)], 8), ([(0, 1), (3, 1), (3, 2), (3, 3), (4, 1), (4, 0), (1, 2), (0, 1), (1, 3), (2, 3)], 0), ([(0, 2), (0, 1), (1, 0), (1, 3), (2, 1), (1, 2), (0, 3), (1, 1), (4, 1), (5, None)], 0), ([(0, 1), (3, 1), (3, 3), (4, 0), (1, 3), (2, 2), (5, None)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5830
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.56408796  0.66599071 -0.48812001]
True reward weights: [-0.98484463 -0.1489005   0.08893646]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running EBIRL with 2 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.93339208 -0.02541343  0.35795723]
True reward weights: [-0.73503463 -0.66449826  0.13478184]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.132715

Running EBIRL with 3 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5194
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.64917178  0.74124479 -0.17068148]
True reward weights: [-0.7901069   0.27511472  0.54776178]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.153657

Running EBIRL with 4 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5304
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.31746027  0.73698967 -0.59671199]
True reward weights: [-0.92783627 -0.29009006 -0.23445175]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.152521

Running EBIRL with 5 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5314
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.1367401   0.85151961 -0.50617832]
True reward weights: [-0.7676069   0.32450397  0.55269957]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.151561

Running EBIRL with 6 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5346
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.02272721  0.97815123 -0.20664858]
True reward weights: [-0.33625352  0.90737047  0.25221497]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.151056

Running experiment 50/50...
Shuffled Demos: [([(0, 2), (0, 0), (1, 3), (2, 3), (2, 2), (1, 2), (0, 3), (1, 3), (2, 2), (1, 2)], 0), ([(0, 3), (1, 1), (4, 2), (3, 3), (4, 0), (1, 0), (1, 0), (1, 1), (4, 0), (1, 1)], 0), ([(0, 3), (3, 3), (4, 3), (5, None)], 3), ([(0, 1), (3, 2), (3, 0), (3, 1), (3, 3), (3, 1), (3, 3), (3, 2), (3, 2), (3, 3)], 0), ([(0, 2), (0, 2), (0, 2), (0, 3), (1, 1), (4, 2), (4, 0), (1, 1), (4, 1), (4, 2)], 0), ([(0, 1), (3, 2), (3, 3), (4, 3), (5, None)], 4)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.5814
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.08216978 0.67346567 0.73463741]
True reward weights: [-0.47840873 -0.41855607  0.77196885]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running EBIRL with 2 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.5934
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.86697932 -0.29703426  0.40014685]
True reward weights: [0.01471196 0.9878529  0.15469389]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123692

Running EBIRL with 3 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.5860
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.48780546  0.87103198 -0.05787163]
True reward weights: [-0.70224211 -0.47255487 -0.53249218]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.147684

Running EBIRL with 4 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.5894
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.12382575  0.93056265 -0.34455818]
True reward weights: [0.10356448 0.99328551 0.0515587 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202537

Running EBIRL with 5 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.5850
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.44013568  0.8885717   0.1293094 ]
True reward weights: [-0.34744255  0.62496828  0.69906962]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.202705

Running EBIRL with 6 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.5750
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.27263537 0.96211355 0.00273297]
True reward weights: [-0.98305109  0.16458911  0.08075255]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.272964

Saving results to files...
Results saved successfully.
Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [([(0, 3), (1, 3), (2, 1), (5, None)], 0), ([(0, 1), (3, 0), (0, 0), (0, 0), (1, 2), (0, 0), (0, 1), (3, 2), (3, 2), (3, 3)], 0), ([(0, 3), (1, 1), (4, 3), (5, None)], 0), ([(0, 0), (0, 2), (3, 1), (3, 2), (3, 0), (0, 2), (0, 1), (3, 1), (3, 0), (0, 3)], 0), ([(0, 1), (3, 3), (4, 2), (3, 0), (0, 1), (3, 2), (3, 0), (0, 2), (0, 2), (0, 3)], 0), ([(0, 1), (0, 1), (3, 0), (0, 0), (1, 0), (1, 3), (2, 3), (2, 1), (5, None)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6026
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.97397406  0.1203794   0.19205032]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5330
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.24144488  0.73988229 -0.62791606]
True reward weights: [0.36239313 0.78365202 0.50454012]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.134495

Running EBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5406
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.63595981  0.51034181  0.57888371]
True reward weights: [ 0.33183069  0.87605545 -0.3498789 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.130639

Running EBIRL with 4 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5550
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.45336422  0.89119067 -0.01549444]
True reward weights: [-0.73277609  0.67709679  0.06766932]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.153602

Running EBIRL with 5 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.102005    0.98496995 -0.13938857]
True reward weights: [-0.50045096  0.76706795 -0.4014419 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.207482

Running EBIRL with 6 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5492
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.24611421  0.96713564  0.06384715]
True reward weights: [-0.88098784 -0.14045389  0.45181094]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.206672

Running experiment 2/50...
Shuffled Demos: [([(0, 1), (3, 2), (3, 1), (3, 2), (3, 3), (4, 3), (5, None)], 6), ([(0, 0), (1, 3), (2, 2), (1, 3), (2, 2), (1, 1), (4, 1), (4, 2), (3, 2), (3, 1)], 0), ([(0, 2), (0, 2), (0, 1), (3, 2), (3, 3), (3, 2), (3, 0), (0, 0), (0, 3), (1, 2)], 0), ([(0, 3), (0, 1), (1, 0), (1, 1), (4, 2), (3, 1), (4, 1), (4, 1), (4, 2), (3, 1)], 0), ([(0, 2), (0, 0), (0, 2), (0, 3), (0, 0), (0, 3), (1, 2), (0, 0), (0, 1), (3, 0)], 0), ([(0, 2), (0, 3), (0, 1), (0, 3), (1, 0), (1, 0), (1, 1), (4, 3), (4, 1), (4, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.27851231 -0.96043214 -0.00099493]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6052
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.03621347  0.84901007  0.52713422]
True reward weights: [ 0.6073611  -0.05950304 -0.79219435]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182156

Running EBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.04749716  0.97706102 -0.20759522]
True reward weights: [-0.07889484  0.63675799 -0.76701686]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.169864

Running EBIRL with 4 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.03821501  0.95099499  0.30683568]
True reward weights: [0.16792279 0.37036462 0.91358195]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.164021

Running EBIRL with 5 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5914
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.30612076  0.94707342 -0.09665415]
True reward weights: [-0.78489467  0.28145103  0.55201964]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.160795

Running EBIRL with 6 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5978
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.26907365  0.94301005 -0.19578408]
True reward weights: [-0.52048635 -0.1629883   0.8381699 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.158603

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 0), (0, 0), (0, 3), (1, 1), (4, 2), (3, 3), (4, 0), (3, 3)], 0), ([(0, 0), (0, 3), (1, 0), (1, 1), (2, 1), (5, None)], 5), ([(0, 0), (0, 2), (0, 2), (0, 0), (0, 3), (1, 2), (1, 3), (2, 3), (2, 0), (2, 3)], 0), ([(0, 0), (0, 2), (0, 2), (0, 0), (0, 1), (0, 2), (0, 2), (0, 2), (0, 1), (3, 3)], 0), ([(0, 0), (0, 1), (3, 0), (0, 3), (1, 0), (1, 1), (4, 0), (1, 0), (1, 0), (1, 1)], 0), ([(0, 1), (3, 3), (0, 3), (1, 3), (2, 0), (2, 2), (1, 1), (4, 0), (1, 1), (4, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6118
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.16265824  0.85315911  0.49564285]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5980
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.05078609  0.64535118 -0.76219592]
True reward weights: [-0.67018126 -0.5599221  -0.48717996]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147820

Running EBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6098
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.11387249  0.90993801 -0.39880556]
True reward weights: [-0.24783975  0.05633398 -0.9671618 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.147981

Running EBIRL with 4 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5972
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.36941013 0.88178414 0.29324542]
True reward weights: [0.5046462  0.8632535  0.01120787]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202817

Running EBIRL with 5 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5998
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.03587917  0.95005025  0.31002774]
True reward weights: [0.1549677  0.81488689 0.5585198 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.202834

Running EBIRL with 6 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.20628582  0.95261806 -0.22352849]
True reward weights: [-0.01640724  0.99683111  0.07783665]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.202856

Running experiment 4/50...
Shuffled Demos: [([(0, 3), (1, 0), (2, 2), (5, None)], 0), ([(0, 2), (0, 0), (0, 1), (3, 3), (4, 1), (4, 3), (5, None)], 6), ([(0, 1), (3, 1), (3, 2), (3, 3), (4, 0), (5, None)], 5), ([(0, 1), (3, 2), (3, 3), (4, 1), (3, 0), (0, 0), (0, 0), (0, 1), (3, 0), (0, 2)], 2), ([(0, 1), (3, 2), (3, 2), (3, 3), (4, 2), (3, 0), (0, 3), (0, 1), (0, 3), (1, 3)], 0), ([(0, 2), (0, 0), (0, 0), (0, 3), (1, 2), (0, 2), (0, 0), (0, 1), (3, 2), (3, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6112
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.21111769 0.95112641 0.22536163]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6090
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.45602589  0.88599668 -0.0839659 ]
True reward weights: [-0.43927783  0.27336578 -0.85574888]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147799

Running EBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6020
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.13338511  0.96024524  0.24522948]
True reward weights: [-0.66487325  0.65822016  0.35311441]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.202740

Running EBIRL with 4 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.33069041  0.85231737  0.40521469]
True reward weights: [-0.04910073 -0.00420963 -0.99878496]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.273072

Running EBIRL with 5 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5434
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.04068782  0.92595154  0.37544408]
True reward weights: [0.36586576 0.9110704  0.18998152]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.291267

Running EBIRL with 6 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5406
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.35958521 0.90122686 0.2418442 ]
True reward weights: [-0.71582853  0.26648115  0.64542801]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.288536

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Shuffled Demos: [([(0, 0), (1, 1), (4, 3), (4, 1), (4, 3), (5, None)], 5), ([(0, 0), (0, 2), (0, 2), (0, 2), (0, 1), (3, 2), (3, 2), (3, 0), (0, 0), (0, 0)], 0), ([(0, 1), (3, 2), (3, 3), (0, 2), (3, 2), (3, 2), (3, 2), (3, 3), (4, 3), (4, 2)], 0), ([(0, 0), (0, 3), (1, 2), (0, 2), (0, 0), (0, 2), (0, 1), (3, 2), (3, 0), (0, 3)], 0), ([(0, 3), (0, 3), (3, 1), (3, 1), (3, 3), (4, 1), (4, 2), (3, 3), (4, 0), (1, 2)], 0), ([(0, 1), (3, 1), (3, 3), (4, 2), (3, 1), (3, 2), (3, 3), (3, 2), (3, 1), (3, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.17472181  0.81896882 -0.54659158]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.21236035  0.85541957 -0.47239861]
True reward weights: [ 0.43419146 -0.11645004  0.89326209]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.10402159 -0.9703508   0.21817159]
True reward weights: [-0.31511942 -0.7574738   0.57178072]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6056
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.28125884  0.81706731  0.50328369]
True reward weights: [-0.11227928  0.95130312 -0.28708143]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.330447

Running EBIRL with 5 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6008
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.1538386   0.83036874  0.53555713]
True reward weights: [-0.22564628  0.72157041 -0.65453793]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.317180

Running EBIRL with 6 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6002
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.24975492 0.95760033 0.1436109 ]
True reward weights: [-0.56303887 -0.35281523 -0.74733436]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.386730

Running experiment 6/50...
Shuffled Demos: [([(0, 0), (0, 1), (3, 3), (3, 0), (3, 0), (0, 1), (3, 2), (3, 2), (3, 0), (3, 3)], 0), ([(0, 1), (3, 1), (3, 0), (0, 1), (3, 0), (0, 2), (3, 2), (3, 2), (3, 1), (3, 3)], 0), ([(0, 2), (0, 3), (1, 3), (2, 2), (1, 3), (2, 0), (2, 1), (5, None)], 7), ([(0, 2), (3, 3), (0, 2), (0, 2), (0, 3), (1, 3), (2, 3), (2, 3), (2, 0), (2, 3)], 0), ([(0, 3), (1, 2), (0, 2), (0, 0), (0, 3), (1, 1), (0, 1), (1, 3), (1, 2), (1, 0)], 0), ([(0, 2), (0, 3), (1, 1), (4, 1), (4, 0), (1, 3), (2, 0), (2, 1), (5, None)], 8)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.04938811 -0.99571396 -0.07819545]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.34913919 0.89037934 0.29210691]
True reward weights: [ 0.26342836 -0.96179397 -0.07455101]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.09540389 -0.99528523  0.0174761 ]
True reward weights: [ 0.31597591 -0.35313022 -0.88060108]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5928
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.37497668  0.91082716 -0.17258728]
True reward weights: [0.28807899 0.91082504 0.29564886]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.329213

Running EBIRL with 5 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5972
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.44422629 0.86436674 0.23565471]
True reward weights: [-0.54274228  0.71039417 -0.4480747 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.316108

Running EBIRL with 6 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.25925785 0.95170632 0.16443982]
True reward weights: [ 0.21848527  0.82849349 -0.51561879]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.385821

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 3), (3, 2), (3, 3), (4, 0), (1, 3), (2, 0), (2, 2), (1, 1)], 0), ([(0, 0), (0, 3), (1, 2), (0, 2), (0, 0), (0, 0), (0, 2), (3, 3), (4, 1), (3, 1)], 0), ([(0, 0), (0, 0), (0, 0), (0, 3), (1, 0), (1, 3), (2, 1), (5, None)], 7), ([(0, 3), (1, 0), (1, 0), (1, 2), (0, 3), (1, 2), (0, 3), (1, 3), (2, 2), (1, 3)], 0), ([(0, 0), (0, 2), (0, 3), (0, 2), (0, 0), (0, 1), (3, 2), (3, 3), (4, 3), (4, 3)], 0), ([(0, 0), (0, 0), (0, 1), (3, 0), (0, 3), (1, 1), (4, 2), (3, 2), (3, 1), (3, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6054
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.84530439 -0.43853345 -0.30520305]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running EBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.5834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.93394443  0.25551282 -0.249922  ]
True reward weights: [-0.37607349  0.38457941 -0.84301092]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123959

Running EBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6030
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.30796196 0.86778438 0.39001243]
True reward weights: [-0.73680963 -0.47878425  0.47736486]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148115

Running EBIRL with 4 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6006
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.10240437 0.9716486  0.21310171]
True reward weights: [0.62164552 0.76967435 0.14545873]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.148097

Running EBIRL with 5 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6006
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.21006694 0.84034751 0.49968785]
True reward weights: [ 0.42159305  0.84221671 -0.33605104]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.202996

Running EBIRL with 6 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6018
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.12894493 0.90999841 0.39405089]
True reward weights: [-0.90738443  0.4065663   0.10657081]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.203165

Running experiment 8/50...
Shuffled Demos: [([(0, 2), (0, 0), (0, 2), (0, 1), (3, 1), (3, 2), (3, 2), (3, 2), (3, 0), (0, 1)], 2), ([(0, 0), (0, 1), (3, 2), (3, 0), (0, 0), (0, 1), (3, 1), (3, 2), (0, 0), (1, 3)], 0), ([(0, 2), (0, 2), (0, 1), (3, 2), (3, 2), (3, 3), (4, 0), (1, 2), (1, 3), (2, 3)], 0), ([(0, 2), (0, 2), (0, 1), (3, 0), (0, 1), (3, 2), (3, 1), (3, 1), (4, 0), (1, 2)], 0), ([(0, 0), (0, 2), (0, 0), (0, 0), (1, 0), (2, 2), (1, 1), (4, 2), (3, 3), (4, 3)], 0), ([(0, 3), (1, 3), (2, 2), (2, 0), (2, 0), (2, 1), (2, 0), (2, 3), (5, None)], 8)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.20927716  0.80939573 -0.54870905]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6012
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.43605799  0.89253716  0.11502546]
True reward weights: [0.06926909 0.81703479 0.57241239]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182548

Running EBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6062
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.27076955 0.94394772 0.18880296]
True reward weights: [0.19051707 0.80113909 0.56734417]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170360

Running EBIRL with 4 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.5912
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.14476371  0.93007356  0.3376487 ]
True reward weights: [-0.01273325  0.99114568 -0.13216692]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.164661

Running EBIRL with 5 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.5950
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.0149861   0.8723261  -0.48869478]
True reward weights: [-0.91111677  0.25209529  0.32605858]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.161181

Running EBIRL with 6 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.5880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.35833277 0.89030287 0.28099543]
True reward weights: [-0.24593742  0.8133745   0.52719703]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.213713

Saving results to files...
Results saved successfully.

Running experiment 9/50...
Shuffled Demos: [([(0, 3), (1, 3), (1, 0), (1, 0), (1, 2), (0, 3), (1, 3), (2, 2), (1, 2), (0, 2)], 0), ([(0, 0), (1, 0), (1, 0), (1, 3), (2, 2), (1, 3), (2, 0), (2, 0), (2, 0), (2, 0)], 0), ([(0, 0), (0, 3), (1, 0), (1, 1), (4, 3), (5, None)], 5), ([(0, 3), (0, 3), (1, 0), (2, 2), (5, None)], 4), ([(0, 0), (1, 1), (4, 2), (3, 2), (3, 1), (3, 2), (3, 1), (4, 2), (1, 2), (0, 1)], 0), ([(0, 1), (3, 0), (0, 3), (1, 2), (1, 0), (0, 3), (1, 0), (1, 3), (2, 3), (2, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6004
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.2223635   0.84346594  0.48899865]
True reward weights: [-0.0505111  -0.01531811  0.99860602]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running EBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.4150295   0.89770246 -0.14792163]
True reward weights: [-0.54938801 -0.43066124  0.71603332]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123675

Running EBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5932
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.20902242  0.60789156 -0.76601402]
True reward weights: [ 0.28041858  0.28883198 -0.91539145]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.147606

Running EBIRL with 4 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6082
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.24666781  0.94380595  0.21996664]
True reward weights: [-0.48916188  0.58425575 -0.64758464]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202440

Running EBIRL with 5 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6108
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.17309306  0.98374991 -0.04769595]
True reward weights: [-0.86657314 -0.24825857 -0.43291879]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.202574

Running EBIRL with 6 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6102
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.23713354  0.84998946 -0.47041004]
True reward weights: [ 0.245948    0.4031686  -0.88145599]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.202632

Running experiment 10/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 0), (0, 0), (0, 2), (0, 2), (0, 0), (0, 2), (0, 2), (0, 1)], 0), ([(0, 2), (0, 3), (1, 1), (4, 0), (3, 2), (0, 0), (0, 0), (0, 3), (1, 2), (0, 1)], 0), ([(0, 1), (3, 0), (0, 1), (3, 3), (4, 1), (4, 0), (1, 2), (4, 0), (1, 1), (4, 1)], 0), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], 4), ([(0, 3), (0, 1), (3, 2), (3, 0), (0, 0), (0, 0), (0, 0), (0, 2), (0, 3), (1, 2)], 0), ([(0, 0), (0, 3), (1, 2), (4, 3), (1, 3), (4, 1), (4, 1), (4, 1), (5, None)], 8)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.00275833 -0.88053753  0.47396841]
True reward weights: [-0.76518493 -0.64328377  0.02603879]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.38057357 0.91623401 0.12521581]
True reward weights: [-0.30279849  0.49883149 -0.81208388]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.181910

Running EBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6106
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.51256497  0.85740498 -0.0461937 ]
True reward weights: [-0.65698161  0.48692223 -0.57557094]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.169720

Running EBIRL with 4 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6032
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.14345863  0.8554371  -0.49764143]
True reward weights: [-0.70267447  0.68784707 -0.18197526]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.218743

Running EBIRL with 5 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6014
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.43915766  0.89084457  0.11634648]
True reward weights: [ 0.49430828  0.53971807 -0.68144239]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.215616

Running EBIRL with 6 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5908
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.11401899 0.98707802 0.11259064]
True reward weights: [-0.86220636 -0.33704456 -0.37815494]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.283667

Saving results to files...
Results saved successfully.

Running experiment 11/50...
Shuffled Demos: [([(0, 0), (0, 1), (3, 3), (4, 0), (1, 0), (1, 2), (0, 0), (0, 0), (0, 3), (1, 2)], 0), ([(0, 3), (3, 2), (3, 3), (4, 0), (1, 2), (0, 3), (3, 2), (3, 0), (0, 0), (1, 3)], 0), ([(0, 1), (3, 1), (3, 2), (3, 1), (3, 0), (0, 0), (1, 0), (1, 2), (0, 1), (3, 1)], 0), ([(0, 2), (0, 3), (1, 0), (0, 0), (0, 2), (3, 2), (0, 3), (1, 2), (0, 1), (3, 3)], 0), ([(0, 0), (1, 2), (0, 0), (0, 3), (1, 3), (2, 2), (1, 3), (2, 2), (1, 0), (1, 2)], 0), ([(0, 3), (1, 3), (1, 0), (1, 1), (4, 2), (3, 1), (3, 0), (0, 2), (0, 0), (0, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6012
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.39577973  0.80913516 -0.43434859]
True reward weights: [-0.87829137 -0.24439244  0.41094598]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running EBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6060
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.78779431  0.22292991  0.57417975]
True reward weights: [-0.78524977 -0.60240657  0.14314022]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123640

Running EBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6022
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.83010199 -0.48599281  0.27338925]
True reward weights: [ 0.13076479  0.88384863 -0.44912377]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123757

Running EBIRL with 4 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5960
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.40582037 -0.0240446   0.91363652]
True reward weights: [-0.69524323  0.71180381 -0.09986082]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.123847

Running EBIRL with 5 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.61567259  0.17538703 -0.76823606]
True reward weights: [-0.09505765  0.54011089  0.83620827]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.123894

Running EBIRL with 6 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5962
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.3899883   0.57726539 -0.7174077 ]
True reward weights: [-0.8581037  -0.27182105  0.43562755]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.123669

Running experiment 12/50...
Shuffled Demos: [([(0, 0), (0, 3), (1, 0), (1, 0), (1, 3), (4, 3), (5, None)], 0), ([(0, 3), (0, 3), (3, 3), (4, 3), (5, None)], 4), ([(0, 2), (0, 2), (0, 0), (0, 3), (1, 2), (0, 2), (0, 3), (1, 0), (0, 1), (3, 2)], 0), ([(0, 0), (0, 1), (3, 0), (0, 0), (0, 0), (0, 2), (0, 2), (0, 0), (0, 2), (3, 2)], 0), ([(0, 0), (0, 2), (0, 0), (0, 2), (0, 0), (0, 3), (1, 0), (1, 3), (4, 1), (4, 0)], 0), ([(0, 0), (0, 1), (3, 2), (3, 0), (0, 2), (0, 3), (1, 0), (0, 2), (0, 2), (0, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6070
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.41479385  0.83493205 -0.36171057]
True reward weights: [-0.8497926  -0.50850291  0.13884283]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.96311998  0.20360708 -0.17590921]
True reward weights: [-0.88271176  0.44301206  0.15671715]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147689

Running EBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.71668816  0.5231802   0.46112965]
True reward weights: [ 0.21622715  0.89912091 -0.38056196]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154249

Running EBIRL with 4 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5692
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.22490611  0.9727949   0.05556364]
True reward weights: [0.37980429 0.61682727 0.68940034]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.207408

Running EBIRL with 5 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5658
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.14142863  0.84365855 -0.51791718]
True reward weights: [-0.00625635  0.47785594  0.87841594]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.206581

Running EBIRL with 6 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5764
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.03036041  0.96151821 -0.27305857]
True reward weights: [-0.46870756  0.74475797  0.47502504]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.206077

Saving results to files...
Results saved successfully.

Running experiment 13/50...
Shuffled Demos: [([(0, 0), (1, 1), (4, 3), (5, None)], 3), ([(0, 3), (1, 2), (4, 2), (3, 0), (4, 3), (5, None)], 5), ([(0, 2), (0, 2), (0, 1), (0, 2), (3, 1), (3, 2), (3, 1), (3, 1), (4, 1), (5, None)], 9), ([(0, 1), (3, 1), (3, 1), (3, 1), (3, 1), (3, 0), (0, 1), (3, 0), (0, 1), (3, 3)], 0), ([(0, 0), (0, 2), (0, 0), (0, 0), (1, 3), (2, 1), (2, 0), (2, 2), (5, None)], 8), ([(0, 2), (0, 0), (1, 0), (1, 1), (0, 3), (1, 3), (2, 3), (2, 2), (1, 1), (4, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.56209279  0.77141254  0.29828574]
True reward weights: [-0.34477193 -0.32328333  0.88126058]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.3969461   0.55634188  0.73001199]
True reward weights: [ 0.83221641  0.17129844 -0.52732599]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.11623782  0.90379405 -0.41187995]
True reward weights: [-0.18994647  0.0156022  -0.98167047]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 13
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.22967969 0.97126477 0.06238583]
True reward weights: [ 0.80921463  0.52633852 -0.26103533]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.350599

Running EBIRL with 5 demonstrations for experiment 13
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.46343368 0.8816167  0.08933767]
True reward weights: [-0.11300146 -0.93009334 -0.34950972]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.432452

Running EBIRL with 6 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.01864277  0.97887499 -0.20360794]
True reward weights: [ 0.6115139  -0.28256135 -0.7390601 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.498455

Running experiment 14/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 2), (3, 3), (4, 2), (3, 0), (3, 3), (4, 0), (5, None)], 8), ([(0, 2), (0, 2), (3, 2), (3, 0), (0, 1), (3, 0), (0, 2), (3, 1), (3, 0), (0, 1)], 1), ([(0, 3), (1, 1), (4, 1), (4, 2), (3, 2), (0, 3), (1, 3), (1, 3), (4, 3), (5, None)], 9), ([(0, 3), (1, 1), (4, 2), (3, 3), (4, 0), (1, 0), (1, 2), (0, 1), (3, 1), (3, 0)], 0), ([(0, 0), (0, 1), (3, 0), (0, 2), (0, 2), (0, 3), (1, 2), (0, 2), (0, 2), (0, 3)], 0), ([(0, 3), (1, 3), (2, 3), (2, 2), (1, 1), (4, 1), (4, 2), (3, 3), (4, 0), (3, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.52242099 0.59207128 0.6136187 ]
True reward weights: [-0.43088031 -0.17122291  0.88601629]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.20370963 -0.93953472 -0.27527604]
True reward weights: [-0.25962783  0.41123857  0.87377127]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.38234889 0.85419523 0.35236321]
True reward weights: [-0.25850995 -0.2751861   0.92598338]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5948
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.00450418 0.99634798 0.08526677]
True reward weights: [-0.26039632  0.31733644  0.91186147]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.330745

Running EBIRL with 5 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5870
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.39012262 0.8594203  0.33045589]
True reward weights: [-0.86492077 -0.27885276  0.41731667]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.317210

Running EBIRL with 6 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.6034
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.0826461  0.98828106 0.12833617]
True reward weights: [-0.86789265  0.05640601 -0.49353896]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.308764

Saving results to files...
Results saved successfully.

Running experiment 15/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 3), (1, 3), (2, 3), (2, 3), (2, 1), (5, None)], 0), ([(0, 1), (3, 1), (3, 3), (4, 2), (3, 2), (3, 3), (4, 0), (1, 2), (0, 1), (3, 0)], 0), ([(0, 3), (1, 0), (2, 1), (5, None)], 0), ([(0, 3), (0, 1), (3, 0), (0, 2), (3, 0), (0, 2), (3, 0), (0, 3), (1, 3), (1, 1)], 0), ([(0, 0), (0, 3), (1, 3), (2, 0), (2, 2), (1, 0), (1, 1), (0, 1), (3, 1), (3, 2)], 0), ([(0, 2), (0, 0), (0, 1), (3, 3), (4, 0), (1, 3), (4, 3), (5, None)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.34360535  0.86758993  0.35947611]
True reward weights: [-0.75433776 -0.02805593  0.65588674]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5554
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.91646436  0.39975629  0.01697026]
True reward weights: [0.39346251 0.76922149 0.50347349]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.135073

Running EBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5420
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.96674983  0.23064308 -0.11044696]
True reward weights: [-0.98416544  0.04344964  0.17184446]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.130907

Running EBIRL with 4 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5376
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.3121784   0.77643771 -0.54743871]
True reward weights: [0.02319895 0.98727117 0.15734497]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.129631

Running EBIRL with 5 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5368
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.36927644 0.68246412 0.63077543]
True reward weights: [-0.29454697  0.30470796 -0.90575667]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.128180

Running EBIRL with 6 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5570
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.35002964  0.57358851  0.7405913 ]
True reward weights: [-0.42433899  0.90208793 -0.07857345]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.127579

Running experiment 16/50...
Shuffled Demos: [([(0, 2), (3, 0), (0, 2), (0, 3), (1, 3), (2, 2), (1, 1), (2, 3), (5, None)], 0), ([(0, 0), (0, 0), (1, 3), (2, 1), (5, None)], 0), ([(0, 3), (1, 3), (4, 2), (3, 1), (3, 1), (3, 0), (0, 3), (0, 3), (1, 2), (4, 3)], 0), ([(0, 1), (3, 2), (3, 1), (3, 2), (3, 2), (3, 0), (0, 1), (3, 1), (3, 0), (0, 2)], 0), ([(0, 0), (0, 0), (0, 3), (1, 2), (0, 3), (3, 2), (3, 0), (3, 1), (4, 3), (5, None)], 0), ([(0, 1), (3, 0), (0, 1), (3, 3), (4, 2), (3, 2), (3, 2), (3, 3), (4, 1), (4, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.9317012  -0.30245631  0.20112946]
True reward weights: [-0.92032338 -0.11683579  0.37330186]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running EBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5760
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.01549673  0.99151564 -0.12906042]
True reward weights: [ 0.09251257  0.96637057 -0.23993615]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126529

Running EBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5516
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.58073316 -0.10792604 -0.80690828]
True reward weights: [ 0.60301484  0.67561038 -0.42417416]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.135182

Running EBIRL with 4 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5392
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.51282246  0.80229385  0.30551219]
True reward weights: [0.36398208 0.82622598 0.42996242]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.156256

Running EBIRL with 5 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5336
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.44574186  0.81523303 -0.36974221]
True reward weights: [0.20371635 0.92880549 0.30954808]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.154588

Running EBIRL with 6 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5450
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.0569494   0.8600395   0.50703928]
True reward weights: [-0.36306945  0.77946375  0.51050645]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.208475

Saving results to files...
Results saved successfully.

Running experiment 17/50...
Shuffled Demos: [([(0, 3), (3, 1), (3, 3), (4, 0), (1, 3), (2, 0), (2, 1), (5, None)], 7), ([(0, 0), (0, 3), (1, 3), (2, 2), (1, 3), (2, 1), (5, None)], 0), ([(0, 3), (1, 3), (2, 3), (2, 1), (5, None)], 4), ([(0, 2), (0, 0), (0, 3), (1, 2), (0, 1), (3, 0), (0, 3), (1, 1), (4, 3), (5, None)], 0), ([(0, 3), (1, 3), (2, 0), (2, 0), (2, 0), (2, 0), (2, 3), (2, 0), (2, 2), (2, 0)], 0), ([(0, 1), (1, 3), (2, 1), (2, 3), (2, 2), (1, 2), (0, 2), (0, 1), (3, 0), (0, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 3.53854326e-04 -9.40653158e-01  3.39369285e-01]
True reward weights: [-0.76208548 -0.28664819  0.58056743]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6012
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.24098947  0.83268999  0.49854935]
True reward weights: [-0.50015791 -0.30049649 -0.8121231 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182422

Running EBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6046
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.07188382  0.93911197 -0.33600807]
True reward weights: [0.40253428 0.76964811 0.49558848]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.224690

Running EBIRL with 4 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6152
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.18406613  0.96866132 -0.16677799]
True reward weights: [-0.32395667 -0.41015559 -0.85254001]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.219025

Running EBIRL with 5 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.5672
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.16068893  0.91909502 -0.35978244]
True reward weights: [-0.09710712  0.46980272  0.87741416]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.225979

Running EBIRL with 6 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.5700
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.07392573  0.96351571 -0.2572401 ]
True reward weights: [-0.53856025 -0.31588062 -0.78113525]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.222100

Running experiment 18/50...
Shuffled Demos: [([(0, 1), (0, 2), (0, 0), (0, 3), (1, 3), (2, 3), (2, 1), (1, 1), (4, 1), (4, 1)], 0), ([(0, 1), (3, 2), (3, 1), (3, 2), (3, 1), (3, 0), (0, 1), (3, 3), (4, 1), (5, None)], 9), ([(0, 1), (3, 3), (4, 2), (3, 2), (3, 3), (4, 0), (1, 0), (1, 0), (1, 1), (4, 1)], 0), ([(0, 0), (0, 0), (0, 2), (0, 0), (0, 2), (0, 3), (1, 3), (2, 2), (1, 0), (1, 3)], 0), ([(0, 1), (3, 1), (3, 1), (4, 1), (4, 2), (3, 0), (0, 3), (1, 1), (4, 0), (1, 2)], 0), ([(0, 3), (3, 1), (3, 2), (3, 0), (0, 0), (0, 3), (1, 2), (0, 1), (3, 2), (3, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.5846
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.45488097  0.64693904  0.6120075 ]
True reward weights: [-0.39174849 -0.30341429  0.86860399]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running EBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6032
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.14566213 0.58395115 0.79861355]
True reward weights: [ 0.27431068  0.39879125 -0.87505382]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147932

Running EBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.00943532  0.95297965 -0.30288738]
True reward weights: [-0.96410781  0.0268938   0.26414551]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148072

Running EBIRL with 4 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.5978
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.69809112  0.64370076 -0.3135572 ]
True reward weights: [-0.76301576  0.28758977  0.57887743]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.148017

Running EBIRL with 5 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6024
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.0134124   0.73884216 -0.67374503]
True reward weights: [-0.41725429 -0.35024719  0.83858557]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.147977

Running EBIRL with 6 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.5976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.28829909  0.69462529 -0.65907461]
True reward weights: [-0.25749695  0.96627843  0.00115037]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.148249

Saving results to files...
Results saved successfully.

Running experiment 19/50...
Shuffled Demos: [([(0, 2), (3, 1), (3, 3), (0, 1), (3, 2), (3, 2), (3, 0), (0, 2), (0, 1), (3, 1)], 0), ([(0, 0), (0, 0), (0, 3), (1, 1), (0, 1), (3, 0), (0, 3), (1, 3), (2, 2), (1, 3)], 0), ([(0, 0), (0, 1), (1, 0), (1, 2), (0, 3), (1, 1), (4, 0), (1, 0), (2, 2), (1, 0)], 0), ([(0, 3), (1, 2), (0, 3), (1, 2), (1, 1), (4, 2), (3, 0), (0, 0), (0, 1), (3, 0)], 0), ([(0, 3), (1, 2), (0, 0), (0, 0), (0, 2), (3, 3), (4, 2), (3, 1), (3, 3), (4, 0)], 0), ([(0, 0), (0, 3), (1, 0), (1, 3), (2, 0), (2, 0), (2, 2), (1, 3), (2, 2), (1, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.35499116 -0.93478721  0.01241535]
True reward weights: [-0.92648321 -0.16748955  0.33701054]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.29834281  0.87745791  0.37557846]
True reward weights: [0.47138772 0.82567939 0.30991476]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182970

Running EBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5982
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.2264238   0.84102272  0.49133801]
True reward weights: [-0.77724287 -0.49548706  0.38779645]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170103

Running EBIRL with 4 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.34776016  0.92868844  0.12884352]
True reward weights: [-0.54190911  0.32343089  0.77571063]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.164165

Running EBIRL with 5 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6026
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.48895703  0.83653659  0.24723986]
True reward weights: [-0.78554364  0.56850733  0.24437799]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.161081

Running EBIRL with 6 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5944
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.07137597  0.86877024 -0.49004462]
True reward weights: [-0.24774734  0.7348237   0.63139163]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.158681

Running experiment 20/50...
Shuffled Demos: [([(0, 1), (3, 2), (3, 0), (0, 3), (1, 1), (4, 0), (1, 1), (0, 2), (0, 2), (0, 1)], 0), ([(0, 0), (0, 1), (3, 0), (3, 2), (3, 3), (4, 2), (3, 0), (0, 2), (0, 2), (0, 2)], 0), ([(0, 1), (3, 3), (4, 3), (5, None)], 3), ([(0, 1), (3, 3), (0, 2), (0, 0), (0, 2), (0, 3), (1, 2), (0, 1), (1, 1), (4, 2)], 0), ([(0, 3), (1, 3), (4, 1), (3, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 1), (4, 3)], 0), ([(0, 0), (0, 3), (1, 3), (2, 3), (2, 1), (5, None)], 5)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5894
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.56759923  0.82330221 -0.00214092]
True reward weights: [-0.61012931 -0.3129562   0.72787406]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running EBIRL with 2 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6048
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.37298367  0.91681906  0.14256924]
True reward weights: [-0.22539456  0.08851302 -0.97023849]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147843

Running EBIRL with 3 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5968
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.41515124  0.87696183 -0.24204834]
True reward weights: [-0.51823522  0.05916407 -0.85318924]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.202678

Running EBIRL with 4 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.53888449  0.80590804 -0.24518513]
True reward weights: [-0.08766182  0.14129172  0.98607913]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202841

Running EBIRL with 5 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5874
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.35684391  0.9159526   0.18355721]
True reward weights: [0.33339897 0.37764692 0.86384485]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.202888

Running EBIRL with 6 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5848
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.06313067  0.8067799  -0.58746976]
True reward weights: [-0.4975898  -0.42063367  0.75859852]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.273142

Saving results to files...
Results saved successfully.

Running experiment 21/50...
Shuffled Demos: [([(0, 1), (3, 1), (3, 0), (0, 0), (1, 1), (4, 3), (4, 1), (4, 0), (1, 3), (2, 3)], 0), ([(0, 1), (3, 1), (3, 1), (3, 0), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)], 8), ([(0, 2), (0, 0), (0, 3), (1, 2), (0, 0), (0, 2), (0, 3), (1, 3), (2, 3), (2, 1)], 0), ([(0, 0), (0, 0), (0, 3), (1, 2), (0, 3), (1, 3), (2, 3), (2, 2), (1, 0), (1, 3)], 0), ([(0, 1), (3, 1), (3, 1), (4, 2), (3, 3), (4, 2), (3, 3), (4, 2), (3, 3), (4, 0)], 0), ([(0, 0), (1, 3), (4, 3), (5, None)], 3)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6054
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.32399633  0.58077828 -0.74680852]
True reward weights: [-0.3738588  -0.3262014   0.86822937]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.3930664   0.91950554 -0.00289104]
True reward weights: [-0.29146625  0.84933773 -0.44008277]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147724

Running EBIRL with 3 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6002
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.49089471  0.72870535 -0.47750487]
True reward weights: [-0.89835285 -0.34798478 -0.26808347]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.147833

Running EBIRL with 4 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6016
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.51071441 0.85622283 0.07780265]
True reward weights: [-0.55221058  0.67156055  0.49403433]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.147788

Running EBIRL with 5 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.23285794  0.81976092 -0.52322961]
True reward weights: [-0.38186618  0.91548613 -0.12674133]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.202719

Running EBIRL with 6 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.31987712  0.89843375 -0.30082457]
True reward weights: [-0.4257935  -0.08814805 -0.90051642]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.273004

Running experiment 22/50...
Shuffled Demos: [([(0, 3), (1, 0), (1, 1), (4, 3), (4, 3), (5, None)], 5), ([(0, 1), (3, 3), (4, 1), (4, 0), (1, 3), (2, 3), (2, 3), (2, 2), (1, 2), (1, 0)], 0), ([(0, 2), (0, 0), (0, 1), (3, 2), (3, 3), (4, 1), (3, 0), (0, 2), (0, 3), (1, 3)], 0), ([(0, 2), (0, 1), (3, 0), (3, 3), (4, 0), (1, 2), (0, 2), (3, 0), (3, 1), (3, 3)], 0), ([(0, 0), (0, 2), (3, 2), (0, 0), (0, 0), (0, 2), (0, 3), (1, 2), (0, 0), (0, 1)], 0), ([(0, 3), (0, 3), (1, 2), (0, 0), (0, 0), (0, 2), (0, 1), (1, 0), (1, 3), (2, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 22
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.90637695  0.33342764 -0.25943561]
True reward weights: [-0.70965126 -0.6051058   0.36089066]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6062
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.36864345  0.39018309  0.84371747]
True reward weights: [-0.61779834 -0.34354327 -0.70732117]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182896

Running EBIRL with 3 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6094
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.00745984  0.95497064 -0.29660651]
True reward weights: [-0.52026977  0.76211799  0.38535119]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170165

Running EBIRL with 4 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.17313353  0.75334158 -0.63442986]
True reward weights: [-0.4385946   0.15176537 -0.88577765]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.164466

Running EBIRL with 5 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.16353313  0.74609872  0.64544064]
True reward weights: [ 0.15877209  0.8825735  -0.44255557]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.161027

Running EBIRL with 6 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.00996276  0.95677907 -0.29064506]
True reward weights: [-0.95273267 -0.29863709  0.0558243 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.158677

Saving results to files...
Results saved successfully.

Running experiment 23/50...
Shuffled Demos: [([(0, 2), (3, 1), (3, 0), (3, 2), (0, 0), (0, 0), (1, 3), (2, 0), (1, 3), (2, 0)], 0), ([(0, 0), (0, 3), (3, 3), (0, 2), (0, 1), (1, 3), (2, 1), (2, 3), (2, 0), (2, 1)], 0), ([(0, 2), (0, 2), (3, 2), (3, 2), (3, 2), (3, 1), (3, 3), (4, 2), (3, 1), (3, 1)], 0), ([(0, 3), (3, 3), (4, 1), (4, 3), (5, None)], 4), ([(0, 1), (3, 2), (3, 2), (3, 1), (3, 1), (3, 1), (3, 1), (3, 3), (4, 2), (3, 2)], 0), ([(0, 0), (0, 0), (0, 1), (0, 0), (0, 2), (0, 0), (0, 1), (1, 0), (1, 2), (0, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6050
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.23844876 0.61160742 0.75437295]
True reward weights: [-0.74528522 -0.55533186  0.36899385]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running EBIRL with 2 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.5962
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.94184815  0.25111893  0.22329653]
True reward weights: [ 0.59428998  0.79271609 -0.1357226 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123952

Running EBIRL with 3 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.19196332  0.97945992  0.0617118 ]
True reward weights: [-0.97499483 -0.11115405 -0.19243145]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148158

Running EBIRL with 4 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.5936
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.34064261  0.86972055 -0.35713972]
True reward weights: [0.24366218 0.39677207 0.88498625]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.203047

Running EBIRL with 5 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6076
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.08697681  0.96418427 -0.2505668 ]
True reward weights: [0.53216923 0.813681   0.23392124]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.273350

Running EBIRL with 6 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.5948
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.05619445  0.95073185 -0.30487889]
True reward weights: [0.6268868  0.76956196 0.12160314]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.273411

Running experiment 24/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 0), (0, 2), (0, 0), (0, 3), (1, 1), (4, 1), (4, 1), (4, 3)], 0), ([(0, 3), (1, 2), (1, 1), (0, 2), (0, 2), (0, 0), (0, 0), (0, 2), (0, 3), (1, 2)], 0), ([(0, 0), (0, 1), (3, 2), (3, 3), (4, 2), (3, 3), (4, 2), (3, 2), (3, 0), (0, 1)], 1), ([(0, 3), (3, 2), (3, 1), (3, 0), (0, 2), (0, 1), (3, 3), (3, 1), (3, 1), (3, 1)], 1), ([(0, 0), (0, 1), (3, 1), (3, 1), (3, 1), (4, 3), (5, None)], 6), ([(0, 2), (0, 3), (1, 3), (1, 2), (4, 3), (5, None)], 5)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.5994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.85730694 -0.26434076 -0.44175647]
True reward weights: [-0.68144434 -0.29187893  0.6711485 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running EBIRL with 2 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.5986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.29489788 0.62433376 0.7233551 ]
True reward weights: [-0.37868421  0.04934308 -0.92420968]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124001

Running EBIRL with 3 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6040
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.16745357  0.94179561  0.29151386]
True reward weights: [-0.48103309  0.84153504 -0.24581691]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148007

Running EBIRL with 4 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6008
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.09234915  0.83566112 -0.54142601]
True reward weights: [-0.68442969  0.43849395  0.58247665]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202944

Running EBIRL with 5 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6008
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.48086051  0.87021903 -0.1072008 ]
True reward weights: [-0.4466751   0.4965128   0.74428247]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.273191

Running EBIRL with 6 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6124
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.06083281  0.83943144  0.5400502 ]
True reward weights: [-0.4719748  -0.26368057  0.84125641]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.351178

Saving results to files...
Results saved successfully.

Running experiment 25/50...
Shuffled Demos: [([(0, 2), (0, 0), (0, 1), (3, 3), (4, 0), (5, None)], 5), ([(0, 3), (1, 2), (1, 2), (0, 0), (0, 2), (0, 3), (1, 3), (2, 3), (2, 0), (2, 1)], 0), ([(0, 3), (3, 1), (4, 1), (3, 2), (3, 0), (0, 1), (1, 0), (1, 3), (2, 2), (1, 2)], 0), ([(0, 3), (1, 1), (4, 1), (4, 1), (4, 0), (1, 3), (2, 0), (2, 2), (1, 1), (4, 1)], 0), ([(0, 2), (0, 1), (3, 2), (3, 1), (3, 1), (3, 2), (3, 3), (3, 2), (0, 0), (0, 2)], 2), ([(0, 0), (0, 3), (1, 3), (4, 3), (5, None)], 4)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 25
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.96234399  0.22513765  0.1523387 ]
True reward weights: [-0.44714936 -0.30119858  0.84222139]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6030
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.86392507  0.50137112  0.04754454]
True reward weights: [-0.70672081 -0.43151379 -0.5606617 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183258

Running EBIRL with 3 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6030
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.67465667  0.5976663  -0.43316669]
True reward weights: [-0.89740906 -0.4411913  -0.00268755]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170292

Running EBIRL with 4 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.66106509  0.60107799 -0.44910823]
True reward weights: [ 0.3921929   0.42833488 -0.81407246]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.164338

Running EBIRL with 5 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6004
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.08435849  0.91571982 -0.39286239]
True reward weights: [-0.73987371  0.05308363 -0.67064822]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.215711

Running EBIRL with 6 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5974
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.07328192  0.95673649 -0.281576  ]
True reward weights: [-0.81129056  0.44500401  0.37918209]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.283777

Running experiment 26/50...
Shuffled Demos: [([(0, 2), (3, 3), (4, 3), (4, 1), (4, 0), (5, None)], 5), ([(0, 2), (0, 1), (3, 3), (4, 2), (3, 2), (0, 1), (3, 1), (3, 0), (0, 1), (3, 2)], 0), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], 4), ([(0, 3), (1, 1), (0, 2), (0, 3), (3, 3), (4, 2), (3, 3), (0, 0), (0, 2), (0, 3)], 0), ([(0, 3), (0, 2), (3, 0), (0, 3), (3, 1), (3, 2), (3, 3), (3, 2), (3, 3), (4, 2)], 0), ([(0, 3), (3, 2), (3, 2), (3, 1), (3, 2), (0, 0), (0, 2), (0, 2), (3, 0), (3, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 26
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.54951152 0.36012329 0.75388879]
True reward weights: [-0.7333898  -0.47830978  0.48307263]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 26
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.20007572  0.93700388  0.28634497]
True reward weights: [-0.29388911  0.67900068 -0.67274606]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 26
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.54193897  0.84032935  0.0121956 ]
True reward weights: [-0.04357415  0.77442687 -0.63116108]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.5986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.41043737 0.89491442 0.17512669]
True reward weights: [0.24235898 0.61152165 0.75319545]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.329769

Running EBIRL with 5 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6046
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.02507341  0.9319997   0.36159076]
True reward weights: [-0.84326072  0.51699469 -0.14706408]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.394254

Running EBIRL with 6 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.5940
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.38609892  0.91019581 -0.14990404]
True reward weights: [-0.07451836  0.96410792 -0.25483901]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.467889

Saving results to files...
Results saved successfully.

Running experiment 27/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 3), (1, 3), (2, 3), (2, 1), (1, 1), (4, 0), (3, 3), (4, 3)], 0), ([(0, 3), (1, 3), (2, 2), (1, 2), (0, 3), (1, 1), (4, 3), (5, None)], 0), ([(0, 3), (3, 1), (3, 3), (4, 2), (3, 1), (4, 3), (5, None)], 6), ([(0, 0), (0, 1), (3, 3), (4, 1), (4, 1), (4, 3), (4, 2), (3, 1), (3, 2), (3, 3)], 0), ([(0, 0), (0, 2), (0, 2), (0, 2), (0, 0), (0, 2), (0, 2), (0, 1), (3, 0), (0, 0)], 0), ([(0, 0), (1, 0), (1, 2), (0, 2), (0, 2), (0, 0), (0, 1), (1, 3), (2, 2), (5, None)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6040
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.89050634  0.30651813 -0.33622179]
True reward weights: [-0.65557811 -0.01374154  0.75500233]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5676
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.10273371  0.50708467  0.85575167]
True reward weights: [-0.77102632 -0.63556561 -0.03968344]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.129778

Running EBIRL with 3 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5758
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.21330027 0.95998658 0.18146285]
True reward weights: [-0.81816316  0.23764312 -0.52357882]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151587

Running EBIRL with 4 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5754
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.40219259  0.89424592  0.19638061]
True reward weights: [-0.42671868 -0.38706968 -0.81736665]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.205558

Running EBIRL with 5 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5694
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.09517721  0.98244936 -0.1604199 ]
True reward weights: [-0.95341388  0.12790269  0.2732085 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.275292

Running EBIRL with 6 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5694
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.00222075  0.93247819 -0.36121946]
True reward weights: [-0.95308575  0.27188587  0.13306249]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.274983

Running experiment 28/50...
Shuffled Demos: [([(0, 3), (1, 0), (0, 2), (0, 2), (0, 3), (1, 1), (4, 0), (1, 3), (1, 1), (4, 2)], 0), ([(0, 2), (0, 1), (3, 0), (3, 3), (4, 0), (1, 3), (2, 2), (5, None)], 7), ([(0, 3), (1, 3), (2, 3), (2, 2), (5, None)], 4), ([(0, 1), (3, 0), (0, 2), (0, 0), (0, 3), (1, 0), (1, 0), (1, 2), (0, 3), (1, 0)], 0), ([(0, 1), (3, 2), (3, 3), (4, 0), (1, 2), (0, 2), (0, 1), (3, 3), (4, 1), (3, 0)], 0), ([(0, 2), (0, 3), (1, 0), (1, 0), (1, 3), (2, 0), (2, 0), (2, 0), (2, 2), (1, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.5928
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.80103121  0.5980922  -0.02519364]
True reward weights: [-0.866301   -0.34967581  0.35672034]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running EBIRL with 2 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6004
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.53503471 0.82669486 0.17410764]
True reward weights: [-0.57316679  0.78512129  0.23465802]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147630

Running EBIRL with 3 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6034
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.37408705  0.87780948  0.2991812 ]
True reward weights: [-0.60424513 -0.33696215  0.72204178]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.202506

Running EBIRL with 4 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6070
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.19818493  0.96866632 -0.14969403]
True reward weights: [-0.26507982  0.90316908  0.33766596]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202600

Running EBIRL with 5 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6022
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.2043722   0.90731919 -0.36742876]
True reward weights: [-0.87287568 -0.05272146 -0.48508607]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.202964

Running EBIRL with 6 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6058
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.33974641  0.93673634  0.08424611]
True reward weights: [ 0.47846128  0.87084597 -0.11270364]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.202953

Saving results to files...
Results saved successfully.

Running experiment 29/50...
Shuffled Demos: [([(0, 1), (3, 2), (3, 0), (0, 3), (1, 3), (2, 2), (1, 1), (4, 2), (3, 1), (3, 3)], 0), ([(0, 0), (0, 2), (0, 0), (1, 3), (2, 3), (2, 0), (1, 0), (1, 1), (4, 0), (1, 2)], 0), ([(0, 0), (1, 0), (1, 2), (0, 3), (1, 1), (2, 1), (5, None)], 0), ([(0, 1), (0, 3), (1, 2), (0, 3), (1, 0), (1, 1), (4, 2), (1, 1), (4, 3), (5, None)], 0), ([(0, 1), (3, 2), (3, 3), (4, 2), (4, 2), (3, 0), (4, 1), (4, 2), (1, 1), (4, 2)], 0), ([(0, 3), (1, 3), (2, 1), (5, None)], 3)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5982
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.47746439 0.87828365 0.02540841]
True reward weights: [-0.6535586  -0.23072892  0.72085041]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.19805092  0.32262049 -0.9255765 ]
True reward weights: [-0.87175997 -0.02176468  0.48944954]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123775

Running EBIRL with 3 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5790
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.94985152  0.23285702  0.2087096 ]
True reward weights: [-0.60421892  0.69628018  0.38744471]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.130245

Running EBIRL with 4 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5686
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.02045485  0.93292188  0.35949738]
True reward weights: [-0.8127651  -0.463353   -0.35315278]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.128479

Running EBIRL with 5 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5680
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.87723777 -0.35261082  0.3257599 ]
True reward weights: [-0.92638122 -0.11534268 -0.35848837]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.127757

Running EBIRL with 6 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5748
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.27580159  0.32400537  0.90496077]
True reward weights: [-0.64693548  0.7225695   0.24365508]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.151009

Running experiment 30/50...
Shuffled Demos: [([(0, 1), (3, 2), (3, 2), (3, 3), (4, 2), (1, 3), (2, 2), (2, 1), (2, 1), (5, None)], 0), ([(0, 3), (0, 1), (3, 0), (0, 3), (1, 3), (2, 0), (2, 0), (2, 1), (1, 2), (1, 1)], 0), ([(0, 2), (0, 1), (3, 3), (3, 1), (4, 0), (5, None)], 5), ([(0, 1), (0, 1), (3, 2), (3, 2), (3, 1), (3, 2), (3, 1), (4, 2), (3, 3), (4, 0)], 0), ([(0, 0), (0, 2), (0, 2), (0, 2), (3, 0), (0, 0), (0, 0), (0, 1), (3, 1), (4, 2)], 0), ([(0, 1), (3, 0), (0, 0), (0, 0), (0, 0), (0, 1), (0, 3), (1, 3), (2, 3), (2, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5980
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.05281529 -0.03246    -0.9980766 ]
True reward weights: [-0.81326386 -0.04441834  0.5801973 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5458
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.48539038  0.82818009 -0.28020334]
True reward weights: [-0.87490367 -0.32149503 -0.36219403]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.134300

Running EBIRL with 3 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5440
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.19571469  0.65672609 -0.72829019]
True reward weights: [-0.78240552  0.57968636  0.22760785]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155051

Running EBIRL with 4 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5424
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.08412497  0.95213254  0.29388198]
True reward weights: [-0.18066342  0.37702397 -0.90841271]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.208080

Running EBIRL with 5 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5366
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.19884771 0.97988524 0.01686702]
True reward weights: [-0.65386383  0.67061624  0.35033691]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.277335

Running EBIRL with 6 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5438
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.17167982  0.98394201 -0.04882784]
True reward weights: [-0.36021962  0.57352511  0.73573825]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.276915

Saving results to files...
Results saved successfully.

Running experiment 31/50...
Shuffled Demos: [([(0, 0), (0, 1), (3, 0), (0, 1), (3, 2), (3, 1), (3, 2), (3, 0), (0, 0), (0, 0)], 0), ([(0, 1), (3, 2), (3, 0), (0, 2), (0, 2), (3, 0), (0, 1), (3, 2), (3, 1), (3, 1)], 0), ([(0, 2), (0, 0), (0, 0), (0, 1), (3, 1), (3, 3), (4, 3), (5, None)], 7), ([(0, 1), (3, 2), (3, 3), (4, 2), (3, 0), (0, 2), (0, 3), (1, 3), (2, 0), (1, 1)], 0), ([(0, 0), (0, 1), (3, 1), (4, 2), (3, 2), (3, 0), (3, 2), (3, 3), (4, 3), (5, None)], 9), ([(0, 1), (0, 3), (1, 1), (4, 0), (1, 1), (4, 3), (5, None)], 6)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 31
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.0746774  -0.98349702  0.1647935 ]
True reward weights: [-0.27534761 -0.19938474  0.94044108]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 31
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.32695991 -0.91798993  0.22448096]
True reward weights: [-0.96665221  0.20229622  0.15703424]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 31
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.52215986  0.83910581  0.1524812 ]
True reward weights: [0.02530276 0.93722914 0.34779492]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6108
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.33007364  0.81276154  0.48007298]
True reward weights: [-0.74040146  0.05167741 -0.67017544]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.332546

Running EBIRL with 5 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.5974
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.27959524  0.95809215 -0.06233733]
True reward weights: [-0.51334793 -0.36556033 -0.77642743]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.396094

Running EBIRL with 6 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.2342668   0.91362871 -0.33226744]
True reward weights: [-0.3693342   0.2709112   0.88893159]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.469198

Running experiment 32/50...
Shuffled Demos: [([(0, 0), (0, 3), (3, 2), (3, 1), (3, 2), (3, 3), (4, 2), (3, 3), (4, 0), (1, 1)], 0), ([(0, 3), (3, 2), (3, 3), (4, 1), (4, 0), (3, 0), (4, 1), (4, 0), (1, 1), (4, 3)], 0), ([(0, 0), (0, 1), (3, 2), (3, 3), (4, 2), (3, 1), (4, 3), (5, None)], 7), ([(0, 0), (0, 1), (3, 1), (3, 1), (3, 0), (0, 3), (0, 2), (0, 0), (0, 2), (0, 0)], 0), ([(0, 0), (0, 0), (0, 1), (3, 3), (4, 0), (1, 2), (0, 3), (1, 1), (4, 1), (4, 1)], 0), ([(0, 0), (0, 1), (3, 0), (4, 0), (1, 2), (0, 1), (3, 2), (3, 1), (3, 0), (0, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5962
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.38302938  0.69090016  0.61314392]
True reward weights: [-0.90753571 -0.40234227  0.1204144 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running EBIRL with 2 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6036
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.62712034  0.04409288  0.77767338]
True reward weights: [-0.96517321  0.17961674 -0.19020649]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123594

Running EBIRL with 3 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.49367215 0.82807661 0.26566319]
True reward weights: [-0.76461575 -0.35432671 -0.538345  ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148131

Running EBIRL with 4 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.14200668  0.89678597  0.41905731]
True reward weights: [-0.50775997  0.16332068  0.84587598]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.203145

Running EBIRL with 5 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.142041    0.97032115 -0.19570697]
True reward weights: [-0.77331037  0.45241134  0.4442016 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.203160

Running EBIRL with 6 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5950
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.45717063  0.86990696 -0.18508621]
True reward weights: [0.27901361 0.70669152 0.65018344]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.203310

Saving results to files...
Results saved successfully.

Running experiment 33/50...
Shuffled Demos: [([(0, 0), (1, 3), (2, 1), (5, None)], 3), ([(0, 0), (0, 3), (1, 3), (1, 2), (0, 0), (0, 1), (3, 0), (0, 1), (3, 1), (3, 0)], 0), ([(0, 3), (1, 0), (1, 3), (2, 0), (2, 1), (5, None)], 5), ([(0, 2), (3, 0), (0, 2), (3, 1), (3, 2), (3, 2), (3, 3), (4, 3), (5, None)], 8), ([(0, 3), (1, 3), (2, 3), (2, 3), (2, 1), (5, None)], 5), ([(0, 0), (0, 0), (0, 0), (0, 2), (0, 2), (0, 3), (1, 1), (4, 2), (3, 1), (3, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 33
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.50275765  0.66297861 -0.55470182]
True reward weights: [-0.32556312 -0.30396656  0.89532842]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6112
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.25481434  0.30678828  0.91703359]
True reward weights: [-0.29390295 -0.77871679  0.5542754 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182812

Running EBIRL with 3 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.5942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.203486    0.97868906  0.0275892 ]
True reward weights: [-0.60526618  0.57670214 -0.54869617]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.224800

Running EBIRL with 4 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.13982545  0.98340952 -0.11556196]
True reward weights: [-0.59472095  0.34728008 -0.72505416]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.289138

Running EBIRL with 5 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6024
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.57397352  0.81831272 -0.03030998]
True reward weights: [ 0.11161882  0.99033975 -0.08227036]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.363739

Running EBIRL with 6 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6064
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.09737606  0.91670672 -0.38751348]
True reward weights: [-0.5047422   0.81839619  0.27470527]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.361897

Running experiment 34/50...
Shuffled Demos: [([(0, 1), (3, 3), (3, 0), (0, 0), (0, 0), (0, 1), (3, 1), (3, 0), (4, 3), (5, None)], 9), ([(0, 0), (1, 1), (4, 1), (4, 2), (3, 1), (3, 1), (3, 2), (3, 3), (0, 3), (3, 0)], 0), ([(0, 0), (0, 2), (0, 2), (0, 1), (3, 1), (3, 2), (3, 2), (3, 1), (3, 1), (3, 0)], 0), ([(0, 1), (3, 2), (3, 2), (3, 0), (3, 0), (0, 2), (0, 0), (0, 2), (0, 2), (3, 3)], 0), ([(0, 0), (1, 1), (4, 0), (1, 2), (4, 3), (4, 2), (3, 1), (4, 2), (3, 1), (4, 1)], 0), ([(0, 0), (0, 2), (0, 3), (1, 2), (1, 2), (0, 3), (1, 3), (4, 3), (5, None)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 34
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.79747499  0.60242903 -0.0333603 ]
True reward weights: [-0.839382   -0.393236    0.37523766]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.6018
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.33461139  0.84441253 -0.41833324]
True reward weights: [ 0.20486723 -0.97196348  0.11539679]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182855

Running EBIRL with 3 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5966
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.39310105  0.8758449  -0.27994155]
True reward weights: [-0.84051366 -0.26289485 -0.47373313]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.225107

Running EBIRL with 4 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5944
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.08402122  0.94115045 -0.32737788]
True reward weights: [-0.90361321 -0.4283419   0.00252663]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.289521

Running EBIRL with 5 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.29028971  0.94704264 -0.13726662]
True reward weights: [-0.99481321  0.10103643  0.0117612 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.286329

Running EBIRL with 6 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5652
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.07548401  0.99687038 -0.0234865 ]
True reward weights: [-0.97948745 -0.18817584 -0.07207066]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.292131

Saving results to files...
Results saved successfully.

Running experiment 35/50...
Shuffled Demos: [([(0, 0), (0, 2), (0, 0), (0, 3), (1, 3), (1, 2), (0, 3), (1, 2), (4, 1), (4, 2)], 0), ([(0, 2), (0, 0), (0, 0), (0, 0), (0, 2), (0, 1), (3, 0), (0, 3), (1, 2), (0, 2)], 0), ([(0, 0), (0, 1), (1, 0), (1, 3), (2, 1), (2, 1), (1, 2), (0, 0), (0, 2), (0, 2)], 0), ([(0, 2), (0, 3), (1, 3), (2, 1), (5, None)], 0), ([(0, 3), (1, 3), (4, 3), (5, None)], 0), ([(0, 3), (0, 1), (3, 2), (3, 1), (3, 3), (4, 3), (1, 3), (2, 2), (1, 1), (4, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5916
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.83749434 -0.53533682  0.1096254 ]
True reward weights: [-0.94121751 -0.05112011  0.33391067]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running EBIRL with 2 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.2694599   0.91858907 -0.28911155]
True reward weights: [-0.04088415  0.45034592 -0.89191762]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124274

Running EBIRL with 3 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6012
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.41438944  0.90710091  0.07381963]
True reward weights: [-0.7352455   0.53359821 -0.41795573]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123979

Running EBIRL with 4 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.03957449  0.99728835  0.06204678]
True reward weights: [-0.60363523  0.79670446 -0.0297744 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.139490

Running EBIRL with 5 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5532
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.97496084 -0.09473407  0.20118849]
True reward weights: [-0.09622112 -0.06614598 -0.99315971]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.136705

Running EBIRL with 6 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5414
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.17350564 0.71344231 0.67889312]
True reward weights: [0.36194948 0.65865103 0.65967521]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.134434

Running experiment 36/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 2), (0, 1), (3, 1), (3, 2), (3, 3), (4, 2), (3, 3), (0, 3)], 0), ([(0, 1), (3, 0), (0, 0), (0, 3), (1, 2), (0, 2), (0, 2), (0, 1), (1, 0), (2, 2)], 0), ([(0, 1), (3, 0), (0, 1), (3, 0), (0, 3), (1, 2), (0, 3), (3, 3), (4, 3), (5, None)], 9), ([(0, 3), (3, 2), (3, 1), (3, 3), (4, 2), (3, 3), (4, 1), (4, 0), (1, 0), (1, 1)], 0), ([(0, 0), (0, 0), (0, 3), (1, 2), (0, 2), (0, 3), (1, 2), (0, 2), (0, 2), (0, 0)], 0), ([(0, 3), (1, 1), (0, 1), (3, 2), (3, 0), (4, 3), (5, None)], 6)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 36
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.23676245 -0.93279853 -0.27171759]
True reward weights: [-0.05456508 -0.040557    0.99768621]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.29983265  0.87987767 -0.36866742]
True reward weights: [ 0.42888491 -0.5932592   0.68124977]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183034

Running EBIRL with 3 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5886
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.06432624 0.98789803 0.14113691]
True reward weights: [-0.66034953  0.74485761  0.09552818]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.225022

Running EBIRL with 4 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.25584712  0.96330008 -0.08121086]
True reward weights: [ 0.22695633  0.49697387 -0.83756063]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.219189

Running EBIRL with 5 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.56149748 0.82194684 0.09551947]
True reward weights: [-0.06841079  0.04864163  0.99647075]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.215810

Running EBIRL with 6 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.11081457 0.97665475 0.18402616]
True reward weights: [-0.80778401  0.54364101 -0.22790228]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.283763

Saving results to files...
Results saved successfully.

Running experiment 37/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)], 5), ([(0, 1), (3, 2), (3, 3), (4, 1), (4, 3), (5, None)], 5), ([(0, 1), (3, 1), (3, 2), (3, 3), (4, 1), (4, 0), (1, 1), (4, 1), (5, None)], 8), ([(0, 2), (0, 3), (1, 1), (2, 1), (5, None)], 4), ([(0, 3), (1, 0), (2, 2), (1, 2), (4, 1), (3, 1), (3, 0), (0, 1), (3, 2), (3, 3)], 0), ([(0, 3), (1, 2), (0, 3), (1, 0), (1, 0), (2, 2), (1, 0), (1, 1), (4, 3), (5, None)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 37
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.74909936  0.57610938 -0.32702925]
True reward weights: [-0.75478167 -0.33513042  0.563908  ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 37
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.5147871   0.62770319 -0.58393745]
True reward weights: [-0.17246216 -0.54041846 -0.82353184]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 37
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.27326174 -0.880554   -0.38723724]
True reward weights: [0.09385736 0.54999662 0.8298762 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 37
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.37470495  0.87153553  0.31626257]
True reward weights: [ 0.84377744  0.52329988 -0.11915062]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.350599

Running EBIRL with 5 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.5980
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.10500961  0.966266   -0.23516592]
True reward weights: [-0.61147723 -0.44592015 -0.65364426]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.415180

Running EBIRL with 6 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.5906
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.18084753  0.94356789 -0.27744153]
True reward weights: [-0.72481762  0.22820055 -0.65004918]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.406747

Running experiment 38/50...
Shuffled Demos: [([(0, 1), (0, 2), (3, 0), (0, 2), (0, 0), (0, 2), (0, 2), (0, 1), (3, 1), (3, 0)], 1), ([(0, 3), (0, 0), (0, 3), (1, 2), (0, 3), (1, 3), (2, 3), (5, None)], 7), ([(0, 3), (1, 2), (0, 2), (0, 1), (3, 2), (3, 3), (4, 0), (1, 3), (4, 3), (5, None)], 9), ([(0, 2), (0, 2), (0, 0), (0, 0), (0, 0), (1, 3), (2, 3), (2, 0), (2, 2), (1, 3)], 0), ([(0, 3), (1, 3), (1, 3), (2, 2), (1, 3), (1, 2), (0, 0), (0, 0), (0, 3), (1, 3)], 0), ([(0, 2), (0, 1), (3, 1), (3, 3), (4, 2), (3, 0), (0, 1), (1, 1), (4, 2), (3, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 38
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.25585743 -0.86343669  0.43475746]
True reward weights: [-0.49248773 -0.22865786  0.83974486]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 38
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.40768627 0.88706094 0.21659822]
True reward weights: [ 0.8885372  -0.40835519 -0.20915949]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 38
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.50678587 0.84835001 0.15320035]
True reward weights: [-0.13918941  0.85763935  0.49505662]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.5932
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.51983565 0.74812805 0.41240189]
True reward weights: [-0.09829033 -0.39094703 -0.91514995]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.328702

Running EBIRL with 5 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.5944
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.29994473 0.93833114 0.17195298]
True reward weights: [-0.56112027  0.82771142  0.00615293]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.315538

Running EBIRL with 6 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.5994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.30592349 0.91554842 0.26111665]
True reward weights: [0.09046942 0.89068321 0.44553193]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.307711

Saving results to files...
Results saved successfully.

Running experiment 39/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 0), (0, 2), (0, 2), (0, 2), (0, 3), (1, 1), (4, 1), (4, 0)], 0), ([(0, 2), (0, 3), (1, 3), (1, 2), (0, 1), (3, 1), (3, 2), (3, 3), (4, 1), (4, 3)], 0), ([(0, 2), (0, 0), (0, 3), (1, 0), (1, 2), (1, 1), (4, 1), (4, 2), (3, 0), (0, 1)], 0), ([(0, 1), (0, 3), (1, 0), (1, 1), (4, 1), (4, 0), (3, 2), (3, 3), (4, 0), (1, 0)], 0), ([(0, 2), (0, 2), (0, 1), (1, 1), (4, 1), (5, None)], 5), ([(0, 2), (0, 1), (3, 3), (0, 3), (1, 3), (2, 0), (2, 3), (2, 2), (1, 1), (2, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.6000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.84621852 -0.1525492  -0.51053204]
True reward weights: [-0.56481614 -0.46100063  0.68444222]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running EBIRL with 2 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.5942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.61190534 -0.11187967  0.78297815]
True reward weights: [ 0.47321446  0.67884248 -0.56146323]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123879

Running EBIRL with 3 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.5940
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.0547461   0.92545589 -0.37487898]
True reward weights: [-0.61596524 -0.10867158  0.78024183]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123927

Running EBIRL with 4 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.5930
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.56623965  0.75694422  0.32620257]
True reward weights: [-0.49898897  0.3880993  -0.77484769]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.124048

Running EBIRL with 5 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.5996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.69002449  0.59495297 -0.41218584]
True reward weights: [ 0.35667154  0.60992175 -0.70765873]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.148145

Running EBIRL with 6 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.6046
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.13264795  0.45167299  0.88226755]
True reward weights: [-0.17491782  0.81876206  0.54683859]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.148057

Running experiment 40/50...
Shuffled Demos: [([(0, 2), (0, 0), (0, 0), (0, 2), (0, 3), (1, 3), (2, 1), (5, None)], 7), ([(0, 0), (0, 2), (0, 1), (3, 2), (0, 3), (1, 3), (4, 1), (5, None)], 7), ([(0, 2), (0, 3), (3, 0), (0, 3), (1, 1), (4, 2), (3, 0), (0, 2), (0, 0), (0, 0)], 0), ([(0, 1), (3, 1), (3, 3), (4, 2), (3, 1), (3, 3), (4, 1), (4, 1), (4, 2), (3, 3)], 0), ([(0, 2), (0, 2), (3, 1), (3, 2), (3, 1), (3, 3), (4, 2), (3, 3), (4, 2), (3, 0)], 0), ([(0, 2), (0, 3), (1, 3), (4, 0), (1, 0), (1, 0), (1, 3), (2, 2), (5, None)], 8)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 40
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.56101159 -0.74961301 -0.35120697]
True reward weights: [-0.6667631  -0.57474284  0.47444455]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 40
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.24429147  0.80681008  0.53793975]
True reward weights: [-0.45749673  0.35219891 -0.81648801]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.5986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.34221775 0.90174245 0.26409765]
True reward weights: [ 0.60665894 -0.67883576 -0.41369909]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.253284

Running EBIRL with 4 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6020
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.00952988 0.96291123 0.26965004]
True reward weights: [-0.34195784  0.76150362 -0.55061518]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.309064

Running EBIRL with 5 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.5982
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.35867925  0.93146501  0.0610092 ]
True reward weights: [-0.72035936  0.67649848 -0.15307579]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.379092

Running EBIRL with 6 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.5984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.4425675  0.88736744 0.12927891]
True reward weights: [-0.99720562 -0.07394066 -0.01066403]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.455889

Saving results to files...
Results saved successfully.

Running experiment 41/50...
Shuffled Demos: [([(0, 3), (3, 0), (0, 2), (0, 2), (0, 1), (3, 1), (3, 1), (3, 1), (3, 3), (4, 1)], 0), ([(0, 3), (3, 0), (0, 1), (3, 1), (3, 2), (0, 2), (0, 1), (3, 1), (3, 3), (4, 1)], 0), ([(0, 2), (0, 1), (3, 2), (3, 1), (3, 0), (0, 1), (3, 2), (3, 0), (0, 3), (1, 0)], 0), ([(0, 1), (3, 1), (3, 0), (0, 0), (0, 3), (3, 2), (0, 3), (1, 2), (0, 2), (0, 0)], 0), ([(0, 0), (0, 3), (1, 3), (2, 3), (2, 1), (2, 1), (2, 2), (5, None)], 7), ([(0, 2), (0, 0), (0, 1), (3, 1), (3, 1), (3, 0), (0, 0), (0, 3), (1, 1), (4, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 41
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.49350453 -0.86399159  0.099859  ]
True reward weights: [-0.46247666 -0.0267168   0.88622884]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 41
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.13474574 0.88539995 0.44487134]
True reward weights: [ 0.21656182  0.90600876 -0.36366071]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.01755532 0.87216793 0.48889151]
True reward weights: [ 0.74614533 -0.46431017  0.47716163]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.252609

Running EBIRL with 4 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5856
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.10936701 0.94288474 0.31465414]
True reward weights: [-0.7368091  -0.14555434 -0.66024714]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.238620

Running EBIRL with 5 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.06105631  0.98675257 -0.15030466]
True reward weights: [-0.45142457 -0.00123564 -0.89230843]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.300958

Running EBIRL with 6 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.12794806 0.82105213 0.55632966]
True reward weights: [-0.47234657 -0.42218622  0.77372315]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.296119

Running experiment 42/50...
Shuffled Demos: [([(0, 1), (3, 1), (3, 2), (3, 0), (0, 2), (0, 3), (0, 1), (3, 1), (3, 1), (3, 2)], 0), ([(0, 3), (1, 2), (0, 3), (1, 3), (2, 3), (2, 2), (1, 2), (0, 2), (0, 1), (3, 1)], 0), ([(0, 3), (1, 1), (4, 2), (1, 0), (2, 3), (2, 2), (1, 3), (2, 0), (2, 3), (2, 0)], 0), ([(0, 2), (3, 3), (4, 1), (4, 2), (3, 3), (4, 0), (3, 2), (3, 2), (3, 2), (3, 2)], 0), ([(0, 2), (0, 2), (0, 1), (3, 3), (4, 0), (1, 1), (4, 1), (5, None)], 7), ([(0, 2), (0, 3), (1, 2), (4, 1), (4, 3), (5, None)], 5)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 42
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.28227471  0.87099771  0.40209946]
True reward weights: [-0.71505276 -0.68463496  0.14133129]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.07362002 0.94197575 0.32750845]
True reward weights: [ 0.97551368 -0.09269772  0.19944974]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182892

Running EBIRL with 3 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.10375111  0.85507168 -0.50802375]
True reward weights: [-0.81755449 -0.5339423  -0.21566241]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170131

Running EBIRL with 4 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.10159685  0.81834365 -0.56567814]
True reward weights: [-0.96159102 -0.25945746 -0.08957977]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.218966

Running EBIRL with 5 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.15199904 0.97207562 0.17878835]
True reward weights: [-0.31780306  0.66399922  0.67683547]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.285753

Running EBIRL with 6 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.6010
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.32215869 0.94330737 0.07990614]
True reward weights: [0.64969697 0.73972999 0.17519531]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.361491

Saving results to files...
Results saved successfully.

Running experiment 43/50...
Shuffled Demos: [([(0, 0), (0, 2), (0, 1), (0, 1), (3, 2), (3, 0), (0, 2), (0, 1), (1, 3), (2, 1)], 0), ([(0, 1), (3, 0), (0, 3), (1, 3), (2, 0), (2, 0), (2, 1), (5, None)], 0), ([(0, 3), (1, 3), (2, 0), (2, 3), (2, 1), (1, 1), (0, 3), (1, 1), (2, 1), (5, None)], 0), ([(0, 1), (3, 1), (3, 0), (0, 0), (0, 3), (1, 3), (2, 2), (1, 0), (1, 2), (0, 0)], 0), ([(0, 1), (3, 3), (4, 1), (5, None)], 3), ([(0, 0), (1, 2), (0, 0), (0, 2), (0, 1), (3, 2), (3, 2), (3, 0), (0, 1), (3, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.6098
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.86478703 -0.33762598  0.37168817]
True reward weights: [-0.94655708 -0.29794542  0.12352418]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running EBIRL with 2 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5426
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.86456705  0.20263191  0.45985228]
True reward weights: [-0.85217816  0.40010982 -0.337201  ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.133086

Running EBIRL with 3 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5428
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.97702442 -0.1163421   0.17857156]
True reward weights: [-0.90996668  0.25349887 -0.32817519]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.129440

Running EBIRL with 4 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5370
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.32944739  0.72549052 -0.60425815]
True reward weights: [-0.99936718  0.02992649  0.01922625]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.128275

Running EBIRL with 5 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5478
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.4944026   0.86791719 -0.04781024]
True reward weights: [-0.8713506   0.06371557 -0.48650639]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.151833

Running EBIRL with 6 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5450
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.50734982  0.86129887 -0.02757577]
True reward weights: [-0.23135865  0.24096491 -0.94255455]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.151313

Running experiment 44/50...
Shuffled Demos: [([(0, 3), (1, 3), (4, 0), (1, 0), (2, 3), (2, 1), (2, 0), (2, 2), (5, None)], 0), ([(0, 3), (1, 1), (4, 0), (1, 0), (2, 2), (1, 1), (4, 1), (4, 1), (4, 0), (1, 2)], 0), ([(0, 2), (3, 1), (3, 2), (3, 3), (4, 3), (5, None)], 5), ([(0, 0), (0, 1), (3, 2), (3, 1), (3, 1), (4, 3), (1, 0), (1, 1), (4, 2), (3, 1)], 0), ([(0, 0), (0, 0), (0, 0), (0, 3), (3, 3), (3, 3), (4, 2), (3, 0), (0, 0), (0, 2)], 0), ([(0, 0), (0, 3), (1, 0), (0, 3), (1, 0), (2, 1), (1, 2), (0, 3), (1, 1), (4, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.6042
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.5157856  0.79224804 0.32604947]
True reward weights: [-0.88315502 -0.32822573  0.33511952]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running EBIRL with 2 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5776
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.84992814 -0.14506535  0.50653549]
True reward weights: [ 0.24179809  0.65288436 -0.71782707]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.130374

Running EBIRL with 3 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5576
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.460897    0.41252999 -0.78574357]
True reward weights: [-0.95554244 -0.197013    0.21937305]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152254

Running EBIRL with 4 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5640
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.81852651  0.54694979 -0.17567094]
True reward weights: [-0.25750053  0.71154464  0.65375661]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.151215

Running EBIRL with 5 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5608
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.26799695  0.84099412 -0.47000694]
True reward weights: [-0.98139786  0.04855724 -0.18574293]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.205399

Running EBIRL with 6 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5704
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.07416994  0.99131067 -0.10863692]
True reward weights: [0.60635011 0.79369447 0.04887364]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.204964

Saving results to files...
Results saved successfully.

Running experiment 45/50...
Shuffled Demos: [([(0, 2), (0, 2), (0, 2), (0, 0), (0, 3), (1, 1), (4, 1), (3, 1), (3, 3), (4, 3)], 0), ([(0, 2), (0, 1), (3, 0), (0, 2), (0, 2), (3, 2), (0, 2), (0, 2), (0, 3), (1, 1)], 0), ([(0, 3), (3, 0), (0, 3), (1, 3), (1, 3), (2, 1), (2, 0), (2, 2), (2, 0), (2, 1)], 0), ([(0, 3), (1, 3), (2, 3), (5, None)], 3), ([(0, 1), (3, 2), (3, 0), (0, 1), (3, 3), (4, 2), (3, 0), (0, 2), (0, 3), (1, 3)], 0), ([(0, 3), (1, 3), (2, 1), (5, None)], 3)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6028
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.10649287  0.5018028  -0.85840155]
True reward weights: [-0.83381491 -0.37890594  0.40147601]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running EBIRL with 2 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.5964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.51648403  0.41558296 -0.74868889]
True reward weights: [-0.86258852 -0.42573997  0.27328834]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123771

Running EBIRL with 3 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6012
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.48411268 0.85719446 0.17564902]
True reward weights: [ 0.37349459  0.77364955 -0.51182826]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123791

Running EBIRL with 4 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.5980
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.56601833  0.72087271  0.39995723]
True reward weights: [ 0.28902978  0.91199246 -0.29108684]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.148008

Running EBIRL with 5 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6050
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.45986307  0.49960623  0.73411142]
True reward weights: [-0.61799937 -0.33651123 -0.7105188 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.148212

Running EBIRL with 6 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6044
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.2999022   0.76992927 -0.56326512]
True reward weights: [-0.80644015  0.33153849  0.48962896]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.202996

Running experiment 46/50...
Shuffled Demos: [([(0, 3), (1, 0), (1, 0), (1, 2), (0, 0), (0, 3), (1, 0), (1, 0), (1, 3), (2, 1)], 0), ([(0, 1), (3, 3), (4, 3), (5, None)], 3), ([(0, 0), (0, 1), (1, 2), (0, 3), (1, 0), (0, 2), (0, 1), (3, 1), (3, 3), (4, 3)], 0), ([(0, 1), (3, 0), (4, 2), (3, 3), (4, 3), (5, None)], 5), ([(0, 3), (1, 1), (0, 2), (0, 1), (3, 2), (3, 3), (4, 3), (5, None)], 7), ([(0, 2), (0, 2), (0, 2), (0, 1), (3, 0), (0, 3), (0, 3), (1, 1), (4, 2), (3, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6042
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.3952058  -0.32138263  0.86053796]
True reward weights: [-0.82098277 -0.18440172  0.54035479]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running EBIRL with 2 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6050
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.35862028  0.61840028  0.69926575]
True reward weights: [0.44171827 0.87033865 0.2177053 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147645

Running EBIRL with 3 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6090
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.15405061  0.9463268  -0.28413729]
True reward weights: [-0.98573312  0.12799207  0.10930803]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.147819

Running EBIRL with 4 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.35803598  0.91461281 -0.18786602]
True reward weights: [-0.31599105  0.08126661 -0.9452753 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202655

Running EBIRL with 5 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.07899498  0.95098045 -0.29899159]
True reward weights: [-0.16662902  0.96631744  0.19612589]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.272917

Running EBIRL with 6 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.5039331   0.86303462 -0.03496681]
True reward weights: [-0.6393273   0.43631659  0.63315751]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.273243

Saving results to files...
Results saved successfully.

Running experiment 47/50...
Shuffled Demos: [([(0, 3), (1, 0), (1, 0), (1, 0), (0, 1), (3, 2), (3, 3), (4, 0), (1, 1), (4, 2)], 0), ([(0, 2), (0, 0), (0, 1), (3, 3), (4, 1), (5, None)], 5), ([(0, 1), (0, 0), (0, 1), (0, 0), (1, 3), (4, 3), (5, None)], 6), ([(0, 2), (0, 3), (1, 2), (0, 2), (0, 1), (3, 0), (0, 3), (1, 1), (2, 0), (2, 2)], 0), ([(0, 0), (0, 3), (1, 0), (1, 1), (4, 3), (5, None)], 0), ([(0, 1), (3, 0), (0, 0), (1, 3), (2, 1), (5, None)], 5)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.34420618  0.14588579  0.92749094]
True reward weights: [-0.73857451 -0.23755048  0.63093381]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6104
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.23497948  0.32770598  0.91509203]
True reward weights: [-0.32525586  0.84802621 -0.41840193]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147799

Running EBIRL with 3 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.3482154  0.92697283 0.13952563]
True reward weights: [-0.8629335   0.12161366  0.49046497]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.202598

Running EBIRL with 4 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6060
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.1183     0.93742547 0.32747304]
True reward weights: [-9.88123122e-01 -1.53663715e-01 -3.97121311e-04]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202742

Running EBIRL with 5 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5572
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.17295051 0.94504175 0.2774603 ]
True reward weights: [-0.707685    0.51517243 -0.4835073 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.213730

Running EBIRL with 6 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5560
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.14372074 0.98794995 0.05743899]
True reward weights: [-0.66787006 -0.28250482 -0.68857869]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.282100

Running experiment 48/50...
Shuffled Demos: [([(0, 0), (1, 1), (4, 3), (5, None)], 0), ([(0, 2), (3, 1), (3, 3), (4, 0), (5, None)], 4), ([(0, 3), (0, 2), (0, 3), (1, 1), (4, 1), (4, 2), (4, 1), (4, 3), (5, None)], 0), ([(0, 3), (1, 2), (0, 1), (3, 1), (3, 2), (3, 2), (0, 1), (3, 1), (3, 1), (3, 1)], 0), ([(0, 3), (1, 0), (1, 0), (1, 1), (4, 0), (1, 3), (2, 3), (2, 3), (2, 3), (2, 2)], 0), ([(0, 3), (1, 3), (2, 3), (2, 2), (2, 2), (1, 0), (0, 1), (3, 0), (0, 1), (1, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.6124
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.60886132  0.47678453 -0.63400663]
True reward weights: [-0.95541883 -0.18852986  0.22722531]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running EBIRL with 2 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.6008
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.66009168  0.49962675  0.56093858]
True reward weights: [-0.79654573 -0.37374758 -0.47521327]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147797

Running EBIRL with 3 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.8392369   0.40855605 -0.35883614]
True reward weights: [-0.87675378  0.46593839  0.11918151]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148027

Running EBIRL with 4 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5384
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.95062219  0.30049574  0.07758706]
True reward weights: [-0.60010139 -0.17730496 -0.78002646]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.162739

Running EBIRL with 5 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.26691143  0.78082949 -0.56485714]
True reward weights: [ 0.33969384  0.51333697 -0.7880947 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.160137

Running EBIRL with 6 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5460
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.78499965  0.59786284  0.16228242]
True reward weights: [0.38137888 0.58470062 0.71601351]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.158133

Saving results to files...
Results saved successfully.

Running experiment 49/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 1), (3, 1), (4, 1), (4, 2), (3, 0), (0, 2), (0, 3), (1, 3)], 0), ([(0, 1), (0, 2), (3, 1), (3, 2), (3, 0), (0, 0), (0, 3), (0, 0), (0, 2), (0, 0)], 0), ([(0, 3), (1, 2), (0, 3), (1, 3), (2, 3), (2, 0), (2, 1), (5, None)], 0), ([(0, 0), (0, 1), (3, 1), (3, 2), (3, 2), (3, 2), (3, 0), (0, 2), (0, 2), (0, 3)], 0), ([(0, 3), (1, 2), (0, 2), (0, 3), (3, 0), (0, 0), (0, 2), (0, 0), (0, 2), (3, 1)], 0), ([(0, 2), (0, 3), (1, 3), (2, 3), (2, 2), (1, 2), (0, 1), (0, 2), (0, 1), (0, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5900
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.04742853  0.79866413 -0.59990511]
True reward weights: [-0.98484463 -0.1489005   0.08893646]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running EBIRL with 2 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5912
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.20379695 0.9353588  0.28908601]
True reward weights: [-0.62126673 -0.08704256 -0.77874979]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148048

Running EBIRL with 3 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5666
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.01517397  0.97298839 -0.23035481]
True reward weights: [-0.19008788  0.71081428  0.67720724]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.156651

Running EBIRL with 4 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5594
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.11502703  0.94291623 -0.31253443]
True reward weights: [-0.95340141 -0.25953962  0.1538341 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.209571

Running EBIRL with 5 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5758
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.19092988  0.93158629  0.30934248]
True reward weights: [-0.03999372  0.9583411   0.28281236]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.208381

Running EBIRL with 6 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5568
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.02353911  0.95571203 -0.29336057]
True reward weights: [0.48470547 0.86542716 0.12687177]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.207489

Running experiment 50/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 0), (1, 1), (4, 2), (3, 1), (3, 3), (4, 2), (3, 2), (0, 0)], 0), ([(0, 1), (3, 1), (3, 3), (4, 2), (3, 3), (4, 2), (3, 0), (0, 1), (0, 0), (0, 1)], 0), ([(0, 1), (3, 0), (0, 1), (3, 2), (0, 2), (0, 3), (1, 0), (1, 2), (4, 1), (4, 3)], 0), ([(0, 1), (3, 3), (4, 0), (1, 3), (2, 2), (1, 3), (4, 1), (4, 2), (3, 0), (0, 0)], 0), ([(0, 2), (0, 3), (1, 0), (1, 2), (0, 1), (3, 2), (3, 3), (4, 2), (1, 3), (2, 1)], 0), ([(0, 2), (0, 2), (0, 3), (1, 2), (0, 2), (0, 1), (3, 3), (4, 0), (3, 2), (3, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.5914
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.79784019  0.45952341 -0.39024257]
True reward weights: [-0.47840873 -0.41855607  0.77196885]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123672

Running EBIRL with 2 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.6018
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.56436042  0.81784312 -0.11238302]
True reward weights: [-0.37657218  0.87498185 -0.30430274]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148255

Running EBIRL with 3 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.5866
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.08728549  0.85394237 -0.51299481]
True reward weights: [ 0.05123867  0.35827197 -0.93221017]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148383

Running EBIRL with 4 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.5954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.3738081   0.92287993  0.09252103]
True reward weights: [-0.37305839  0.92445678  0.07878513]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.148245

Running EBIRL with 5 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.6052
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.02996374  0.91623779 -0.3995128 ]
True reward weights: [-0.90922569  0.38276753  0.16369991]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.148375

Running EBIRL with 6 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.6022
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.00811382  0.98185304  0.18946971]
True reward weights: [0.29864671 0.94791497 0.11075804]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.148367

Saving results to files...
Results saved successfully.
