Config file loaded successfully.
Feature weights for environment: [-0.43287867 -0.20494655 -0.09382998  0.87281665]
Feature weights for environment: [-0.2322253  -0.00465236  0.43453482  0.87018921]
Feature weights for environment: [-0.6467127  -0.20075906  0.06245093  0.73318372]
Feature weights for environment: [-0.69449962 -0.28165855  0.22055331  0.62425554]
Feature weights for environment: [-0.58844088 -0.39506705 -0.37329827  0.59858814]
Feature weights for environment: [-0.62480116 -0.58775562  0.29021502  0.42419581]
Feature weights for environment: [-0.86648592 -0.43712062 -0.12519764  0.20604187]
Feature weights for environment: [-0.66202557 -0.27713495  0.4113014   0.56191594]
Feature weights for environment: [-0.70843905 -0.48585435  0.3607345   0.3632221 ]
Feature weights for environment: [-0.59500066 -0.16244209  0.37080191  0.69432898]
Feature weights for environment: [-0.0702138  -0.06544465  0.14444996  0.98484579]
Feature weights for environment: [-0.58571354 -0.27832207  0.48101309  0.59000244]
Feature weights for environment: [-0.83842198 -0.21600546  0.03565977  0.49911783]
Feature weights for environment: [-0.27410583  0.06391249  0.48081808  0.83041867]
Feature weights for environment: [-0.67868375 -0.62331644 -0.1171628   0.3703213 ]
Feature weights for environment: [-0.92679793  0.00735039  0.0581375   0.37096037]
Feature weights for environment: [-0.34813921  0.02418599  0.41049361  0.84244235]
Feature weights for environment: [-0.70806601  0.20943992  0.21689523  0.63854045]
Feature weights for environment: [-0.55653737  0.11873141  0.39462253  0.72141671]
Initialized 19 GridWorld environments.
Generated optimal policies for all environments.
Config file loaded successfully.
Feature weights for environment: [-0.43287867 -0.20494655 -0.09382998  0.87281665]
Feature weights for environment: [-0.2322253  -0.00465236  0.43453482  0.87018921]
Feature weights for environment: [-0.6467127  -0.20075906  0.06245093  0.73318372]
Feature weights for environment: [-0.69449962 -0.28165855  0.22055331  0.62425554]
Feature weights for environment: [-0.58844088 -0.39506705 -0.37329827  0.59858814]
Feature weights for environment: [-0.62480116 -0.58775562  0.29021502  0.42419581]
Feature weights for environment: [-0.86648592 -0.43712062 -0.12519764  0.20604187]
Feature weights for environment: [-0.66202557 -0.27713495  0.4113014   0.56191594]
Feature weights for environment: [-0.70843905 -0.48585435  0.3607345   0.3632221 ]
Feature weights for environment: [-0.59500066 -0.16244209  0.37080191  0.69432898]
Feature weights for environment: [-0.0702138  -0.06544465  0.14444996  0.98484579]
Feature weights for environment: [-0.58571354 -0.27832207  0.48101309  0.59000244]
Feature weights for environment: [-0.83842198 -0.21600546  0.03565977  0.49911783]
Feature weights for environment: [-0.27410583  0.06391249  0.48081808  0.83041867]
Feature weights for environment: [-0.67868375 -0.62331644 -0.1171628   0.3703213 ]
Feature weights for environment: [-0.92679793  0.00735039  0.0581375   0.37096037]
Feature weights for environment: [-0.34813921  0.02418599  0.41049361  0.84244235]
Feature weights for environment: [-0.70806601  0.20943992  0.21689523  0.63854045]
Feature weights for environment: [-0.55653737  0.11873141  0.39462253  0.72141671]
Initialized 19 GridWorld environments.
Generated optimal policies for all environments.
Shuffled demonstration order: [16, 12, 9, 19, 18, 6, 5, 10, 15, 21, 11, 17, 1, 14, 22, 13, 2, 23, 4, 24, 7, 8, 0, 3, 20]

Running experiment 1/19...
Shuffled Demos: [([(0, 1), (5, 1), (10, 2), (10, 0)], 0), ([(0, 3), (1, 1), (6, 0), (1, 2)], 0), ([(0, 2), (0, 0), (0, 2), (0, 1)], 0), ([(0, 1), (5, 1), (10, 1), (15, 3)], 0), ([(0, 2), (0, 1), (5, 0), (0, 3)], 0), ([(0, 2), (0, 0), (0, 1), (5, 0)], 0), ([(0, 2), (0, 1), (5, 1), (10, 1)], 0), ([(0, 3), (1, 1), (6, 0), (1, 3)], 0), ([(0, 1), (5, 1), (10, 1), (15, 2)], 0), ([(0, 0), (0, 3), (1, 1), (6, 0)], 0), ([(0, 3), (1, 0), (1, 1), (6, 1)], 0), ([(0, 2), (0, 0), (0, 0), (0, 3)], 0), ([(0, 1), (5, 0), (0, 3), (1, 2)], 0), ([(0, 1), (5, 0), (0, 0), (0, 3)], 0), ([(0, 0), (0, 3), (1, 0), (1, 3)], 0), ([(0, 1), (5, 2), (5, 2), (5, 1)], 0), ([(0, 0), (0, 1), (5, 3), (6, 3)], 0), ([(0, 2), (0, 3), (1, 2), (0, 3)], 0), ([(0, 0), (0, 1), (5, 0), (0, 2)], 0), ([(0, 1), (5, 3), (6, 1), (11, 3)], 0), ([(0, 1), (5, 2), (5, 3), (6, 3)], 0), ([(0, 3), (1, 3), (2, 1), (7, 3)], 0), ([(0, 3), (1, 3), (2, 3), (3, 2)], 0), ([(0, 1), (5, 3), (6, 1), (11, 3)], 0), ([(0, 2), (0, 3), (1, 3), (2, 3)], 0), ([(0, 0), (0, 0), (0, 1), (5, 1)], 0), ([(0, 1), (5, 0), (0, 3), (1, 2)], 0), ([(0, 2), (0, 1), (5, 3), (6, 3)], 0), ([(0, 2), (0, 1), (5, 2), (5, 0)], 0), ([(0, 1), (5, 1), (10, 3), (11, 2)], 0), ([(0, 3), (1, 0), (1, 1), (6, 3)], 0), ([(0, 0), (0, 1), (5, 2), (5, 2)], 0), ([(0, 2), (0, 1), (5, 0), (0, 2)], 0), ([(0, 1), (5, 2), (5, 0), (0, 1)], 0), ([(0, 3), (1, 2), (0, 1), (5, 3)], 0), ([(0, 3), (1, 0), (1, 0), (1, 3)], 0), ([(0, 3), (1, 1), (6, 2), (5, 1)], 0), ([(0, 3), (1, 2), (0, 1), (5, 3)], 0), ([(0, 2), (0, 2), (0, 3), (1, 1)], 0), ([(0, 3), (1, 3), (2, 2), (1, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5876
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.08589152 -0.40918748  0.43437424  0.79781406]
True reward weights: [-0.43287867 -0.20494655 -0.09382998  0.87281665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.128229

Running EBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4598
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.30865773 -0.44488519 -0.35998471 -0.75974902]
True reward weights: [ 0.05055516 -0.26929292 -0.89781579  0.34474962]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123003

Running EBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4058
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.52970163 -0.11863492 -0.11673359  0.83169418]
True reward weights: [ 0.1688473  -0.80583202 -0.53069292 -0.20122219]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.176428

Running EBIRL with 4 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.56276874 -0.30566036 -0.73174142 -0.23327577]
True reward weights: [-0.32128821 -0.44058021 -0.16428671 -0.82199321]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.203189

Running EBIRL with 5 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3902
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.43854257 -0.58318531 -0.23051843 -0.64376747]
True reward weights: [-0.04359656 -0.24309137 -0.81040506 -0.53127165]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.206303

Running EBIRL with 6 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.25073946 -0.87579392 -0.30511353  0.27752563]
True reward weights: [ 0.36589166 -0.37566456 -0.74517614 -0.4119611 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.208131

Running EBIRL with 7 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [ 0.28667211 -0.3561198  -0.84196083  0.28653055]
True reward weights: [-0.10341445 -0.83778054 -0.14887134  0.51504033]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.210649

Running EBIRL with 8 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [ 0.41466112 -0.46061279 -0.74444358 -0.24838633]
True reward weights: [ 0.08190899 -0.51988711 -0.82063582  0.22263235]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.211895

Running EBIRL with 9 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [ 0.07065124 -0.92521948 -0.37219238  0.0212165 ]
True reward weights: [-0.70384899 -0.24691546 -0.25621347 -0.61480404]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.213122

Running EBIRL with 10 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3928
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [ 0.41933024 -0.81806021 -0.20171418  0.3380104 ]
True reward weights: [ 0.15939682 -0.59548318 -0.75809052  0.21281729]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.213686

Running EBIRL with 11 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.51697991 -0.52560655 -0.60209064  0.30652307]
True reward weights: [ 0.33793689 -0.81377777 -0.05472759 -0.46964805]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.216492

Running EBIRL with 12 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3858
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [ 0.2197081  -0.56926415 -0.7815097  -0.13003564]
True reward weights: [-0.50643653 -0.47301213 -0.58779323  0.41746938]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.288957

Running EBIRL with 13 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3890
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.93438435 -0.32601544 -0.13705342 -0.04308343]
True reward weights: [-0.37395625 -0.48897104 -0.13184221  0.77696955]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.288814

Running EBIRL with 14 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.16658419 -0.85010405 -0.37161684 -0.33387683]
True reward weights: [-6.10952559e-04 -6.51692129e-01 -4.78233254e-01 -5.88718907e-01]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.291061

Running EBIRL with 15 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3832
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.31435461 -0.81525379 -0.48025615  0.07678839]
True reward weights: [-0.7003955   0.00464076 -0.21282884  0.68126976]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.370393

Running EBIRL with 16 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3876
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.41863451 -0.29660662 -0.11703302  0.85033695]
True reward weights: [ 0.08829013 -0.22909323 -0.61641074  0.7481704 ]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.371063

Running EBIRL with 17 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.73178971 -0.37140776 -0.303787   -0.48399748]
True reward weights: [-0.22545819 -0.88129937 -0.18993268 -0.3693313 ]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.371235

Running EBIRL with 18 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3850
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.38575762 -0.27780658 -0.86527979 -0.15907685]
True reward weights: [ 0.57140654 -0.48408896 -0.64527079  0.15092399]
MAP Policy for current environment:
Information gain 18 demonstrations: 0.454050

Running EBIRL with 19 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.10894484 -0.71546496 -0.66572918  0.18178441]
True reward weights: [-3.06458662e-01 -9.14214896e-01 -2.65130502e-01 -1.71948177e-04]
MAP Policy for current environment:
Information gain 19 demonstrations: 0.454430

Running EBIRL with 20 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [ 0.55443455 -0.51023087 -0.62128232 -0.21511641]
True reward weights: [-0.14638907 -0.03724117 -0.98803818  0.0310465 ]
MAP Policy for current environment:
Information gain 20 demonstrations: 0.454880

Running EBIRL with 21 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.67788312 -0.33937297 -0.50794556 -0.40901317]
True reward weights: [ 0.04857918 -0.20034577 -0.51914521 -0.82945156]
MAP Policy for current environment:
Information gain 21 demonstrations: 0.455167

Running EBIRL with 22 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [ 0.01217028 -0.27707017 -0.71883288  0.63746631]
True reward weights: [-0.58687441 -0.72221277 -0.03561371 -0.36430592]
MAP Policy for current environment:
Information gain 22 demonstrations: 0.455393

Running EBIRL with 23 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3912
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.22639667 -0.96361704 -0.14205546  0.00264496]
True reward weights: [ 0.66027443 -0.47579223 -0.54614247 -0.19846369]
MAP Policy for current environment:
Information gain 23 demonstrations: 0.481125

Running EBIRL with 24 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.00982402 -0.64510735 -0.70357628  0.29785972]
True reward weights: [-0.61110244 -0.69493549 -0.29497807 -0.23792101]
MAP Policy for current environment:
Information gain 24 demonstrations: 0.481417

Running EBIRL with 25 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3836
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [ 0.1272512  -0.36570638 -0.20382352 -0.89917849]
True reward weights: [ 0.11559395 -0.98730486 -0.10468421 -0.03013914]
MAP Policy for current environment:
Information gain 25 demonstrations: 0.481684

Running EBIRL with 26 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3956
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [ 0.28606778 -0.46292841 -0.82745148 -0.13851553]
True reward weights: [-0.29138638 -0.38543047 -0.64900794 -0.58764448]
MAP Policy for current environment:
Information gain 26 demonstrations: 0.482027

Running EBIRL with 27 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3824
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.49504467 -0.39864138 -0.60871005 -0.47485566]
True reward weights: [-0.56175924 -0.42924686 -0.20216036  0.67772035]
MAP Policy for current environment:
Information gain 27 demonstrations: 0.482450

Running EBIRL with 28 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3784
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.44397596 -0.44347276 -0.77541534  0.07034419]
True reward weights: [-0.02844418 -0.78225263 -0.05677115 -0.61971669]
MAP Policy for current environment:
Information gain 28 demonstrations: 0.482660

Running EBIRL with 29 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3926
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.21700777 -0.63576855 -0.55446058 -0.49120204]
True reward weights: [-0.04278012 -0.2927656  -0.01700748 -0.95507534]
MAP Policy for current environment:
Information gain 29 demonstrations: 0.482583

Running EBIRL with 30 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3728
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.14889206 -0.89912583 -0.28192653 -0.29986885]
True reward weights: [ 0.3214893  -0.76331302 -0.41249057  0.37927482]
MAP Policy for current environment:
Information gain 30 demonstrations: 0.482467

Running EBIRL with 31 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3874
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.73624438 -0.28565789 -0.35485159 -0.50042395]
True reward weights: [-0.03480333 -0.7269568  -0.57881469 -0.36782616]
MAP Policy for current environment:
Information gain 31 demonstrations: 0.482587

Running EBIRL with 32 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.51024343 -0.46861395 -0.65468009  0.30240137]
True reward weights: [ 0.26926939 -0.59505576 -0.2180419  -0.7251623 ]
MAP Policy for current environment:
Information gain 32 demonstrations: 0.482608

Running EBIRL with 33 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.81192001 -0.32660958 -0.4495379  -0.17896299]
True reward weights: [-0.32965781 -0.67140243 -0.65944867  0.07531239]
MAP Policy for current environment:
Information gain 33 demonstrations: 0.482731

Running EBIRL with 34 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3874
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [ 0.01925817 -0.17817233 -0.982074   -0.05843289]
True reward weights: [-0.71050025 -0.06523269 -0.48292523  0.50765866]
MAP Policy for current environment:
Information gain 34 demonstrations: 0.482862

Running EBIRL with 35 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3806
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.30784654 -0.47838881 -0.37164716  0.7336573 ]
True reward weights: [-0.42303524 -0.25776154 -0.37489315  0.78361681]
MAP Policy for current environment:
Information gain 35 demonstrations: 0.483009

Running EBIRL with 36 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3856
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.33552601 -0.30130243 -0.31934775  0.83346035]
True reward weights: [-0.60307479 -0.17743901 -0.77020667  0.10769345]
MAP Policy for current environment:
Information gain 36 demonstrations: 0.567523
Config file loaded successfully.
Feature weights for environment: [-0.43287867 -0.20494655 -0.09382998  0.87281665]
Feature weights for environment: [-0.2322253  -0.00465236  0.43453482  0.87018921]
Feature weights for environment: [-0.6467127  -0.20075906  0.06245093  0.73318372]
Feature weights for environment: [-0.69449962 -0.28165855  0.22055331  0.62425554]
Feature weights for environment: [-0.58844088 -0.39506705 -0.37329827  0.59858814]
Feature weights for environment: [-0.62480116 -0.58775562  0.29021502  0.42419581]
Feature weights for environment: [-0.86648592 -0.43712062 -0.12519764  0.20604187]
Feature weights for environment: [-0.66202557 -0.27713495  0.4113014   0.56191594]
Feature weights for environment: [-0.70843905 -0.48585435  0.3607345   0.3632221 ]
Feature weights for environment: [-0.59500066 -0.16244209  0.37080191  0.69432898]
Feature weights for environment: [-0.0702138  -0.06544465  0.14444996  0.98484579]
Feature weights for environment: [-0.58571354 -0.27832207  0.48101309  0.59000244]
Feature weights for environment: [-0.83842198 -0.21600546  0.03565977  0.49911783]
Feature weights for environment: [-0.27410583  0.06391249  0.48081808  0.83041867]
Feature weights for environment: [-0.67868375 -0.62331644 -0.1171628   0.3703213 ]
Feature weights for environment: [-0.92679793  0.00735039  0.0581375   0.37096037]
Feature weights for environment: [-0.34813921  0.02418599  0.41049361  0.84244235]
Feature weights for environment: [-0.70806601  0.20943992  0.21689523  0.63854045]
Feature weights for environment: [-0.55653737  0.11873141  0.39462253  0.72141671]
Initialized 19 GridWorld environments.
Generated optimal policies for all environments.
Shuffled demonstration order: [16, 12, 9, 19, 18, 6, 5, 10, 15, 21, 11, 17, 1, 14, 22, 13, 2, 23, 4, 24, 7, 8, 0, 3, 20]

Running experiment 1/19...
Shuffled Demos: [([(0, 3), (1, 0), (1, 1), (6, 1)], 0), ([(0, 3), (1, 3), (2, 3), (3, 2)], 0), ([(0, 2), (0, 0), (0, 0), (0, 3)], 0), ([(0, 0), (0, 0), (0, 1), (5, 1)], 0), ([(0, 1), (5, 2), (5, 2), (5, 1)], 0), ([(0, 3), (1, 2), (0, 1), (5, 3)], 0), ([(0, 1), (5, 3), (6, 1), (11, 3)], 0), ([(0, 2), (0, 3), (1, 3), (2, 3)], 0), ([(0, 1), (5, 3), (6, 1), (11, 3)], 0), ([(0, 1), (5, 0), (0, 0), (0, 3)], 0), ([(0, 2), (0, 1), (5, 0), (0, 3)], 0), ([(0, 2), (0, 0), (0, 1), (5, 0)], 0), ([(0, 2), (0, 1), (5, 0), (0, 2)], 0), ([(0, 1), (5, 1), (10, 1), (15, 3)], 0), ([(0, 2), (0, 1), (5, 2), (5, 0)], 0), ([(0, 3), (1, 0), (1, 0), (1, 3)], 0), ([(0, 0), (0, 3), (1, 1), (6, 0)], 0), ([(0, 2), (0, 1), (5, 3), (6, 3)], 0), ([(0, 3), (1, 0), (1, 1), (6, 3)], 0), ([(0, 2), (0, 1), (5, 1), (10, 1)], 0), ([(0, 3), (1, 2), (0, 1), (5, 3)], 0), ([(0, 0), (0, 1), (5, 0), (0, 2)], 0), ([(0, 3), (1, 1), (6, 0), (1, 3)], 0), ([(0, 0), (0, 1), (5, 3), (6, 3)], 0), ([(0, 1), (5, 1), (10, 3), (11, 2)], 0), ([(0, 0), (0, 3), (1, 0), (1, 3)], 0), ([(0, 3), (1, 1), (6, 0), (1, 2)], 0), ([(0, 2), (0, 3), (1, 2), (0, 3)], 0), ([(0, 3), (1, 3), (2, 2), (1, 3)], 0), ([(0, 1), (5, 1), (10, 2), (10, 0)], 0), ([(0, 3), (1, 1), (6, 2), (5, 1)], 0), ([(0, 1), (5, 2), (5, 0), (0, 1)], 0), ([(0, 1), (5, 0), (0, 3), (1, 2)], 0), ([(0, 1), (5, 1), (10, 1), (15, 2)], 0), ([(0, 1), (5, 0), (0, 3), (1, 2)], 0), ([(0, 1), (5, 2), (5, 3), (6, 3)], 0), ([(0, 2), (0, 0), (0, 2), (0, 1)], 0), ([(0, 2), (0, 2), (0, 3), (1, 1)], 0), ([(0, 0), (0, 1), (5, 2), (5, 2)], 0), ([(0, 3), (1, 3), (2, 1), (7, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.15443549  0.175151   -0.45376418  0.85998249]
True reward weights: [-0.43287867 -0.20494655 -0.09382998  0.87281665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.164221

Running EBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5232
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.12041383  0.17168441 -0.73776335 -0.64166207]
True reward weights: [ 0.42231437 -0.80031336  0.09013343  0.4159628 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.188896

Running EBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4690
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.56020001 -0.49882983 -0.65879068 -0.05778917]
True reward weights: [ 0.65216627 -0.75507414 -0.05145928  0.04352178]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.260229

Running EBIRL with 4 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4740
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.53801775 -0.25450266 -0.78769915  0.1590451 ]
True reward weights: [-0.64640537 -0.06142789 -0.44681403 -0.61542176]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.269629

Running EBIRL with 5 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4010
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.56344588 -0.52481755 -0.46355856 -0.43841618]
True reward weights: [-0.58297103 -0.19084895 -0.78931336 -0.0265683 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.270996

Running EBIRL with 6 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3988
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.07371073 -0.19533131 -0.97715265 -0.03981331]
True reward weights: [-0.59260351 -0.19713371 -0.29182465  0.72442926]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.267233

Running EBIRL with 7 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.13526836 -0.26687268 -0.82522965 -0.47903806]
True reward weights: [-0.78496219 -0.51302148 -0.01405959  0.34705281]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.264199

Running EBIRL with 8 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.41747895 -0.71813316 -0.54818431 -0.09741695]
True reward weights: [ 0.19342542 -0.89744275 -0.21014807 -0.33618583]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.262542

Running EBIRL with 9 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3918
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.72770508 -0.45205887 -0.25623242 -0.4476975 ]
True reward weights: [-0.42187866 -0.69779319 -0.48861224 -0.31042092]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.261638

Running EBIRL with 10 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [ 0.1525004  -0.49311298 -0.71905146  0.46534741]
True reward weights: [-0.35495297 -0.12857532 -0.16216775  0.91168986]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.260272

Running EBIRL with 11 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4010
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.44396783 -0.81294654 -0.36983518  0.07233551]
True reward weights: [-0.50332503 -0.78132882 -0.36774285 -0.03089313]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.261150

Running EBIRL with 12 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3874
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [ 0.32112444 -0.31561135 -0.62913303 -0.63360887]
True reward weights: [-0.49780521 -0.59994293 -0.6207108   0.08352571]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.260325

Running EBIRL with 13 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4034
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.23746562 -0.53071012 -0.77427782  0.2499014 ]
True reward weights: [-0.41748312 -0.85416354 -0.22852549 -0.20949605]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.257879

Running EBIRL with 14 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.52092338 -0.35484444 -0.21969006 -0.74462107]
True reward weights: [ 0.14018731 -0.1885753  -0.91689141 -0.3226407 ]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.259420

Running EBIRL with 15 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3866
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [ 0.63303766 -0.70094542 -0.30953658 -0.1101179 ]
True reward weights: [-0.3894198  -0.03544635 -0.82985762 -0.3980353 ]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.257863

Running EBIRL with 16 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.51758271 -0.74803179 -0.40849708 -0.07541026]
True reward weights: [-0.09309231 -0.90038453 -0.21208629 -0.36832177]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.332736

Running EBIRL with 17 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3874
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [ 0.15326854 -0.96129595 -0.21707801 -0.07277355]
True reward weights: [-0.4758188  -0.41903716 -0.52162632 -0.57088555]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.331539

Running EBIRL with 18 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.42512906 -0.31196442 -0.75809865  0.38370551]
True reward weights: [-0.60478118 -0.59660984 -0.05862599  0.52427036]
MAP Policy for current environment:
Information gain 18 demonstrations: 0.330771

Running EBIRL with 19 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3852
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [ 0.16330455 -0.40078766 -0.7704994  -0.46800806]
True reward weights: [ 0.0767844  -0.79236307 -0.41527119 -0.44024397]
MAP Policy for current environment:
Information gain 19 demonstrations: 0.330450

Running EBIRL with 20 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.62817735 -0.39226804 -0.16777485  0.65066935]
True reward weights: [ 0.25882538 -0.79761321 -0.54475037  0.0083438 ]
MAP Policy for current environment:
Information gain 20 demonstrations: 0.330281

Running EBIRL with 21 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.19489152 -0.36625782 -0.63689587 -0.64979701]
True reward weights: [ 0.21599248 -0.54249283 -0.78034236 -0.22386285]
MAP Policy for current environment:
Information gain 21 demonstrations: 0.330260

Running EBIRL with 22 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3886
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.25120267 -0.69665127 -0.66676107  0.08368935]
True reward weights: [ 0.33626327 -0.80083837 -0.19046583 -0.45750156]
MAP Policy for current environment:
Information gain 22 demonstrations: 0.330392

Running EBIRL with 23 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3872
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [ 0.05850901 -0.92793608 -0.31880863 -0.18404452]
True reward weights: [-0.40048037 -0.43561282 -0.80353145  0.06476227]
MAP Policy for current environment:
Information gain 23 demonstrations: 0.330149

Running EBIRL with 24 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3848
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [ 0.3366216  -0.82913955 -0.19078245 -0.40350411]
True reward weights: [ 0.37613799 -0.56537542 -0.69995405  0.22121296]
MAP Policy for current environment:
Information gain 24 demonstrations: 0.330467

Running EBIRL with 25 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3826
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.45179941 -0.53879233 -0.37313208  0.60527066]
True reward weights: [ 0.26509639 -0.18470422 -0.20025212 -0.9249364 ]
MAP Policy for current environment:
Information gain 25 demonstrations: 0.330088

Running EBIRL with 26 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3950
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.029029   -0.16442931 -0.57219648  0.80293929]
True reward weights: [ 0.05851776 -0.73543305 -0.66494792  0.11643953]
MAP Policy for current environment:
Information gain 26 demonstrations: 0.410543

Running EBIRL with 27 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3832
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [ 0.00254403 -0.79580438 -0.25568169  0.54892239]
True reward weights: [-0.56173183 -0.42824022 -0.20159431  0.67854801]
MAP Policy for current environment:
Information gain 27 demonstrations: 0.410414

Running EBIRL with 28 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3790
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.71265699 -0.67962789 -0.1714164  -0.02902354]
True reward weights: [ 0.10759401 -0.82891034 -0.05301785  0.5463701 ]
MAP Policy for current environment:
Information gain 28 demonstrations: 0.493700

Running EBIRL with 29 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [ 0.42960891 -0.65503565 -0.56539025 -0.25826025]
True reward weights: [ 0.02122638 -0.78422105 -0.48312498 -0.38876347]
MAP Policy for current environment:
Information gain 29 demonstrations: 0.493226

Running EBIRL with 30 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3730
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.03082012 -0.22349633 -0.48525643  0.8447637 ]
True reward weights: [ 0.34605925 -0.59474187 -0.48887947  0.53621075]
MAP Policy for current environment:
Information gain 30 demonstrations: 0.492583

Running EBIRL with 31 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3892
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.85777237 -0.13895696 -0.48677483  0.08926247]
True reward weights: [ 0.58387042 -0.58595697 -0.4897294  -0.27552654]
MAP Policy for current environment:
Information gain 31 demonstrations: 0.492153

Running EBIRL with 32 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3826
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [ 0.03302785 -0.59938181 -0.33942467 -0.72418333]
True reward weights: [-0.95821677 -0.12099855 -0.17547992 -0.19075315]
MAP Policy for current environment:
Information gain 32 demonstrations: 0.491901

Running EBIRL with 33 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.29851319 -0.36312757 -0.40647817 -0.78345628]
True reward weights: [ 0.23660825 -0.34327276 -0.23961301 -0.876793  ]
MAP Policy for current environment:
Information gain 33 demonstrations: 0.491855

Running EBIRL with 34 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.07838956 -0.23279186 -0.86979764  0.42791949]
True reward weights: [-0.1964191  -0.01634006 -0.4281165   0.88196871]
MAP Policy for current environment:
Information gain 34 demonstrations: 0.491847

Running EBIRL with 35 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3746
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [ 0.04094211 -0.55777    -0.24927477  0.79061904]
True reward weights: [-0.61460322 -0.6399031  -0.41834373  0.19435899]
MAP Policy for current environment:
Information gain 35 demonstrations: 0.491839

Running EBIRL with 36 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3850
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [ 0.08314903 -0.50506075 -0.44493452 -0.73486948]
True reward weights: [ 0.18355911 -0.75212803 -0.63292678 -0.00362881]
MAP Policy for current environment:
Information gain 36 demonstrations: 0.491722

Running EBIRL with 37 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3750
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.30154569 -0.41451929 -0.85167603  0.10904999]
True reward weights: [-0.09273656 -0.11967551 -0.93914793  0.3083486 ]
MAP Policy for current environment:
Information gain 37 demonstrations: 0.576029

Running EBIRL with 38 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.56592464 -0.14663246 -0.55384668 -0.59285924]
True reward weights: [ 0.47392343 -0.80568184 -0.03044129  0.35404332]
MAP Policy for current environment:
Information gain 38 demonstrations: 0.661396

Running EBIRL with 39 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.50836739 -0.42109996 -0.68169286  0.31548735]
True reward weights: [-0.42693226 -0.86981101 -0.18678691  0.16207502]
MAP Policy for current environment:
Information gain 39 demonstrations: 0.661339

Running EBIRL with 40 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3826
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [ 0.28498091 -0.67316402 -0.4288485   0.53077777]
True reward weights: [-0.24300086 -0.43717615 -0.73335924 -0.46044741]
MAP Policy for current environment:
Information gain 40 demonstrations: 0.661408

Running experiment 2/19...
Shuffled Demos: [([(0, 3), (1, 3), (2, 3), (3, 2)], 0), ([(0, 1), (5, 0), (0, 1), (5, 1)], 3), ([(0, 3), (1, 1), (6, 3), (7, 2)], 3), ([(0, 2), (0, 2), (0, 1), (5, 0)], 3), ([(0, 1), (5, 2), (5, 3), (6, 2)], 3), ([(0, 2), (0, 1), (5, 3), (6, 0)], 3), ([(0, 2), (0, 2), (0, 1), (5, 3)], 3), ([(0, 3), (1, 3), (2, 2), (1, 0)], 0), ([(0, 0), (0, 1), (5, 0), (0, 2)], 2), ([(0, 0), (0, 0), (0, 2), (0, 2)], 0), ([(0, 3), (1, 3), (2, 3), (3, 2)], 0), ([(0, 2), (0, 2), (0, 2), (0, 2)], 0), ([(0, 1), (5, 1), (10, 0), (5, 3)], 3), ([(0, 3), (1, 1), (6, 2), (5, 3)], 3), ([(0, 1), (5, 0), (0, 0), (0, 2)], 1), ([(0, 0), (0, 2), (0, 0), (0, 3)], 0), ([(0, 2), (0, 3), (1, 0), (1, 3)], 0), ([(0, 0), (0, 1), (5, 1), (10, 2)], 2), ([(0, 2), (0, 2), (0, 2), (0, 2)], 0), ([(0, 0), (0, 2), (0, 1), (5, 2)], 3), ([(0, 2), (0, 0), (0, 3), (1, 3)], 0), ([(0, 2), (0, 2), (0, 1), (5, 2)], 3), ([(0, 0), (0, 2), (0, 2), (0, 0)], 0), ([(0, 1), (5, 3), (6, 3), (7, 0)], 3), ([(0, 2), (0, 3), (1, 3), (2, 0)], 0), ([(0, 3), (1, 1), (6, 2), (5, 1)], 3), ([(0, 2), (0, 0), (0, 1), (5, 3)], 3), ([(0, 0), (0, 0), (0, 0), (0, 3)], 0), ([(0, 1), (5, 2), (5, 1), (10, 1)], 2), ([(0, 2), (0, 2), (0, 0), (0, 3)], 0), ([(0, 2), (0, 3), (1, 1), (6, 0)], 3), ([(0, 3), (1, 1), (6, 0), (1, 2)], 2), ([(0, 3), (1, 1), (6, 0), (1, 1)], 2), ([(0, 0), (0, 1), (5, 2), (5, 3)], 3), ([(0, 3), (1, 0), (1, 1), (6, 3)], 0), ([(0, 3), (1, 3), (2, 2), (1, 0)], 0), ([(0, 0), (0, 1), (5, 1), (10, 0)], 2), ([(0, 1), (5, 2), (5, 2), (5, 1)], 3), ([(0, 1), (5, 0), (0, 0), (0, 0)], 1), ([(0, 1), (5, 1), (10, 0), (5, 3)], 3)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5810
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.62122895  0.26331381 -0.58701527  0.44738519]
True reward weights: [-0.2322253  -0.00465236  0.43453482  0.87018921]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.136981

Running EBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.56092997  0.15593888  0.48082785 -0.65562582]
True reward weights: [-0.27833738 -0.77677321 -0.4617064  -0.32554398]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.184339

Running EBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5796
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.87648494  0.24629805  0.39221026  0.13146305]
True reward weights: [-0.4968655   0.22943139 -0.83054196  0.10337293]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.250986

Running EBIRL with 4 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5826
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.44738103 -0.17870621  0.81691256 -0.31712486]
True reward weights: [ 0.10302571 -0.76177952 -0.4944301   0.40572963]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.327341

Running EBIRL with 5 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.60914507  0.59098081 -0.49320295 -0.19087904]
True reward weights: [-0.67364241  0.42325945  0.23673964 -0.55768422]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.408370

Running EBIRL with 6 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5872
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.14723476 -0.18627776  0.82537091  0.51223566]
True reward weights: [-0.69908468 -0.04757511  0.08965174  0.70779925]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.491457

Running EBIRL with 7 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5936
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.14329827 -0.24874481  0.93563226 -0.20538719]
True reward weights: [ 0.15227966 -0.84305653  0.27262221  0.43788552]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.576211

Running EBIRL with 8 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5446
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.53754454  0.50309169 -0.23421414  0.6348924 ]
True reward weights: [ 0.10922056 -0.23311573  0.66560722  0.70049622]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.574363

Running EBIRL with 9 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4110
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.4048756  -0.48575732  0.58700342  0.50551217]
True reward weights: [-0.84741682  0.50317865  0.1069021  -0.13140744]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.670723

Running EBIRL with 10 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4088
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.46129323 -0.44767602  0.7654141  -0.03059376]
True reward weights: [-0.75225132 -0.65345117 -0.06629914  0.05219144]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.754635

Running EBIRL with 11 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4208
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.36499568 -0.42813542  0.35417407 -0.74702004]
True reward weights: [-0.38499438 -0.29271123  0.87511983 -0.01627131]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.753816

Running EBIRL with 12 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4008
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.51331994 -0.39693109  0.55156881 -0.52413758]
True reward weights: [-0.53054829 -0.04297023  0.78516647  0.31652122]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.838594

Running EBIRL with 13 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4100
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.15069355 -0.47907746  0.58555849 -0.63631556]
True reward weights: [-0.23322839 -0.37966406 -0.62077402  0.64505762]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.922496

Running EBIRL with 14 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4050
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.48571347 -0.43247268  0.75213453 -0.10650569]
True reward weights: [-0.43067467 -0.16541059  0.48976588  0.73978919]
MAP Policy for current environment:
Information gain 14 demonstrations: 1.006569

Running EBIRL with 15 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4114
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.53640162 -0.3581816   0.76403952  0.01492848]
True reward weights: [-0.52216196 -0.06665993  0.10979097 -0.84311879]
MAP Policy for current environment:
Information gain 15 demonstrations: 1.007315

Running EBIRL with 16 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4028
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.16220527 -0.64529243  0.72174096 -0.19072787]
True reward weights: [-0.65386329 -0.71509584  0.11907902 -0.21661237]
MAP Policy for current environment:
Information gain 16 demonstrations: 1.092203

Running EBIRL with 17 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.52169136 -0.54292379  0.64048134  0.15118047]
True reward weights: [-0.52705594 -0.61155076 -0.51087604  0.29533605]
MAP Policy for current environment:
Information gain 17 demonstrations: 1.091260

Running EBIRL with 18 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4110
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.26287938 -0.60305015  0.75188608  0.04350038]
True reward weights: [-0.57528829 -0.77178011  0.13649799 -0.23402383]
MAP Policy for current environment:
Information gain 18 demonstrations: 1.091618

Running EBIRL with 19 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4146
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.58653978 -0.62427944  0.49233083  0.1544559 ]
True reward weights: [-0.05905135 -0.71112553  0.22487638 -0.66350888]
MAP Policy for current environment:
Information gain 19 demonstrations: 1.177069

Running EBIRL with 20 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4062
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.32247446 -0.46789878  0.78542442  0.24533535]
True reward weights: [-0.12022563 -0.49318498  0.85725743  0.08616302]
MAP Policy for current environment:
Information gain 20 demonstrations: 1.262968

Running EBIRL with 21 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4020
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.4058779  -0.59528858  0.62425997  0.30198364]
True reward weights: [-0.93506756 -0.02816914  0.25986091  0.23943156]
MAP Policy for current environment:
Information gain 21 demonstrations: 1.263023

Running EBIRL with 22 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4054
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.23498311 -0.3340756   0.18944523 -0.89290925]
True reward weights: [-0.4630722  -0.10860636  0.11640521 -0.87190517]
MAP Policy for current environment:
Information gain 22 demonstrations: 1.348295

Running EBIRL with 23 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4084
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.581253   -0.53217075  0.60403967  0.11863946]
True reward weights: [-0.03312996 -0.66540035 -0.68472567  0.29545818]
MAP Policy for current environment:
Information gain 23 demonstrations: 1.433723

Running EBIRL with 24 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4048
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.76277818 -0.33291632  0.53753489 -0.13561864]
True reward weights: [-0.18904919 -0.91045351  0.18971516 -0.31518718]
MAP Policy for current environment:
Information gain 24 demonstrations: 1.518988

Running EBIRL with 25 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4014
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.41598346 -0.45830456  0.55826905  0.55249467]
True reward weights: [-0.10953261 -0.34090081  0.27327232 -0.892811  ]
MAP Policy for current environment:
Information gain 25 demonstrations: 1.518253

Running EBIRL with 26 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4022
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.55337169 -0.49314866  0.5903165  -0.31954749]
True reward weights: [-0.07086575 -0.93785751  0.06925658 -0.3325731 ]
MAP Policy for current environment:
Information gain 26 demonstrations: 1.603636

Running EBIRL with 27 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4088
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.34610234 -0.77351294  0.46973596  0.24746522]
True reward weights: [-0.51345494 -0.51342233  0.67305041  0.14058691]
MAP Policy for current environment:
Information gain 27 demonstrations: 1.688820

Running EBIRL with 28 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4118
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.25091423 -0.4188545   0.35143557 -0.79880911]
True reward weights: [-0.29940026 -0.2348198  -0.82367503 -0.42045047]
MAP Policy for current environment:
Information gain 28 demonstrations: 1.774909

Running EBIRL with 29 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4028
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.59160521 -0.59954431  0.5066728  -0.18393633]
True reward weights: [-0.74917028 -0.6266366  -0.09927431  0.19030257]
MAP Policy for current environment:
Information gain 29 demonstrations: 1.775162

Running EBIRL with 30 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4034
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.31774448 -0.54574744  0.60808817  0.48106855]
True reward weights: [-0.39787175 -0.65816926 -0.63842702 -0.03036835]
MAP Policy for current environment:
Information gain 30 demonstrations: 1.860776

Running EBIRL with 31 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4046
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.76658722 -0.35468143  0.4679335  -0.259968  ]
True reward weights: [-0.20685212 -0.83402462 -0.33367671  0.38765318]
MAP Policy for current environment:
Information gain 31 demonstrations: 1.946243

Running EBIRL with 32 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4022
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.57962182 -0.59085156  0.48380101 -0.28437574]
True reward weights: [-0.45740025 -0.8371064   0.24229039  0.17701199]
MAP Policy for current environment:
Information gain 32 demonstrations: 1.946101

Running EBIRL with 33 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4068
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.49566199 -0.46284644  0.62122644 -0.39264497]
True reward weights: [-0.6897385  -0.29479575 -0.61497632  0.24322909]
MAP Policy for current environment:
Information gain 33 demonstrations: 1.945708

Running EBIRL with 34 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4058
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.50932997 -0.54638694  0.62990285  0.21275968]
True reward weights: [-0.43186874 -0.3003491   0.66135582 -0.53468522]
MAP Policy for current environment:
Information gain 34 demonstrations: 2.031122

Running EBIRL with 35 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3810
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.37870127 -0.35383736  0.22270011 -0.82570524]
True reward weights: [-0.74791657 -0.22586369  0.60189062  0.16533021]
MAP Policy for current environment:
Information gain 35 demonstrations: 2.100011

Running EBIRL with 36 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3806
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.61909093 -0.60261754  0.48221905  0.14506308]
True reward weights: [-0.16467733 -0.56214025  0.13779503 -0.79868156]
MAP Policy for current environment:
Information gain 36 demonstrations: 2.099218

Running EBIRL with 37 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3852
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.41559611 -0.3592003   0.33174359  0.76694277]
True reward weights: [-0.471972   -0.40596072 -0.63763029 -0.45372452]
MAP Policy for current environment:
Information gain 37 demonstrations: 2.099225

Running EBIRL with 38 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.62941245 -0.49824854  0.59135629 -0.07672088]
True reward weights: [-0.27405974 -0.81913512  0.171602   -0.47377385]
MAP Policy for current environment:
Information gain 38 demonstrations: 2.184546

Running EBIRL with 39 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3908
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.46265565 -0.549465    0.39545016  0.57241343]
True reward weights: [-0.24011107 -0.76772092 -0.33021456 -0.49387206]
MAP Policy for current environment:
Information gain 39 demonstrations: 2.184301

Running EBIRL with 40 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.7491383  -0.34748791  0.19461323 -0.52931055]
True reward weights: [-0.6710235  -0.53135392 -0.18955048 -0.48110403]
MAP Policy for current environment:
Information gain 40 demonstrations: 2.269902

Saving results to files...
Results saved successfully.

Running experiment 3/19...
Shuffled Demos: [([(0, 0), (0, 0), (0, 2), (0, 1)], 0), ([(0, 0), (0, 3), (1, 0), (1, 3)], 0), ([(0, 2), (0, 3), (1, 2), (0, 0)], 0), ([(0, 0), (0, 2), (0, 1), (5, 2)], 0), ([(0, 0), (0, 3), (1, 2), (0, 1)], 0), ([(0, 2), (0, 3), (1, 2), (0, 1)], 0), ([(0, 0), (0, 3), (1, 0), (1, 3)], 0), ([(0, 3), (1, 1), (6, 1), (11, 3)], 0), ([(0, 1), (5, 0), (0, 1), (5, 3)], 1), ([(0, 0), (0, 0), (0, 0), (0, 1)], 0), ([(0, 2), (0, 1), (5, 0), (0, 0)], 0), ([(0, 2), (0, 3), (1, 1), (6, 3)], 0), ([(0, 0), (0, 3), (1, 0), (1, 3)], 0), ([(0, 2), (0, 3), (1, 3), (2, 0)], 0), ([(0, 2), (0, 3), (1, 3), (2, 3)], 0), ([(0, 2), (0, 3), (1, 2), (0, 0)], 0), ([(0, 1), (5, 1), (10, 1), (15, 0)], 0), ([(0, 2), (0, 0), (0, 0), (0, 0)], 0), ([(0, 3), (1, 3), (2, 3), (3, 1)], 1), ([(0, 3), (1, 0), (1, 0), (1, 0)], 3), ([(0, 1), (5, 1), (10, 1), (15, 3)], 0), ([(0, 1), (5, 2), (5, 3), (6, 3)], 2), ([(0, 1), (5, 3), (6, 3), (7, 0)], 1), ([(0, 3), (1, 2), (0, 2), (0, 1)], 0), ([(0, 1), (5, 2), (5, 2), (5, 0)], 3), ([(0, 3), (1, 3), (2, 2), (1, 3)], 1), ([(0, 1), (5, 2), (5, 3), (6, 2)], 2), ([(0, 0), (0, 1), (5, 1), (10, 2)], 0), ([(0, 2), (0, 1), (5, 0), (0, 0)], 0), ([(0, 0), (0, 3), (1, 2), (0, 0)], 0), ([(0, 2), (0, 2), (0, 1), (5, 2)], 0), ([(0, 0), (0, 0), (0, 3), (1, 1)], 0), ([(0, 1), (5, 3), (6, 1), (11, 1)], 0), ([(0, 1), (5, 3), (6, 3), (7, 3)], 1), ([(0, 3), (1, 3), (2, 0), (2, 2)], 0), ([(0, 3), (1, 0), (1, 3), (2, 1)], 2), ([(0, 1), (5, 2), (5, 3), (6, 2)], 2), ([(0, 1), (5, 3), (6, 0), (1, 3)], 1), ([(0, 3), (1, 2), (0, 3), (1, 0)], 1), ([(0, 3), (1, 0), (1, 1), (6, 1)], 2)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6044
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.36383382 -0.28400199  0.12729937  0.87793091]
True reward weights: [-0.6467127  -0.20075906  0.06245093  0.73318372]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.1798782  -0.15765355 -0.96861638 -0.06761293]
True reward weights: [-0.90381653 -0.00900003 -0.31116744 -0.29361454]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.248040

Running EBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4550
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.23423223 -0.56918909 -0.7435653  -0.26128468]
True reward weights: [-0.55054837 -0.63842298 -0.07468721  0.53266726]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.243564

Running EBIRL with 4 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4648
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.66125698 -0.28603865 -0.69132123  0.05473622]
True reward weights: [ 0.17185101 -0.0385261  -0.36180495 -0.91546718]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.240932

Running EBIRL with 5 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4688
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.45072209 -0.68979957 -0.01944272 -0.566258  ]
True reward weights: [-0.43607113 -0.17581353  0.04540391 -0.88140233]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.237517

Running EBIRL with 6 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4630
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.11972304 -0.76705313 -0.43710699 -0.45412924]
True reward weights: [-0.1121693  -0.22254101 -0.23381049  0.93980115]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.237284

Running EBIRL with 7 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4608
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.57171794 -0.33729509 -0.72317939 -0.19074119]
True reward weights: [ 0.16502887 -0.33337463  0.06358316  0.92605832]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.235570

Running EBIRL with 8 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4352
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [ 0.58903266 -0.55502662 -0.57182307 -0.13418033]
True reward weights: [-0.2726551  -0.73784889 -0.57046722 -0.23623157]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.294625

Running EBIRL with 9 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4320
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.65535517 -0.37257844 -0.64552522  0.1224422 ]
True reward weights: [-0.23727132 -0.79905873  0.12623646 -0.53783996]
MAP Policy for current environment:
Information gain 9 demonstrations: 1.031788

Running EBIRL with 10 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4332
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [ 0.04655291 -0.60396387 -0.70059735 -0.37712575]
True reward weights: [ 0.66229002 -0.54701178 -0.3771146   0.34631578]
MAP Policy for current environment:
Information gain 10 demonstrations: 1.118466

Running EBIRL with 11 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4344
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [ 0.01453496 -0.38140401 -0.92429045 -0.00262171]
True reward weights: [-0.52101009 -0.7461746  -0.40888703  0.06770043]
MAP Policy for current environment:
Information gain 11 demonstrations: 1.145408

Running EBIRL with 12 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4360
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [ 0.08051377 -0.42833507 -0.16815846  0.88417721]
True reward weights: [-0.10950967 -0.23534274 -0.04335927  0.9647494 ]
MAP Policy for current environment:
Information gain 12 demonstrations: 1.158551

Running EBIRL with 13 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4328
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [ 0.04815754 -0.63016983 -0.19478133  0.75008471]
True reward weights: [-0.52658893 -0.76466319 -0.33098767  0.16864599]
MAP Policy for current environment:
Information gain 13 demonstrations: 1.161692

Running EBIRL with 14 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4390
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.28605207 -0.50256539 -0.46901882 -0.66755044]
True reward weights: [-0.73214632 -0.63581199 -0.23826401  0.05417696]
MAP Policy for current environment:
Information gain 14 demonstrations: 1.173524

Running EBIRL with 15 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4402
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.01430359 -0.65345924 -0.00621901 -0.756801  ]
True reward weights: [-0.69501108 -0.4585371  -0.5465084   0.08962083]
MAP Policy for current environment:
Information gain 15 demonstrations: 1.187675

Running EBIRL with 16 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4316
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.70864733 -0.67520607 -0.15399233 -0.13491512]
True reward weights: [-0.52882799 -0.24604094 -0.53681879  0.60961496]
MAP Policy for current environment:
Information gain 16 demonstrations: 1.204532

Running EBIRL with 17 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3936
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.73152911 -0.50416256 -0.24121966  0.39051037]
True reward weights: [-0.22994133 -0.54844144  0.05905243  0.80178038]
MAP Policy for current environment:
Information gain 17 demonstrations: 1.195282

Running EBIRL with 18 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4014
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [ 0.14731633 -0.43260066 -0.40123541  0.79382915]
True reward weights: [-0.21560513 -0.90100073 -0.35605479 -0.12221744]
MAP Policy for current environment:
Information gain 18 demonstrations: 1.277097

Running EBIRL with 19 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4042
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [ 0.1912493  -0.78827873 -0.54685899  0.20732966]
True reward weights: [ 0.39134179 -0.54143419 -0.69229935 -0.27280439]
MAP Policy for current environment:
Information gain 19 demonstrations: 1.585475

Running EBIRL with 20 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3974
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.77435505 -0.5364489   0.07127227 -0.32789802]
True reward weights: [ 0.14005392 -0.05307519 -0.93097332  0.33295134]
MAP Policy for current environment:
Information gain 20 demonstrations: 1.671346

Running EBIRL with 21 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3936
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.6781823  -0.48863854  0.05837009 -0.54579674]
True reward weights: [-0.4897896  -0.38747996 -0.66588874 -0.40811471]
MAP Policy for current environment:
Information gain 21 demonstrations: 1.672987

Running EBIRL with 22 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.82223538 -0.48818411  0.06069367 -0.28621938]
True reward weights: [ 0.04910468 -0.78156298 -0.2868247   0.55179673]
MAP Policy for current environment:
Information gain 22 demonstrations: 1.908729

Running EBIRL with 23 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.73617141 -0.63371331  0.10540781  0.21295137]
True reward weights: [-0.04838253 -0.97034814 -0.07113693 -0.22588307]
MAP Policy for current environment:
Information gain 23 demonstrations: 2.103798

Running EBIRL with 24 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4020
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.79459981 -0.52375652  0.08538871  0.29495597]
True reward weights: [ 0.09279685 -0.65482402 -0.18460238  0.7269912 ]
MAP Policy for current environment:
Information gain 24 demonstrations: 2.143380

Running EBIRL with 25 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3924
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.63996018 -0.5873824   0.10447715  0.48426998]
True reward weights: [ 0.48895242 -0.2568872  -0.81161778  0.19029211]
MAP Policy for current environment:
Information gain 25 demonstrations: 2.229418

Running EBIRL with 26 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4038
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.72427314 -0.65893893  0.14040754 -0.14667524]
True reward weights: [ 0.39019526 -0.65899891 -0.40678994  0.49798598]
MAP Policy for current environment:
Information gain 26 demonstrations: 2.411992

Running EBIRL with 27 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.66669486 -0.71856143  0.1312998   0.14814789]
True reward weights: [-0.30855139 -0.61982605 -0.72010517  0.04538995]
MAP Policy for current environment:
Information gain 27 demonstrations: 2.565711

Running EBIRL with 28 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3970
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.66734958 -0.72190708  0.15212193 -0.1017528 ]
True reward weights: [ 0.57962651 -0.32370039 -0.30602178 -0.68235023]
MAP Policy for current environment:
Information gain 28 demonstrations: 2.569425

Running EBIRL with 29 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3968
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.38248829 -0.36314542  0.01412406  0.84948727]
True reward weights: [-0.24074839 -0.55503174 -0.70797919  0.36434247]
MAP Policy for current environment:
Information gain 29 demonstrations: 2.616763

Running EBIRL with 30 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4026
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.67852738 -0.65618955  0.10994184 -0.31133368]
True reward weights: [-0.52944297 -0.08324079 -0.84296566  0.04658326]
MAP Policy for current environment:
Information gain 30 demonstrations: 2.662057

Running EBIRL with 31 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3966
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.7276655  -0.65175748  0.1149657   0.180272  ]
True reward weights: [ 0.40092162 -0.33521716 -0.78029143 -0.34356454]
MAP Policy for current environment:
Information gain 31 demonstrations: 2.699253

Running EBIRL with 32 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4044
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.63672812 -0.72186283  0.1326824  -0.23640378]
True reward weights: [-0.66813486 -0.2761093  -0.35632503  0.59193913]
MAP Policy for current environment:
Information gain 32 demonstrations: 2.737945

Running EBIRL with 33 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4054
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.85709799 -0.49604241  0.07885169 -0.11448745]
True reward weights: [-0.70042799 -0.31809594 -0.63136921  0.09792102]
MAP Policy for current environment:
Information gain 33 demonstrations: 2.738710

Running EBIRL with 34 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3900
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.76475226 -0.52881466  0.08644585  0.35782141]
True reward weights: [-0.20025562 -0.26334492 -0.01885111 -0.94349975]
MAP Policy for current environment:
Information gain 34 demonstrations: 2.902411

Running EBIRL with 35 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3928
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.6532398  -0.72745698  0.14321997  0.15353226]
True reward weights: [-0.40017756 -0.85250965  0.09519336 -0.32252665]
MAP Policy for current environment:
Information gain 35 demonstrations: 2.942740

Running EBIRL with 36 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3970
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.67930938 -0.60865823  0.09979501  0.39763662]
True reward weights: [-0.13916708 -0.35593853 -0.73913993 -0.55462821]
MAP Policy for current environment:
Information gain 36 demonstrations: 3.103672

Running EBIRL with 37 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3934
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.88735091 -0.44591219  0.04613144  0.10790073]
True reward weights: [-0.66018179 -0.59055069 -0.45974257 -0.06361334]
MAP Policy for current environment:
Information gain 37 demonstrations: 3.250802

Running EBIRL with 38 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3936
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.72550716 -0.63740059  0.12776391 -0.22591198]
True reward weights: [-0.14765565 -0.46924687 -0.56212755 -0.66484419]
MAP Policy for current environment:
Information gain 38 demonstrations: 3.388436

Running EBIRL with 39 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3916
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.71011596 -0.69073936  0.1273898   0.04884983]
True reward weights: [-0.82711576 -0.40927841 -0.37011919 -0.10668873]
MAP Policy for current environment:
Information gain 39 demonstrations: 3.518489

Running EBIRL with 40 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.3936
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.65454808 -0.60028001  0.12180903 -0.44316281]
True reward weights: [-0.45808039 -0.6591288  -0.27056298 -0.53151412]
MAP Policy for current environment:
Information gain 40 demonstrations: 3.642707

Running experiment 4/19...
Shuffled Demos: [([(0, 1), (5, 2), (5, 3), (6, 2)], 0), ([(0, 1), (5, 3), (6, 2), (5, 1)], 0), ([(0, 2), (0, 0), (0, 3), (1, 1)], 1), ([(0, 2), (0, 3), (1, 2), (0, 2)], 1), ([(0, 2), (0, 3), (1, 3), (2, 1)], 0), ([(0, 1), (5, 3), (6, 2), (5, 3)], 0), ([(0, 3), (1, 3), (2, 3), (3, 2)], 0), ([(0, 0), (0, 3), (1, 3), (2, 2)], 0), ([(0, 0), (0, 3), (1, 1), (6, 0)], 0), ([(0, 3), (1, 3), (2, 3), (3, 1)], 0), ([(0, 3), (1, 2), (0, 1), (5, 3)], 0), ([(0, 1), (5, 1), (10, 3), (11, 2)], 0), ([(0, 0), (0, 3), (1, 2), (0, 2)], 1), ([(0, 3), (1, 2), (0, 1), (5, 2)], 0), ([(0, 0), (0, 1), (5, 0), (0, 2)], 1), ([(0, 1), (5, 2), (5, 0), (0, 3)], 0), ([(0, 2), (0, 0), (0, 0), (0, 3)], 3), ([(0, 3), (1, 0), (1, 2), (0, 3)], 0), ([(0, 2), (0, 3), (1, 3), (2, 3)], 0), ([(0, 0), (0, 0), (0, 0), (0, 1)], 3), ([(0, 0), (0, 3), (1, 1), (6, 2)], 0), ([(0, 1), (5, 0), (0, 0), (0, 1)], 3), ([(0, 1), (5, 3), (6, 2), (5, 2)], 0), ([(0, 3), (1, 2), (0, 0), (0, 1)], 0), ([(0, 1), (5, 3), (6, 0), (1, 2)], 0), ([(0, 3), (1, 3), (2, 1), (7, 2)], 0), ([(0, 0), (0, 3), (1, 1), (6, 0)], 0), ([(0, 1), (5, 2), (5, 1), (10, 1)], 0), ([(0, 1), (5, 3), (6, 1), (11, 3)], 0), ([(0, 0), (0, 0), (0, 1), (5, 2)], 2), ([(0, 1), (5, 1), (10, 3), (11, 3)], 0), ([(0, 2), (0, 1), (5, 1), (10, 2)], 0), ([(0, 1), (5, 2), (5, 3), (6, 3)], 0), ([(0, 3), (1, 3), (2, 2), (1, 1)], 0), ([(0, 3), (1, 2), (0, 1), (5, 0)], 0), ([(0, 0), (0, 0), (0, 3), (1, 3)], 1), ([(0, 2), (0, 3), (1, 2), (0, 2)], 1), ([(0, 3), (1, 1), (6, 0), (1, 2)], 0), ([(0, 2), (0, 1), (5, 3), (6, 2)], 0), ([(0, 3), (1, 3), (2, 0), (2, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5846
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.55224146 -0.52953871 -0.39273376 -0.51027279]
True reward weights: [-0.69449962 -0.28165855  0.22055331  0.62425554]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.129186

Running EBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.48376489 -0.8036434   0.27323864  0.21323569]
True reward weights: [ 0.14860035 -0.43880203 -0.7302472   0.50210531]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.128172

Running EBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4480
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.96872559 -0.22793566  0.04040743 -0.08934937]
True reward weights: [-0.14124395 -0.37664173 -0.323162    0.85659645]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.139643

Running EBIRL with 4 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4426
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.76688802 -0.64108646  0.00925592  0.02837681]
True reward weights: [-0.9700102   0.20405405  0.12146346  0.05185341]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.139169

Running EBIRL with 5 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4288
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.77801446  0.12966653 -0.18456045  0.58635957]
True reward weights: [-0.96712206 -0.24637376 -0.03785125 -0.05041996]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.148363

Running EBIRL with 6 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4380
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.14323176 -0.60371662 -0.76372683  0.17813546]
True reward weights: [-0.80273986  0.13942361  0.53364857  0.22669138]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.148521

Running EBIRL with 7 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4178
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.82950769  0.16768166 -0.45984759  0.26896104]
True reward weights: [-0.24899131 -0.59974297 -0.64358797 -0.40510026]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.152147

Running EBIRL with 8 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4188
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.38690237 -0.43448074 -0.34806396  0.73510852]
True reward weights: [-0.72940592 -0.0118766   0.36936846 -0.57566734]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.154217

Running EBIRL with 9 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4186
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.50471606 -0.5397144   0.17130472  0.65163238]
True reward weights: [-0.38656721 -0.1901973   0.31958247 -0.84395368]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.149015

Running EBIRL with 10 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4116
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.49803452  0.03378803 -0.84478827 -0.19275055]
True reward weights: [-0.57072861 -0.31696401 -0.21809737  0.7254214 ]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.150247

Running EBIRL with 11 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4122
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.57545071 -0.42892977  0.16246027  0.67711328]
True reward weights: [-0.47564287 -0.30080901  0.05301583 -0.82490431]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.150544

Running EBIRL with 12 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4088
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.87327844 -0.44663329  0.05540417 -0.18663829]
True reward weights: [-0.00513193 -0.4070506  -0.90843062  0.09506461]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.143251

Running EBIRL with 13 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4144
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.98556288  0.07785117  0.00875467 -0.15009453]
True reward weights: [-0.91524228  0.32154048 -0.0799461  -0.22924205]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.146928

Running EBIRL with 14 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4152
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.65149256 -0.40167648  0.11752417 -0.63277289]
True reward weights: [-0.17597868 -0.93993676 -0.19257414  0.22014903]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.145710

Running EBIRL with 15 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3792
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.34143062 -0.55364045 -0.07707766 -0.75562319]
True reward weights: [-0.95753962  0.19973088  0.17748672 -0.108277  ]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.141666

Running EBIRL with 16 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3736
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.3359082  -0.18651837 -0.52672307 -0.75824757]
True reward weights: [-0.48107745 -0.79404393  0.09281586 -0.35978318]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.153233

Running EBIRL with 17 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3824
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.74828405 -0.50288275  0.07028642 -0.42689547]
True reward weights: [-0.66641361 -0.57824624  0.10101127 -0.45969655]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.209467

Running EBIRL with 18 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.64119321 -0.40406858 -0.04746883  0.65065088]
True reward weights: [-0.91667446 -0.26799695  0.12473341  0.26893707]
MAP Policy for current environment:
Information gain 18 demonstrations: 0.209233

Running EBIRL with 19 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3776
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.7475817  -0.62508241 -0.07066804 -0.21307184]
True reward weights: [-0.175337   -0.54152881 -0.60680564 -0.5547886 ]
MAP Policy for current environment:
Information gain 19 demonstrations: 0.211329

Running EBIRL with 20 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.53846307 -0.29456124 -0.07964089 -0.78546071]
True reward weights: [-0.8038015  -0.10329323 -0.36928425 -0.45482173]
MAP Policy for current environment:
Information gain 20 demonstrations: 0.282746

Running EBIRL with 21 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3768
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.56809798 -0.7558127   0.15996123 -0.2835917 ]
True reward weights: [-0.27720143 -0.67717531  0.13836136 -0.66741973]
MAP Policy for current environment:
Information gain 21 demonstrations: 0.283835

Running EBIRL with 22 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3782
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.73837237 -0.4892926   0.15898914 -0.43602919]
True reward weights: [-0.46838934 -0.43178851 -0.74040132  0.21442014]
MAP Policy for current environment:
Information gain 22 demonstrations: 0.362797

Running EBIRL with 23 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3750
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.76364842 -0.51174978  0.06424527  0.38836298]
True reward weights: [-0.75231553 -0.33195407 -0.12737481  0.55462013]
MAP Policy for current environment:
Information gain 23 demonstrations: 0.362982

Running EBIRL with 24 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3692
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.79731354 -0.43634753  0.0617289  -0.41240937]
True reward weights: [-0.52322927 -0.79234879  0.27243812 -0.15553777]
MAP Policy for current environment:
Information gain 24 demonstrations: 0.363591

Running EBIRL with 25 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3710
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.76243294 -0.43243428  0.04408803 -0.47932541]
True reward weights: [-0.80336353 -0.42534282 -0.19208295  0.36985762]
MAP Policy for current environment:
Information gain 25 demonstrations: 0.363835

Running EBIRL with 26 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3710
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.76986092 -0.57469112  0.15978158 -0.22696723]
True reward weights: [-0.23744957 -0.21151854 -0.10870402  0.94183918]
MAP Policy for current environment:
Information gain 26 demonstrations: 0.364387

Running EBIRL with 27 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3710
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.88692135 -0.4580861   0.05716568 -0.01611622]
True reward weights: [-0.41683582 -0.09572422 -0.80844087  0.40436139]
MAP Policy for current environment:
Information gain 27 demonstrations: 0.364771

Running EBIRL with 28 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3576
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.76024438 -0.5616035   0.15872722  0.28536934]
True reward weights: [-0.00601283 -0.61750354 -0.3694747   0.69436422]
MAP Policy for current environment:
Information gain 28 demonstrations: 0.365851

Running EBIRL with 29 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3554
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.67331147 -0.60921726  0.15981894 -0.38725173]
True reward weights: [-0.7434702  -0.09519003  0.04788322  0.66022581]
MAP Policy for current environment:
Information gain 29 demonstrations: 0.365062

Running EBIRL with 30 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3632
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.68287211 -0.42606938  0.06524355  0.58982527]
True reward weights: [-0.78591091 -0.1504181  -0.42941585 -0.41871287]
MAP Policy for current environment:
Information gain 30 demonstrations: 0.365929

Running EBIRL with 31 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3604
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.82452588 -0.47741675  0.0717467   0.29509783]
True reward weights: [-0.06076259 -0.94784651 -0.2932604  -0.10905611]
MAP Policy for current environment:
Information gain 31 demonstrations: 0.365552

Running EBIRL with 32 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3534
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.64207147 -0.46205572  0.06748473  0.60802512]
True reward weights: [-0.13198354 -0.04337929 -0.89651648 -0.42066231]
MAP Policy for current environment:
Information gain 32 demonstrations: 0.366100

Running EBIRL with 33 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3680
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.76346305 -0.44028156  0.06605475  0.4678815 ]
True reward weights: [-0.4021809  -0.10797743 -0.71937686  0.55595713]
MAP Policy for current environment:
Information gain 33 demonstrations: 0.365875

Running EBIRL with 34 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3570
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.73537149 -0.53042974  0.07156467  0.41563393]
True reward weights: [-0.49647382 -0.30228284 -0.36923589 -0.72512322]
MAP Policy for current environment:
Information gain 34 demonstrations: 0.366054

Running EBIRL with 35 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3576
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.88687002 -0.39516959  0.04108358 -0.23582769]
True reward weights: [-0.35280531 -0.25846962 -0.74059738 -0.51013468]
MAP Policy for current environment:
Information gain 35 demonstrations: 0.365614

Running EBIRL with 36 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3640
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.60313475 -0.43519522  0.04666592 -0.66682523]
True reward weights: [-0.67724156 -0.33651365 -0.3909019   0.5246886 ]
MAP Policy for current environment:
Information gain 36 demonstrations: 0.365794

Running EBIRL with 37 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3624
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.87903483 -0.47179945  0.0546213  -0.04146766]
True reward weights: [-0.81851515 -0.25372309 -0.03054947 -0.51451363]
MAP Policy for current environment:
Information gain 37 demonstrations: 0.366492

Running EBIRL with 38 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3624
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.82375591 -0.53287875  0.07569425 -0.1781483 ]
True reward weights: [-0.53398381 -0.65282569 -0.46441061 -0.27019012]
MAP Policy for current environment:
Information gain 38 demonstrations: 0.365787

Running EBIRL with 39 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3538
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.66575999 -0.59872213  0.15845489 -0.41615802]
True reward weights: [-0.62958983 -0.0524879  -0.65490953  0.41467478]
MAP Policy for current environment:
Information gain 39 demonstrations: 0.365826

Running EBIRL with 40 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3612
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.50705024 -0.45935398  0.06088228 -0.72676497]
True reward weights: [-0.09105625 -0.83734333 -0.53899649  0.00690566]
MAP Policy for current environment:
Information gain 40 demonstrations: 0.366014

Saving results to files...
Results saved successfully.

Running experiment 5/19...
Shuffled Demos: [([(0, 3), (1, 3), (2, 0), (2, 2)], 0), ([(0, 3), (1, 1), (6, 2), (5, 3)], 0), ([(0, 3), (1, 3), (2, 1), (7, 3)], 0), ([(0, 0), (0, 1), (5, 2), (5, 1)], 0), ([(0, 3), (1, 3), (2, 1), (7, 0)], 0), ([(0, 2), (0, 3), (1, 1), (6, 0)], 0), ([(0, 2), (0, 2), (0, 2), (0, 1)], 0), ([(0, 3), (1, 1), (6, 0), (1, 1)], 0), ([(0, 1), (5, 3), (6, 3), (7, 3)], 0), ([(0, 0), (0, 2), (0, 3), (1, 1)], 0), ([(0, 2), (0, 0), (0, 3), (1, 3)], 0), ([(0, 2), (0, 0), (0, 0), (0, 3)], 0), ([(0, 2), (0, 2), (0, 3), (1, 1)], 0), ([(0, 0), (0, 1), (5, 0), (0, 0)], 0), ([(0, 3), (1, 0), (1, 1), (6, 2)], 0), ([(0, 2), (0, 1), (5, 2), (5, 3)], 0), ([(0, 0), (0, 1), (5, 1), (10, 2)], 0), ([(0, 2), (0, 3), (1, 2), (0, 1)], 0), ([(0, 0), (0, 3), (1, 3), (2, 0)], 0), ([(0, 1), (5, 1), (10, 0), (5, 3)], 0), ([(0, 3), (1, 3), (2, 2), (1, 2)], 0), ([(0, 0), (0, 1), (5, 1), (10, 3)], 0), ([(0, 3), (1, 3), (2, 2), (1, 2)], 0), ([(0, 2), (0, 1), (5, 2), (5, 1)], 0), ([(0, 0), (0, 1), (5, 3), (6, 0)], 0), ([(0, 1), (5, 3), (6, 2), (5, 3)], 0), ([(0, 2), (0, 2), (0, 2), (0, 0)], 0), ([(0, 3), (1, 0), (1, 0), (1, 2)], 0), ([(0, 1), (5, 0), (0, 0), (0, 1)], 0), ([(0, 3), (1, 2), (0, 2), (0, 2)], 0), ([(0, 2), (0, 3), (1, 3), (2, 1)], 0), ([(0, 3), (1, 0), (1, 1), (6, 1)], 0), ([(0, 3), (1, 3), (2, 3), (3, 3)], 0), ([(0, 0), (0, 3), (1, 2), (0, 3)], 0), ([(0, 3), (1, 1), (6, 0), (1, 2)], 0), ([(0, 1), (5, 2), (5, 2), (5, 1)], 0), ([(0, 3), (1, 1), (6, 0), (1, 0)], 0), ([(0, 0), (0, 3), (1, 1), (6, 3)], 0), ([(0, 1), (5, 1), (10, 0), (5, 0)], 0), ([(0, 3), (1, 0), (1, 1), (6, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.46964502 -0.05398135  0.88017509  0.04256046]
True reward weights: [-0.58844088 -0.39506705 -0.37329827  0.59858814]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.161686

Running EBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5840
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.07927268 -0.4002615   0.82048336 -0.40039185]
True reward weights: [ 0.71191738 -0.54100023 -0.01097742 -0.44762918]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.162715

Running EBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5772
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.00436816 -0.33049397  0.58018469 -0.74440606]
True reward weights: [-0.25183322 -0.51699543 -0.61099422  0.54404211]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.162975

Running EBIRL with 4 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5272
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.63667667 -0.36054701  0.14351465 -0.66637243]
True reward weights: [-0.94476366  0.12755932  0.23457786 -0.19006178]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.228879

Running EBIRL with 5 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.70904456 -0.62033319  0.02131995  0.33464608]
True reward weights: [-0.86428146 -0.21779015 -0.0513546   0.45049718]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.234132

Running EBIRL with 6 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5276
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.08791619 -0.71957681  0.16026455  0.66992181]
True reward weights: [ 0.63378245 -0.62171543  0.45987251  0.01752144]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.238514

Running EBIRL with 7 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5400
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.7566026  -0.59278218 -0.04610699  0.27209548]
True reward weights: [-0.0152246  -0.32893886  0.16790551  0.92917984]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.317120

Running EBIRL with 8 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4568
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.21951184 -0.44218334 -0.4243106   0.75911064]
True reward weights: [ 0.09124763 -0.57987302  0.79893281 -0.13087212]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.317656

Running EBIRL with 9 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4604
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.65036936 -0.24783961  0.24703599 -0.67421691]
True reward weights: [-0.75978463 -0.49060539  0.26879016  0.33133898]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.397083

Running EBIRL with 10 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4618
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.311654   -0.46977997  0.51309225 -0.64723636]
True reward weights: [-0.57336841 -0.34387739 -0.64332047 -0.37300908]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.396574

Running EBIRL with 11 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4458
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.18588282 -0.2790893   0.46375834  0.82005179]
True reward weights: [-0.81482061 -0.44194933 -0.28975013  0.23831287]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.395786

Running EBIRL with 12 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4596
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.40144083 -0.46611569  0.77869256 -0.12336665]
True reward weights: [-0.72240587 -0.06780523 -0.35415581 -0.59000497]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.477700

Running EBIRL with 13 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4606
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.11663122 -0.93376461 -0.034962   -0.33653895]
True reward weights: [ 0.07630751 -0.95015835  0.29861579 -0.04695623]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.477033

Running EBIRL with 14 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4732
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.4914416  -0.3647471  -0.64740727 -0.45421199]
True reward weights: [ 0.14966635 -0.94360549 -0.04873873 -0.29126139]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.560550

Running EBIRL with 15 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4608
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.48862277 -0.64253481 -0.31637216 -0.49830258]
True reward weights: [-0.00746918 -0.51522439 -0.05056704 -0.85552967]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.560696

Running EBIRL with 16 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.75128311 -0.39411879 -0.45514894 -0.27033962]
True reward weights: [-0.44059858 -0.5243831   0.54190546 -0.48706645]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.644930

Running EBIRL with 17 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4368
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.3655869  -0.82524157  0.37177062 -0.21704651]
True reward weights: [-0.50140532 -0.82462825  0.07466298  0.25101074]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.769158

Running EBIRL with 18 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4434
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.48915221 -0.46618637 -0.23133413  0.69991779]
True reward weights: [ 0.05316226 -0.53702125 -0.81388679 -0.2153375 ]
MAP Policy for current environment:
Information gain 18 demonstrations: 0.768242

Running EBIRL with 19 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4344
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.33154835 -0.7037253   0.58227555 -0.23622356]
True reward weights: [-0.47982802 -0.52247269  0.53247816 -0.46179473]
MAP Policy for current environment:
Information gain 19 demonstrations: 0.768377

Running EBIRL with 20 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4314
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.27364382 -0.49119928 -0.73157727 -0.38553472]
True reward weights: [ 0.09350391 -0.6429883  -0.28925393  0.70296176]
MAP Policy for current environment:
Information gain 20 demonstrations: 0.771075

Running EBIRL with 21 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4314
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.13411902 -0.61519029 -0.77123146 -0.09356829]
True reward weights: [-0.81419707 -0.47021191 -0.34054815  0.00329385]
MAP Policy for current environment:
Information gain 21 demonstrations: 0.770724

Running EBIRL with 22 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4234
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.7473323  -0.14478691 -0.30709185 -0.57116178]
True reward weights: [-0.6508561  -0.38771408 -0.37826378  0.53195924]
MAP Policy for current environment:
Information gain 22 demonstrations: 0.771080

Running EBIRL with 23 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4384
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.67887026 -0.40319723 -0.05281562  0.6113736 ]
True reward weights: [-0.51852245 -0.56199928 -0.31096202 -0.56444122]
MAP Policy for current environment:
Information gain 23 demonstrations: 0.770933

Running EBIRL with 24 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4352
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.11360353 -0.71777246  0.16522226  0.66678222]
True reward weights: [-0.5345765  -0.82001163  0.12620798 -0.16087399]
MAP Policy for current environment:
Information gain 24 demonstrations: 0.856273

Running EBIRL with 25 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4324
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [ 0.05583118 -0.46404098  0.14349207  0.87232957]
True reward weights: [ 0.10105266 -0.78595199 -0.36412282 -0.48936939]
MAP Policy for current environment:
Information gain 25 demonstrations: 0.940994

Running EBIRL with 26 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4308
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.16240073 -0.29943541 -0.87810485  0.33600047]
True reward weights: [-0.20745644 -0.77723969 -0.19058333 -0.56261735]
MAP Policy for current environment:
Information gain 26 demonstrations: 1.026446

Running EBIRL with 27 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4304
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.0606932  -0.8482191  -0.52341181 -0.05367291]
True reward weights: [ 0.01719199 -0.84243328  0.52262405  0.12990266]
MAP Policy for current environment:
Information gain 27 demonstrations: 1.112208

Running EBIRL with 28 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3870
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.21246771 -0.71392649  0.48111593 -0.46227038]
True reward weights: [-0.02809218 -0.56622448  0.30386437  0.76568082]
MAP Policy for current environment:
Information gain 28 demonstrations: 1.110868

Running EBIRL with 29 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.41804828 -0.37411514 -0.48645452 -0.66980258]
True reward weights: [-0.31087035 -0.83667967  0.24684085  0.37735442]
MAP Policy for current environment:
Information gain 29 demonstrations: 1.195295

Running EBIRL with 30 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3832
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.50946402 -0.28814323  0.11371411 -0.80280072]
True reward weights: [-0.30065168 -0.87202995 -0.31357305 -0.22548677]
MAP Policy for current environment:
Information gain 30 demonstrations: 1.193908

Running EBIRL with 31 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3784
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.240592   -0.48388375 -0.74212231 -0.39651793]
True reward weights: [-0.42143812 -0.8964734  -0.07459939 -0.11471828]
MAP Policy for current environment:
Information gain 31 demonstrations: 1.192799

Running EBIRL with 32 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.39107591 -0.86657449 -0.02858082  0.30869308]
True reward weights: [-0.19772309 -0.04834033  0.02380962  0.97877571]
MAP Policy for current environment:
Information gain 32 demonstrations: 1.191365

Running EBIRL with 33 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3892
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.12435875 -0.38663128 -0.88821337 -0.21477469]
True reward weights: [-0.12016633 -0.53256848 -0.80891632 -0.21814045]
MAP Policy for current environment:
Information gain 33 demonstrations: 1.190140

Running EBIRL with 34 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3820
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.45448073 -0.5945209   0.48614732 -0.45127923]
True reward weights: [-0.90510688 -0.34017466 -0.11440958  0.22797629]
MAP Policy for current environment:
Information gain 34 demonstrations: 1.189375

Running EBIRL with 35 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.41261199 -0.85965095  0.17616138  0.24437423]
True reward weights: [-0.01762529 -0.05338477 -0.97286787  0.2244271 ]
MAP Policy for current environment:
Information gain 35 demonstrations: 1.188756

Running EBIRL with 36 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.34859269 -0.75349011  0.38282718 -0.40519025]
True reward weights: [-0.27307106 -0.94594595 -0.11114458  0.1351493 ]
MAP Policy for current environment:
Information gain 36 demonstrations: 1.273191

Running EBIRL with 37 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.97059566 -0.18198814 -0.15564736  0.0244596 ]
True reward weights: [-0.57781788 -0.05686368 -0.65195502 -0.48769629]
MAP Policy for current environment:
Information gain 37 demonstrations: 1.272755

Running EBIRL with 38 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.27396669 -0.31733606 -0.16137906  0.89341864]
True reward weights: [-0.71830294 -0.56142305  0.18534474 -0.36673202]
MAP Policy for current environment:
Information gain 38 demonstrations: 1.271346

Running EBIRL with 39 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.24357277 -0.42387697 -0.02779498  0.87191058]
True reward weights: [-0.75405108 -0.4648543  -0.27578273  0.37317736]
MAP Policy for current environment:
Information gain 39 demonstrations: 1.271268

Running EBIRL with 40 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.65355219 -0.71177377 -0.21111108 -0.14724046]
True reward weights: [-0.06173311 -0.07004833 -0.9878502  -0.12423459]
MAP Policy for current environment:
Information gain 40 demonstrations: 1.270287

Running experiment 6/19...
Shuffled Demos: [([(0, 3), (1, 1), (6, 1), (11, 3)], 0), ([(0, 2), (0, 1), (5, 3), (6, 1)], 0), ([(0, 0), (0, 1), (5, 1), (10, 0)], 0), ([(0, 2), (0, 3), (1, 2), (0, 2)], 0), ([(0, 1), (5, 1), (10, 3), (11, 1)], 0), ([(0, 3), (1, 1), (6, 2), (5, 2)], 0), ([(0, 0), (0, 3), (1, 2), (0, 0)], 0), ([(0, 3), (1, 1), (6, 3), (7, 1)], 0), ([(0, 1), (5, 1), (10, 3), (11, 0)], 0), ([(0, 3), (1, 1), (6, 0), (1, 3)], 0), ([(0, 1), (5, 3), (6, 0), (1, 3)], 0), ([(0, 2), (0, 2), (0, 3), (1, 1)], 0), ([(0, 3), (1, 2), (0, 3), (1, 0)], 0), ([(0, 1), (5, 2), (5, 1), (10, 3)], 0), ([(0, 1), (5, 0), (0, 3), (1, 0)], 0), ([(0, 2), (0, 3), (1, 2), (0, 2)], 0), ([(0, 2), (0, 2), (0, 1), (5, 1)], 0), ([(0, 3), (1, 0), (1, 2), (0, 1)], 0), ([(0, 3), (1, 1), (6, 2), (5, 1)], 0), ([(0, 1), (5, 2), (5, 3), (6, 0)], 0), ([(0, 2), (0, 2), (0, 3), (1, 3)], 0), ([(0, 0), (0, 2), (0, 2), (0, 3)], 0), ([(0, 1), (5, 2), (5, 2), (5, 1)], 0), ([(0, 2), (0, 3), (1, 3), (2, 2)], 0), ([(0, 0), (0, 0), (0, 2), (0, 1)], 0), ([(0, 0), (0, 0), (0, 2), (0, 3)], 0), ([(0, 3), (1, 2), (0, 3), (1, 1)], 0), ([(0, 2), (0, 2), (0, 0), (0, 2)], 0), ([(0, 0), (0, 0), (0, 0), (0, 3)], 0), ([(0, 0), (0, 3), (1, 3), (2, 1)], 0), ([(0, 3), (1, 3), (2, 0), (2, 3)], 0), ([(0, 3), (1, 3), (2, 3), (3, 1)], 0), ([(0, 1), (5, 2), (5, 0), (0, 2)], 0), ([(0, 0), (0, 0), (0, 3), (1, 3)], 0), ([(0, 0), (0, 0), (0, 3), (1, 3)], 0), ([(0, 1), (5, 2), (5, 1), (10, 2)], 0), ([(0, 1), (5, 0), (0, 3), (1, 3)], 0), ([(0, 3), (1, 2), (0, 0), (0, 3)], 0), ([(0, 3), (1, 3), (2, 3), (3, 2)], 0), ([(0, 0), (0, 3), (1, 1), (6, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.53929852 -0.23239966 -0.8008336  -0.11752891]
True reward weights: [-0.62480116 -0.58775562  0.29021502  0.42419581]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.129833

Running EBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4550
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.19369263  0.54932206 -0.55225866 -0.59643843]
True reward weights: [-0.17367336  0.06144746 -0.66140262  0.72705457]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126750

Running EBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4606
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.79768415 -0.43912068 -0.40849158 -0.06330596]
True reward weights: [-0.29782282 -0.94522855 -0.10928332  0.07682254]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.136529

Running EBIRL with 4 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4214
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.80262199 -0.4890409  -0.34151036 -0.00275903]
True reward weights: [-0.24009818  0.81496194 -0.52725792 -0.01374741]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.175754

Running EBIRL with 5 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4058
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.14670805 -0.67887271 -0.67340256 -0.253254  ]
True reward weights: [-0.59638241 -0.21282725 -0.72511502 -0.27063034]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.179438

Running EBIRL with 6 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4100
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.03914736 -0.30928177 -0.74487701 -0.58989026]
True reward weights: [-0.57434388 -0.77365108  0.10556962 -0.24586208]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.182800

Running EBIRL with 7 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3940
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.67062598  0.09542838 -0.4191144  -0.60456376]
True reward weights: [-0.55159079 -0.66787173  0.05241727 -0.49693801]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.185133

Running EBIRL with 8 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.21993029 -0.00674621 -0.36071157  0.9063511 ]
True reward weights: [-0.07063421 -0.05464022 -0.8543845   0.51191052]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.189342

Running EBIRL with 9 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3830
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-5.66444469e-01 -3.73488504e-01 -5.41939784e-04 -7.34606499e-01]
True reward weights: [-0.44853835  0.38191185 -0.79785972  0.12797091]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.188172

Running EBIRL with 10 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.51421218 -0.20203643 -0.7698885  -0.31943515]
True reward weights: [-0.37868711  0.36907231 -0.84685008 -0.05680357]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.188524

Running EBIRL with 11 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.39531871 -0.43469335 -0.39998355  0.70340455]
True reward weights: [-0.66652737 -0.4981649  -0.27773077 -0.48004023]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.194987

Running EBIRL with 12 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3928
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.74659147 -0.45825805 -0.39703982  0.27378846]
True reward weights: [-0.25109336 -0.78757296 -0.0188625  -0.56242791]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.189059

Running EBIRL with 13 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3778
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.4222936  -0.76254851 -0.48215122 -0.08785262]
True reward weights: [ 0.01215279 -0.67458912 -0.13359388  0.72590254]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.187414

Running EBIRL with 14 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.1620582  -0.77062552 -0.61631504 -0.00540623]
True reward weights: [-0.37552963 -0.66078662  0.04524846 -0.64829863]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.192612

Running EBIRL with 15 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3798
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.64420244 -0.02693781 -0.56606755  0.51365854]
True reward weights: [-0.19233664 -0.84665571 -0.22813654 -0.4406069 ]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.185510

Running EBIRL with 16 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3734
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.43184808 -0.10152496 -0.62417889  0.6431179 ]
True reward weights: [-0.8117017  -0.29300857 -0.43788628 -0.25207523]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.190327

Running EBIRL with 17 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3716
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.39831123 -0.66698765 -0.35955768 -0.51690802]
True reward weights: [-0.00610549 -0.91677926 -0.28668923 -0.27800683]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.248278

Running EBIRL with 18 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3670
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.28769905 -0.31630777 -0.49195391  0.75839304]
True reward weights: [-0.8293468  -0.42656144  0.00197439 -0.36086747]
MAP Policy for current environment:
Information gain 18 demonstrations: 0.250580

Running EBIRL with 19 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3676
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.51867847 -0.55352941 -0.42045586 -0.49778982]
True reward weights: [-0.60188536 -0.24332621 -0.6032963  -0.46320616]
MAP Policy for current environment:
Information gain 19 demonstrations: 0.251427

Running EBIRL with 20 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3638
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.60933596 -0.56691392 -0.48215517 -0.27357759]
True reward weights: [-0.40016918 -0.03427099 -0.91511747  0.0353574 ]
MAP Policy for current environment:
Information gain 20 demonstrations: 0.252838

Running EBIRL with 21 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.96799041 -0.24398748 -0.04386807 -0.03924625]
True reward weights: [-0.17379462 -0.6139692  -0.39634065  0.66011464]
MAP Policy for current environment:
Information gain 21 demonstrations: 0.255504

Running EBIRL with 22 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3670
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.20913317 -0.23191174 -0.06846037  0.94751963]
True reward weights: [-0.7573056  -0.63104611 -0.1659693  -0.02689279]
MAP Policy for current environment:
Information gain 22 demonstrations: 0.335015

Running EBIRL with 23 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3742
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.32076575 -0.37279861 -0.84042408 -0.22763545]
True reward weights: [-0.6411531  -0.58541153 -0.26804216  0.41757567]
MAP Policy for current environment:
Information gain 23 demonstrations: 0.418252

Running EBIRL with 24 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3742
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.55721808 -0.34909536 -0.23382823  0.7162156 ]
True reward weights: [-0.32526926 -0.33746373 -0.40045    -0.78737408]
MAP Policy for current environment:
Information gain 24 demonstrations: 0.419280

Running EBIRL with 25 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3676
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.14057027 -0.33759963 -0.26581486 -0.89196914]
True reward weights: [-0.54538691 -0.71952282 -0.39282116  0.17473286]
MAP Policy for current environment:
Information gain 25 demonstrations: 0.503931

Running EBIRL with 26 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3662
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.69400243 -0.64566375  0.09911044 -0.30274752]
True reward weights: [-0.54946836 -0.6104705  -0.12459854 -0.5566736 ]
MAP Policy for current environment:
Information gain 26 demonstrations: 0.589510

Running EBIRL with 27 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3716
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.38438818 -0.68314569 -0.28287396 -0.55275674]
True reward weights: [-0.38619468 -0.23949508 -0.80137317  0.38896892]
MAP Policy for current environment:
Information gain 27 demonstrations: 0.590464

Running EBIRL with 28 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3666
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.12726594 -0.38455476 -0.61222773  0.67904214]
True reward weights: [-0.62867688 -0.34960065 -0.06310607  0.69178204]
MAP Policy for current environment:
Information gain 28 demonstrations: 0.676177

Running EBIRL with 29 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3580
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.56535516 -0.64496408 -0.50951863 -0.06917836]
True reward weights: [-0.53321865 -0.72715644 -0.13652596 -0.41022195]
MAP Policy for current environment:
Information gain 29 demonstrations: 0.762399

Running EBIRL with 30 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3754
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.66565626 -0.68369885  0.04311342 -0.29597105]
True reward weights: [-0.88645138 -0.28239424 -0.36168412  0.0603493 ]
MAP Policy for current environment:
Information gain 30 demonstrations: 0.762913

Running EBIRL with 31 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3544
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.79070055 -0.12077346 -0.60001392 -0.01377359]
True reward weights: [-0.20656002 -0.21278635 -0.4414592  -0.84685814]
MAP Policy for current environment:
Information gain 31 demonstrations: 0.762940

Running EBIRL with 32 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3556
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.3550561  -0.42797286 -0.51808616  0.64989316]
True reward weights: [-0.45493657 -0.23922664 -0.85433353 -0.07692564]
MAP Policy for current environment:
Information gain 32 demonstrations: 0.762969

Running EBIRL with 33 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3598
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.33067548 -0.25954649 -0.40676164 -0.81106986]
True reward weights: [-0.35573221 -0.4921283  -0.79107249  0.07395028]
MAP Policy for current environment:
Information gain 33 demonstrations: 0.848738

Running EBIRL with 34 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3578
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.47837499 -0.19350818 -0.81147301 -0.27426905]
True reward weights: [-0.37264534 -0.133833   -0.6132732   0.68346189]
MAP Policy for current environment:
Information gain 34 demonstrations: 0.849106

Running EBIRL with 35 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3564
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.21217248 -0.83235288 -0.17615552  0.48077099]
True reward weights: [-0.04667937 -0.78814122 -0.12978136 -0.59984269]
MAP Policy for current environment:
Information gain 35 demonstrations: 0.849740

Running EBIRL with 36 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3586
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.65652529 -0.53645765  0.00751024 -0.53021819]
True reward weights: [-0.3692393  -0.74319848 -0.08070926  0.55209091]
MAP Policy for current environment:
Information gain 36 demonstrations: 0.850360

Running EBIRL with 37 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3530
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.17892307 -0.83904887 -0.37059729  0.35586681]
True reward weights: [-0.28286521 -0.53510297 -0.18384576  0.77450166]
MAP Policy for current environment:
Information gain 37 demonstrations: 0.850372

Running EBIRL with 38 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3594
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.68324838 -0.68578124 -0.24945754  0.02542993]
True reward weights: [-0.0095864  -0.17857921 -0.84977429  0.49588428]
MAP Policy for current environment:
Information gain 38 demonstrations: 0.850812

Running EBIRL with 39 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3542
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.17982851 -0.58166367 -0.60298103 -0.51550262]
True reward weights: [-0.03126124 -0.27092156 -0.48408407  0.83143662]
MAP Policy for current environment:
Information gain 39 demonstrations: 0.851127

Running EBIRL with 40 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3512
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.39051051 -0.62163023 -0.34705825  0.58363342]
True reward weights: [-0.04343486 -0.45238595 -0.66070994  0.59743011]
MAP Policy for current environment:
Information gain 40 demonstrations: 0.851375

Saving results to files...
Results saved successfully.

Running experiment 7/19...
Shuffled Demos: [([(0, 0), (0, 3), (1, 1), (6, 1)], 0), ([(0, 0), (0, 1), (5, 2), (5, 2)], 0), ([(0, 0), (0, 0), (0, 3), (1, 0)], 0), ([(0, 0), (0, 0), (0, 1), (5, 0)], 0), ([(0, 2), (0, 3), (1, 3), (2, 1)], 0), ([(0, 0), (0, 1), (5, 2), (5, 0)], 0), ([(0, 2), (0, 0), (0, 2), (0, 0)], 0), ([(0, 1), (5, 3), (6, 3), (7, 0)], 0), ([(0, 2), (0, 3), (1, 3), (2, 3)], 0), ([(0, 0), (0, 2), (0, 1), (5, 0)], 0), ([(0, 3), (1, 3), (2, 2), (1, 1)], 0), ([(0, 3), (1, 2), (0, 2), (0, 3)], 0), ([(0, 2), (0, 0), (0, 3), (1, 1)], 0), ([(0, 2), (0, 0), (0, 0), (0, 0)], 0), ([(0, 0), (0, 2), (0, 1), (5, 3)], 0), ([(0, 2), (0, 2), (0, 3), (1, 3)], 0), ([(0, 1), (5, 2), (5, 1), (10, 0)], 0), ([(0, 0), (0, 1), (5, 0), (0, 1)], 0), ([(0, 1), (5, 3), (6, 0), (1, 2)], 0), ([(0, 2), (0, 1), (5, 3), (6, 2)], 0), ([(0, 0), (0, 0), (0, 0), (0, 2)], 0), ([(0, 1), (5, 0), (0, 3), (1, 0)], 0), ([(0, 1), (5, 3), (6, 3), (7, 1)], 0), ([(0, 0), (0, 2), (0, 3), (1, 3)], 0), ([(0, 3), (1, 1), (6, 2), (5, 2)], 0), ([(0, 1), (5, 3), (6, 2), (5, 2)], 0), ([(0, 0), (0, 1), (5, 0), (0, 2)], 0), ([(0, 2), (0, 2), (0, 0), (0, 3)], 0), ([(0, 0), (0, 0), (0, 0), (0, 1)], 0), ([(0, 0), (0, 0), (0, 1), (5, 2)], 0), ([(0, 2), (0, 1), (5, 3), (6, 3)], 0), ([(0, 1), (5, 2), (5, 2), (5, 3)], 0), ([(0, 3), (1, 2), (0, 2), (0, 3)], 0), ([(0, 2), (0, 1), (5, 1), (10, 2)], 0), ([(0, 3), (1, 3), (2, 2), (1, 1)], 0), ([(0, 1), (5, 1), (10, 3), (11, 2)], 0), ([(0, 2), (0, 1), (5, 2), (5, 1)], 0), ([(0, 3), (1, 1), (6, 1), (11, 2)], 0), ([(0, 2), (0, 0), (0, 0), (0, 2)], 0), ([(0, 0), (0, 0), (0, 2), (0, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.5968
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.31055828 -0.80383223 -0.43944807 -0.25356005]
True reward weights: [-0.86648592 -0.43712062 -0.12519764  0.20604187]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.150734

Running EBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4670
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.91505141  0.37944187  0.00418819 -0.13670126]
True reward weights: [ 0.22120027 -0.32683245 -0.42668738  0.81374988]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.225002

Running EBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4780
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.46159799 -0.32164785  0.03502874 -0.82597999]
True reward weights: [-0.68969137  0.3192919   0.09764369 -0.64252953]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.264351

Running EBIRL with 4 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4780
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.34005929  0.14545945 -0.58894109 -0.71857471]
True reward weights: [-0.70202522  0.58701547 -0.40084972 -0.0435079 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.351337

Running EBIRL with 5 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4612
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.56630333  0.18422192 -0.78535177  0.16907223]
True reward weights: [-0.4992178   0.63346221 -0.35880516 -0.46985751]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.356647

Running EBIRL with 6 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4690
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.4688594  -0.65629132 -0.58577626 -0.07949047]
True reward weights: [-0.21834229  0.25682562 -0.7790459   0.52863479]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.443521

Running EBIRL with 7 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4714
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.32505917 -0.50980627 -0.6337237   0.48252293]
True reward weights: [-0.55752642  0.67216028 -0.38981452 -0.29224901]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.528524

Running EBIRL with 8 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4544
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.17947102 -0.60024266 -0.17080321  0.76047693]
True reward weights: [-0.23605783 -0.15242219 -0.47344694 -0.83480068]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.585923

Running EBIRL with 9 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4548
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.83873366 -0.33524275  0.23064515  0.36185765]
True reward weights: [-0.47575467 -0.49856533 -0.38363958  0.61474448]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.587369

Running EBIRL with 10 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.48416313  0.53027788 -0.55163419 -0.42437148]
True reward weights: [-0.74195141  0.0816869  -0.49525537 -0.44447437]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.672227

Running EBIRL with 11 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.30759207 -0.38144651 -0.21707604 -0.84425332]
True reward weights: [-0.51897543  0.39906535  0.05177489 -0.75414236]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.669469

Running EBIRL with 12 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3908
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.54655418  0.06317294 -0.74032053  0.38628127]
True reward weights: [-0.11420448  0.19487975 -0.51553395 -0.82656153]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.669304

Running EBIRL with 13 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3982
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.14042598 -0.63490562 -0.67232703  0.35376795]
True reward weights: [-0.18605864  0.18025897 -0.40691508 -0.87596176]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.667072

Running EBIRL with 14 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.7071344  -0.48107516 -0.50053217  0.13414614]
True reward weights: [-0.63382722  0.08253685 -0.23983917  0.7307037 ]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.749962

Running EBIRL with 15 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4016
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.58923869 -0.55093491  0.31173606  0.50208477]
True reward weights: [-0.74037936 -0.63886437  0.13551281 -0.15914458]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.834757

Running EBIRL with 16 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.78076462 -0.0497697  -0.56273143 -0.26695117]
True reward weights: [-0.50721119 -0.59928668  0.24367348  0.56939926]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.834155

Running EBIRL with 17 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3946
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.82212949 -0.34660567 -0.01704465  0.45130598]
True reward weights: [-0.17515498  0.23668829 -0.79406044  0.53175878]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.917704

Running EBIRL with 18 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.70274953  0.01928944 -0.41524573 -0.57735778]
True reward weights: [-0.67057519 -0.39820068  0.62271421 -0.06318352]
MAP Policy for current environment:
Information gain 18 demonstrations: 1.001670

Running EBIRL with 19 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.18457841 -0.9587856   0.02616795  0.21442069]
True reward weights: [-0.18837577 -0.20468503  0.00298222 -0.96052575]
MAP Policy for current environment:
Information gain 19 demonstrations: 1.000258

Running EBIRL with 20 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3966
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.28559956 -0.62156229 -0.11178675  0.72083072]
True reward weights: [-0.93385385 -0.18818696 -0.01658755 -0.30368983]
MAP Policy for current environment:
Information gain 20 demonstrations: 1.001215

Running EBIRL with 21 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3932
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.32765291 -0.71954771 -0.61064584  0.04479202]
True reward weights: [-0.56325952 -0.33334922  0.37893954  0.65423378]
MAP Policy for current environment:
Information gain 21 demonstrations: 1.085508

Running EBIRL with 22 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.37319954 -0.41567766 -0.78090132  0.27951264]
True reward weights: [-0.60453532  0.14015229 -0.70011721  0.35317173]
MAP Policy for current environment:
Information gain 22 demonstrations: 1.084225

Running EBIRL with 23 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.13019686 -0.51680188 -0.39667005  0.74740716]
True reward weights: [-0.40193338 -0.26490613  0.05682748  0.87466847]
MAP Policy for current environment:
Information gain 23 demonstrations: 1.084028

Running EBIRL with 24 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4020
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.57584148 -0.78716541 -0.1227036   0.18363287]
True reward weights: [-0.70889644 -0.23457085  0.32228308  0.58187281]
MAP Policy for current environment:
Information gain 24 demonstrations: 1.084138

Running EBIRL with 25 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4010
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.78261996 -0.55789688  0.10236596  0.25647276]
True reward weights: [-0.39735633 -0.39009192  0.65503049 -0.51075562]
MAP Policy for current environment:
Information gain 25 demonstrations: 1.084343

Running EBIRL with 26 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.74003754 -0.55423917 -0.22360856  0.30848434]
True reward weights: [-0.08379677 -0.47977225 -0.81617071  0.31090522]
MAP Policy for current environment:
Information gain 26 demonstrations: 1.084451

Running EBIRL with 27 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.4030
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.75568163 -0.52603959  0.1596464   0.35600652]
True reward weights: [-0.09525089  0.09268678 -0.86347565 -0.48656576]
MAP Policy for current environment:
Information gain 27 demonstrations: 1.169312

Running EBIRL with 28 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3956
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.607997   -0.62318905 -0.48734743 -0.0668397 ]
True reward weights: [-0.4441049   0.08483667 -0.87507706 -0.17266651]
MAP Policy for current environment:
Information gain 28 demonstrations: 1.254885

Running EBIRL with 29 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.92950439 -0.33234309  0.13352703  0.08797842]
True reward weights: [-0.02902116 -0.84238278 -0.53717327 -0.03152637]
MAP Policy for current environment:
Information gain 29 demonstrations: 1.340205

Running EBIRL with 30 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.87754517 -0.45014449  0.16504686 -0.00662843]
True reward weights: [-0.54058185 -0.76558218 -0.07816706 -0.33991925]
MAP Policy for current environment:
Information gain 30 demonstrations: 1.425686

Running EBIRL with 31 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.62472146 -0.33096884 -0.49379677 -0.50630768]
True reward weights: [-0.03995086 -0.82455037 -0.20879506  0.52433313]
MAP Policy for current environment:
Information gain 31 demonstrations: 1.425976

Running EBIRL with 32 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.47294249  0.1844991  -0.8613063  -0.02090294]
True reward weights: [-0.61668258 -0.06478759 -0.71701008 -0.31843636]
MAP Policy for current environment:
Information gain 32 demonstrations: 1.511830

Running EBIRL with 33 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.41995431 -0.55602031  0.12711411 -0.70591912]
True reward weights: [-0.52879684 -0.46150194 -0.36080906 -0.61417154]
MAP Policy for current environment:
Information gain 33 demonstrations: 1.511922

Running EBIRL with 34 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.29172769 -0.12238368 -0.81743296  0.48137361]
True reward weights: [-0.22632383 -0.50923579 -0.82920068 -0.04338963]
MAP Policy for current environment:
Information gain 34 demonstrations: 1.597739

Running EBIRL with 35 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3978
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.53591905 -0.20350339 -0.78397088 -0.23825784]
True reward weights: [-0.11482263 -0.94808014 -0.19645566 -0.22218235]
MAP Policy for current environment:
Information gain 35 demonstrations: 1.597374

Running EBIRL with 36 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.55196394 -0.07577562 -0.24075593  0.79475181]
True reward weights: [-0.63060211 -0.23664836 -0.66068955 -0.33140285]
MAP Policy for current environment:
Information gain 36 demonstrations: 1.597673

Running EBIRL with 37 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3912
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.33997988 -0.89085781 -0.16871244 -0.24964405]
True reward weights: [-0.3055921  -0.46443241  0.51635568 -0.65137763]
MAP Policy for current environment:
Information gain 37 demonstrations: 1.682918

Running EBIRL with 38 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3922
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.83427783 -0.39886298 -0.35159955  0.14583067]
True reward weights: [-0.45514157 -0.1379173  -0.66505313 -0.57578581]
MAP Policy for current environment:
Information gain 38 demonstrations: 1.682618

Running EBIRL with 39 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3936
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.56422561  0.04568708 -0.80084139  0.19548712]
True reward weights: [-0.7516747  -0.2931779  -0.36630634  0.4635208 ]
MAP Policy for current environment:
Information gain 39 demonstrations: 1.768758

Running EBIRL with 40 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3916
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.5738317  -0.51086619 -0.61869837 -0.16415005]
True reward weights: [-0.170595    0.00768898 -0.60107513 -0.78073485]
MAP Policy for current environment:
Information gain 40 demonstrations: 1.853799

Running experiment 8/19...
Shuffled Demos: [([(0, 0), (0, 2), (0, 3), (1, 2)], 3), ([(0, 2), (0, 0), (0, 0), (0, 1)], 3), ([(0, 1), (5, 0), (0, 2), (0, 3)], 3), ([(0, 2), (0, 2), (0, 2), (0, 2)], 3), ([(0, 1), (5, 2), (5, 2), (5, 0)], 0), ([(0, 3), (1, 3), (2, 2), (1, 3)], 1), ([(0, 1), (5, 3), (6, 0), (1, 3)], 0), ([(0, 3), (1, 2), (0, 1), (5, 2)], 2), ([(0, 2), (0, 0), (0, 2), (0, 3)], 3), ([(0, 3), (1, 2), (0, 2), (0, 3)], 3), ([(0, 1), (5, 3), (6, 0), (1, 0)], 0), ([(0, 3), (1, 1), (6, 0), (1, 1)], 3), ([(0, 1), (5, 2), (5, 0), (0, 3)], 0), ([(0, 0), (0, 3), (1, 3), (2, 0)], 2), ([(0, 0), (0, 0), (0, 2), (0, 1)], 3), ([(0, 1), (5, 1), (10, 0), (5, 1)], 0), ([(0, 3), (1, 1), (6, 1), (11, 3)], 1), ([(0, 1), (5, 3), (6, 0), (1, 2)], 0), ([(0, 1), (5, 2), (5, 3), (6, 1)], 0), ([(0, 3), (1, 0), (1, 3), (2, 3)], 2), ([(0, 2), (0, 2), (0, 0), (0, 0)], 3), ([(0, 3), (1, 3), (2, 1), (7, 0)], 0), ([(0, 1), (5, 1), (10, 2), (10, 0)], 0), ([(0, 3), (1, 3), (2, 2), (1, 3)], 1), ([(0, 0), (0, 3), (1, 0), (1, 0)], 3), ([(0, 2), (0, 3), (1, 1), (6, 3)], 2), ([(0, 1), (5, 2), (5, 3), (6, 2)], 0), ([(0, 1), (5, 0), (0, 3), (1, 2)], 3), ([(0, 3), (1, 1), (6, 1), (11, 3)], 1), ([(0, 0), (0, 3), (1, 1), (6, 0)], 2), ([(0, 1), (5, 2), (5, 1), (10, 1)], 0), ([(0, 3), (1, 0), (1, 1), (6, 1)], 2), ([(0, 1), (5, 3), (6, 2), (5, 0)], 0), ([(0, 0), (0, 3), (1, 1), (6, 3)], 2), ([(0, 2), (0, 2), (0, 3), (1, 3)], 3), ([(0, 0), (0, 1), (5, 1), (10, 0)], 0), ([(0, 1), (5, 0), (0, 2), (0, 0)], 3), ([(0, 2), (0, 3), (1, 2), (0, 0)], 3), ([(0, 0), (0, 3), (1, 0), (1, 3)], 3), ([(0, 1), (5, 3), (6, 3), (7, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.1040002   0.70805022  0.63882754 -0.28239725]
True reward weights: [-0.66202557 -0.27713495  0.4113014   0.56191594]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.19063253  0.73998063  0.63971768 -0.08275985]
True reward weights: [-0.91182953 -0.27594924  0.25178681 -0.17035941]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.4547741  -0.59133582  0.63899911  0.18757025]
True reward weights: [-0.61631331 -0.2675508   0.54057667 -0.5063115 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 8
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.39380399 0.63901595 0.63938866 0.16661086]
True reward weights: [ 0.8512878  -0.07126159  0.42536699 -0.29882067]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.350599

Running EBIRL with 5 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.5882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.23213517  0.00623918  0.63961168  0.73278321]
True reward weights: [-0.1529643  -0.896074   -0.39187425 -0.14173171]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.422030

Running EBIRL with 6 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.45099367 -0.60560127  0.05511552  0.6533101 ]
True reward weights: [-0.00582984 -0.55967932  0.60335602 -0.56805509]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.418649

Running EBIRL with 7 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4644
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.89422418 -0.41561917  0.15861864  0.04963819]
True reward weights: [-0.22842266 -0.32694463 -0.89941852  0.17882007]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.440916

Running EBIRL with 8 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4502
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.50453679 -0.06304342  0.06595932  0.85855549]
True reward weights: [-0.45327824 -0.6109353   0.38014751  0.52610338]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.439938

Running EBIRL with 9 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4516
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.51815958 -0.80501666  0.15943968 -0.2409104 ]
True reward weights: [-0.19150461 -0.41165605 -0.63284972  0.62718938]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.522260

Running EBIRL with 10 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4564
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.89930807  0.06591991  0.31958242 -0.29114709]
True reward weights: [-0.1738322  -0.27246011  0.03997375  0.94548927]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.606600

Running EBIRL with 11 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4638
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.65779205 -0.73622598  0.15871839 -0.00945528]
True reward weights: [-0.74284818 -0.05190817  0.04492816  0.66593061]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.606005

Running EBIRL with 12 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4628
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.66833677 -0.13985709  0.15991677  0.71287628]
True reward weights: [-0.53446433 -0.52030653 -0.66385371  0.05410395]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.689997

Running EBIRL with 13 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4424
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.88557057  0.11274112  0.319176    0.31808942]
True reward weights: [-0.19312084 -0.69248806 -0.29417932  0.62978025]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.689260

Running EBIRL with 14 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4540
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.68911781 -0.27305857  0.31953865 -0.59029714]
True reward weights: [-0.25806212  0.32780657 -0.73840751 -0.52981237]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.689569

Running EBIRL with 15 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4560
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.82625814  0.06079718  0.07202912  0.55534943]
True reward weights: [-0.93659912  0.05136328 -0.31159072  0.15183912]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.774814

Running EBIRL with 16 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4444
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.53525569 -0.02041815  0.07197747 -0.84137013]
True reward weights: [-0.7748806   0.54494159  0.04818619  0.31666512]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.773735

Running EBIRL with 17 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3766
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.45836798 -0.52095325  0.07087605  0.71657734]
True reward weights: [-0.55978228 -0.64778919 -0.07598201  0.51111614]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.806114

Running EBIRL with 18 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3760
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.47313904 -0.23052108  0.0793219  -0.8465858 ]
True reward weights: [-0.50676351 -0.1323304  -0.69798953  0.48835441]
MAP Policy for current environment:
Information gain 18 demonstrations: 0.802753

Running EBIRL with 19 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3818
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.41728274 -0.42943521  0.05405832 -0.79908586]
True reward weights: [-0.51573993 -0.55739743 -0.62177927 -0.19160107]
MAP Policy for current environment:
Information gain 19 demonstrations: 0.800190

Running EBIRL with 20 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3738
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.46532623 -0.44615117  0.05113733  0.76276183]
True reward weights: [-0.07544752 -0.80457516 -0.37046677  0.45795289]
MAP Policy for current environment:
Information gain 20 demonstrations: 0.797575

Running EBIRL with 21 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3838
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.46242196 -0.25591658  0.04841101  0.84754292]
True reward weights: [-0.10039229 -0.32853607 -0.88156926 -0.32376084]
MAP Policy for current environment:
Information gain 21 demonstrations: 0.880871

Running EBIRL with 22 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3632
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.79543123 -0.50144308  0.31910557 -0.11838765]
True reward weights: [-0.6282741  -0.25301101  0.19237357  0.71010527]
MAP Policy for current environment:
Information gain 22 demonstrations: 0.878778

Running EBIRL with 23 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3742
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.70506031 -0.65547886  0.15921913 -0.21883028]
True reward weights: [-0.44851391 -0.44540053 -0.75688829  0.16605343]
MAP Policy for current environment:
Information gain 23 demonstrations: 0.876963

Running EBIRL with 24 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3836
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.76982522 -0.26588818  0.15997617 -0.55774567]
True reward weights: [-0.58839847 -0.52537549  0.60280633  0.11996821]
MAP Policy for current environment:
Information gain 24 demonstrations: 0.875444

Running EBIRL with 25 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3682
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.50302703 -0.75109143  0.06376395  0.42279975]
True reward weights: [-0.12211459 -0.89237197 -0.21245204 -0.37897813]
MAP Policy for current environment:
Information gain 25 demonstrations: 0.959602

Running EBIRL with 26 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3748
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.42626393 -0.36181815  0.07510472  0.8256791 ]
True reward weights: [-0.81750939 -0.11734014 -0.53745739  0.17043839]
MAP Policy for current environment:
Information gain 26 demonstrations: 0.959347

Running EBIRL with 27 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3674
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.64067081 -0.6885985   0.31834668  0.11844159]
True reward weights: [-0.619387   -0.44670899 -0.23047666 -0.60306826]
MAP Policy for current environment:
Information gain 27 demonstrations: 0.958209

Running EBIRL with 28 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3686
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.74950967 -0.36465161  0.31937122 -0.45085085]
True reward weights: [-0.95593744 -0.12818621  0.26350362 -0.01782534]
MAP Policy for current environment:
Information gain 28 demonstrations: 1.042740

Running EBIRL with 29 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.77558759 -0.3691243   0.1585761  -0.48689296]
True reward weights: [-0.36702591 -0.7090683   0.12010806 -0.58998998]
MAP Policy for current environment:
Information gain 29 demonstrations: 1.042262

Running EBIRL with 30 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3706
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.55527213 -0.50041634  0.07615643  0.65989132]
True reward weights: [-0.94895784 -0.16225828  0.0654887   0.26241666]
MAP Policy for current environment:
Information gain 30 demonstrations: 1.041376

Running EBIRL with 31 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3694
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.4918912  -0.83989512  0.15911387 -0.1652332 ]
True reward weights: [-0.10968595 -0.88051309 -0.44127942  0.13393344]
MAP Policy for current environment:
Information gain 31 demonstrations: 1.040733

Running EBIRL with 32 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3680
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.86905012 -0.46819876  0.15842002  0.02109289]
True reward weights: [-0.39110504 -0.54336019 -0.15611542 -0.72624   ]
MAP Policy for current environment:
Information gain 32 demonstrations: 1.039999

Running EBIRL with 33 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3740
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.85628448 -0.37076892  0.15939561  0.32233575]
True reward weights: [-0.7659964  -0.28313351 -0.4669479  -0.33918225]
MAP Policy for current environment:
Information gain 33 demonstrations: 1.039159

Running EBIRL with 34 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3736
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.42707866 -0.76047526  0.04829495 -0.48677386]
True reward weights: [-0.25948321 -0.67171981 -0.68547635 -0.10762503]
MAP Policy for current environment:
Information gain 34 demonstrations: 1.038994

Running EBIRL with 35 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3714
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.66925902 -0.38869689  0.15892927  0.61298334]
True reward weights: [-0.06374687 -0.83365133 -0.52275643  0.16639565]
MAP Policy for current environment:
Information gain 35 demonstrations: 1.123599

Running EBIRL with 36 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3744
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.57705416 -0.53601331  0.15889414  0.59535777]
True reward weights: [-0.4387943  -0.83265444  0.01563619  0.33749319]
MAP Policy for current environment:
Information gain 36 demonstrations: 1.123023

Running EBIRL with 37 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3710
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.8437647  -0.4939721   0.15958223  0.13633127]
True reward weights: [-0.89847929 -0.05046679  0.43516169  0.02867695]
MAP Policy for current environment:
Information gain 37 demonstrations: 1.208259

Running EBIRL with 38 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3652
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.67290966 -0.61913885  0.15891266 -0.37229887]
True reward weights: [-0.87785923 -0.2146855  -0.40903185 -0.12635761]
MAP Policy for current environment:
Information gain 38 demonstrations: 1.293446

Running EBIRL with 39 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.45186646 -0.42834971  0.05981138  0.78022806]
True reward weights: [-0.84014603 -0.18950874  0.46821392  0.19752673]
MAP Policy for current environment:
Information gain 39 demonstrations: 1.378676

Running EBIRL with 40 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3716
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.67852493 -0.58423987  0.31835777 -0.31131337]
True reward weights: [-0.92902434 -0.21073226 -0.30383586 -0.01376456]
MAP Policy for current environment:
Information gain 40 demonstrations: 1.378036

Saving results to files...
Results saved successfully.

Running experiment 9/19...
Shuffled Demos: [([(0, 0), (0, 2), (0, 3), (1, 3)], 0), ([(0, 3), (1, 2), (0, 3), (1, 1)], 0), ([(0, 1), (5, 0), (0, 0), (0, 3)], 0), ([(0, 0), (0, 1), (5, 2), (5, 2)], 0), ([(0, 2), (0, 0), (0, 0), (0, 2)], 0), ([(0, 0), (0, 2), (0, 0), (0, 0)], 0), ([(0, 3), (1, 3), (2, 1), (7, 0)], 0), ([(0, 2), (0, 1), (5, 2), (5, 1)], 0), ([(0, 3), (1, 3), (2, 1), (7, 0)], 0), ([(0, 1), (5, 1), (10, 0), (5, 1)], 0), ([(0, 2), (0, 2), (0, 0), (0, 1)], 0), ([(0, 1), (5, 2), (5, 1), (10, 2)], 0), ([(0, 2), (0, 3), (1, 3), (2, 1)], 0), ([(0, 2), (0, 1), (5, 0), (0, 0)], 0), ([(0, 3), (1, 3), (2, 2), (1, 1)], 0), ([(0, 3), (1, 3), (2, 2), (1, 2)], 0), ([(0, 0), (0, 3), (1, 0), (1, 2)], 0), ([(0, 3), (1, 1), (6, 2), (5, 2)], 0), ([(0, 2), (0, 0), (0, 3), (1, 0)], 0), ([(0, 1), (5, 3), (6, 2), (5, 0)], 0), ([(0, 0), (0, 1), (5, 0), (0, 0)], 0), ([(0, 3), (1, 2), (0, 1), (5, 1)], 0), ([(0, 3), (1, 2), (0, 2), (0, 1)], 0), ([(0, 2), (0, 3), (1, 0), (1, 0)], 0), ([(0, 2), (0, 0), (0, 2), (0, 1)], 0), ([(0, 2), (0, 1), (5, 1), (10, 2)], 0), ([(0, 1), (5, 3), (6, 3), (7, 1)], 3), ([(0, 2), (0, 2), (0, 1), (5, 3)], 0), ([(0, 2), (0, 3), (1, 3), (2, 3)], 0), ([(0, 1), (5, 2), (5, 3), (6, 0)], 0), ([(0, 3), (1, 0), (1, 3), (2, 2)], 0), ([(0, 2), (0, 3), (1, 3), (2, 3)], 0), ([(0, 0), (0, 3), (1, 1), (6, 1)], 0), ([(0, 1), (5, 1), (10, 0), (5, 0)], 0), ([(0, 0), (0, 2), (0, 3), (1, 0)], 0), ([(0, 2), (0, 0), (0, 1), (5, 0)], 0), ([(0, 3), (1, 3), (2, 0), (2, 1)], 0), ([(0, 0), (0, 3), (1, 0), (1, 0)], 0), ([(0, 0), (0, 2), (0, 3), (1, 1)], 0), ([(0, 2), (0, 3), (1, 3), (2, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.50629832 -0.79119546 -0.32953595  0.09527758]
True reward weights: [-0.70843905 -0.48585435  0.3607345   0.3632221 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.161053

Running EBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5222
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.49438451 -0.47259271  0.72794802 -0.04828841]
True reward weights: [-0.41497927  0.16964179  0.37859533 -0.80974036]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.159689

Running EBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5052
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.14868699 -0.63358999 -0.38508336 -0.65434449]
True reward weights: [-0.55555862 -0.50848459 -0.47660376 -0.45348307]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.156357

Running EBIRL with 4 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5142
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.79030275 -0.14415244 -0.53823245 -0.25484795]
True reward weights: [-0.40215395 -0.45264899 -0.69592316 -0.3860985 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.151564

Running EBIRL with 5 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4628
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.72526336 -0.53125703 -0.17556198  0.40116957]
True reward weights: [-0.95802791  0.16613868  0.16422666 -0.16616279]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.207737

Running EBIRL with 6 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4642
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.70699888 -0.25595847  0.15825719  0.63999415]
True reward weights: [-0.77577466 -0.30300099  0.5304962  -0.15791723]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.285934

Running EBIRL with 7 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4268
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.45484753 -0.87688543  0.13047678  0.08462548]
True reward weights: [-0.06185714 -0.95856909  0.00553063  0.27800793]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.335502

Running EBIRL with 8 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4368
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.38025203 -0.18443885 -0.08406551  0.90239886]
True reward weights: [-0.37668614 -0.04342833 -0.77362693  0.50766417]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.337533

Running EBIRL with 9 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4382
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.30276946 -0.6770423  -0.16753679  0.64951967]
True reward weights: [-0.33234086 -0.4626806  -0.72581437  0.38557712]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.340328

Running EBIRL with 10 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4286
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.20632756 -0.85801688 -0.46874193  0.03894823]
True reward weights: [-0.01197226 -0.32161881 -0.27844047  0.90492481]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.341511

Running EBIRL with 11 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4328
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.26044531 -0.58229531 -0.70170149 -0.31735695]
True reward weights: [-0.18398101 -0.91106863 -0.13666323  0.34267783]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.424329

Running EBIRL with 12 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4318
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.38854692 -0.67954715 -0.27715379 -0.5571649 ]
True reward weights: [-0.44140365 -0.4748691  -0.75302429 -0.11232355]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.425647

Running EBIRL with 13 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4382
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.21324521 -0.73399064 -0.63750559 -0.09680313]
True reward weights: [-0.38936254 -0.0532014  -0.5908386  -0.70461065]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.426637

Running EBIRL with 14 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4412
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.55440058 -0.59726056  0.02677499  0.57896712]
True reward weights: [ 0.17852992 -0.7602393  -0.5291466  -0.33191437]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.427358

Running EBIRL with 15 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4294
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.92401212 -0.21249165 -0.29766957  0.11154248]
True reward weights: [-0.36207112 -0.32014082  0.40652966  0.77533734]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.428156

Running EBIRL with 16 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4280
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.80130053 -0.22505139 -0.22245601  0.50772302]
True reward weights: [ 0.01281252 -0.28052342 -0.1721318   0.94419971]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.428901

Running EBIRL with 17 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4250
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.63331333 -0.74389089  0.01773536 -0.21266413]
True reward weights: [-0.83394941 -0.06711477  0.39777789 -0.37655908]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.429211

Running EBIRL with 18 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3934
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.22065742 -0.26650386 -0.5857613  -0.73291862]
True reward weights: [-0.61461961 -0.43480685 -0.59044051 -0.29080189]
MAP Policy for current environment:
Information gain 18 demonstrations: 0.428659

Running EBIRL with 19 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3848
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.01050487 -0.44759294 -0.68279664 -0.57735514]
True reward weights: [-0.66878281 -0.05876869  0.47793215 -0.5664421 ]
MAP Policy for current environment:
Information gain 19 demonstrations: 0.427485

Running EBIRL with 20 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4004
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.30998862 -0.27191893 -0.56456508  0.71500589]
True reward weights: [-0.25667538 -0.85109718  0.44804336 -0.09491308]
MAP Policy for current environment:
Information gain 20 demonstrations: 0.426382

Running EBIRL with 21 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.38099853 -0.88308686 -0.10869288 -0.25136342]
True reward weights: [-0.39383763 -0.2026333   0.42857578  0.78749887]
MAP Policy for current environment:
Information gain 21 demonstrations: 0.425888

Running EBIRL with 22 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.42281703 -0.89794443  0.03381685  0.11737961]
True reward weights: [-0.36384562 -0.41962685  0.59056417  0.58546018]
MAP Policy for current environment:
Information gain 22 demonstrations: 0.425695

Running EBIRL with 23 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3948
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.53845983 -0.48483052 -0.50674672 -0.46712755]
True reward weights: [-0.05720937 -0.44820519 -0.81636746  0.35969899]
MAP Policy for current environment:
Information gain 23 demonstrations: 0.425797

Running EBIRL with 24 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3922
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.38479454 -0.82332153  0.19316455  0.36981384]
True reward weights: [-0.40547685 -0.28811934 -0.28089627  0.82077589]
MAP Policy for current environment:
Information gain 24 demonstrations: 0.425487

Running EBIRL with 25 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3970
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [ 0.03880746 -0.46295446 -0.87204296  0.15397476]
True reward weights: [-0.60598592 -0.35021796 -0.71422428 -0.00348133]
MAP Policy for current environment:
Information gain 25 demonstrations: 0.508266

Running EBIRL with 26 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.25838617 -0.88143262 -0.07177405  0.38879508]
True reward weights: [-0.81364504 -0.4430967   0.17223691  0.3346364 ]
MAP Policy for current environment:
Information gain 26 demonstrations: 0.507835

Running EBIRL with 27 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.72526141 -0.19989673  0.34235099  0.5628792 ]
True reward weights: [-0.31528743 -0.84033075 -0.44023709  0.02508729]
MAP Policy for current environment:
Information gain 27 demonstrations: 0.592059

Running EBIRL with 28 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.82093013 -0.15713499  0.38427026  0.39206975]
True reward weights: [-0.42066196 -0.6734516  -0.04322362 -0.60633174]
MAP Policy for current environment:
Information gain 28 demonstrations: 0.592033

Running EBIRL with 29 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4044
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.59648842 -0.530792    0.4625156   0.38540983]
True reward weights: [-0.29752332 -0.0432281  -0.13530166 -0.94408933]
MAP Policy for current environment:
Information gain 29 demonstrations: 0.591597

Running EBIRL with 30 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.73665486 -0.48769969  0.46278107  0.07295412]
True reward weights: [-0.58580548 -0.24443175  0.04703396  0.77128001]
MAP Policy for current environment:
Information gain 30 demonstrations: 0.590728

Running EBIRL with 31 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4002
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.45748921 -0.38814007  0.32064894  0.73296328]
True reward weights: [-0.56791504 -0.4857937  -0.19806863  0.63422851]
MAP Policy for current environment:
Information gain 31 demonstrations: 0.590621

Running EBIRL with 32 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4012
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.62703163 -0.30241686  0.58811563 -0.41169818]
True reward weights: [-0.07657529 -0.32529383 -0.83969485 -0.4280569 ]
MAP Policy for current environment:
Information gain 32 demonstrations: 0.590554

Running EBIRL with 33 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3978
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.70155243 -0.34466505  0.36929732  0.50264269]
True reward weights: [-0.63883783 -0.40468303 -0.62530149 -0.19265493]
MAP Policy for current environment:
Information gain 33 demonstrations: 0.590742

Running EBIRL with 34 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3974
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.75952707 -0.41717384  0.45015145  0.21551867]
True reward weights: [-0.21451154 -0.10928159 -0.74861323 -0.61775446]
MAP Policy for current environment:
Information gain 34 demonstrations: 0.590622

Running EBIRL with 35 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.51337346 -0.47550632  0.34426239  0.62595914]
True reward weights: [-0.31972407 -0.49509321  0.04210105 -0.80677552]
MAP Policy for current environment:
Information gain 35 demonstrations: 0.591089

Running EBIRL with 36 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3970
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.7659483  -0.36636467  0.44187044 -0.28956976]
True reward weights: [-0.20900582 -0.47970947 -0.41155908 -0.74620005]
MAP Policy for current environment:
Information gain 36 demonstrations: 0.591027

Running EBIRL with 37 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4040
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.53922853 -0.40695755  0.32337797  0.66260459]
True reward weights: [-0.27025625 -0.14138343 -0.42145325 -0.85401958]
MAP Policy for current environment:
Information gain 37 demonstrations: 0.590707

Running EBIRL with 38 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.77674449 -0.13729117  0.53723242  0.29866445]
True reward weights: [-0.19380675 -0.97117996  0.13018155 -0.04797066]
MAP Policy for current environment:
Information gain 38 demonstrations: 0.590491

Running EBIRL with 39 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4076
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.61093922 -0.43595366  0.62566506 -0.2126991 ]
True reward weights: [-0.97575801 -0.10490903 -0.15324184  0.11579008]
MAP Policy for current environment:
Information gain 39 demonstrations: 0.590452

Running EBIRL with 40 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3908
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.7945937  -0.37333224  0.47785376 -0.02999465]
True reward weights: [-0.26889863 -0.46559046 -0.72932292  0.42309235]
MAP Policy for current environment:
Information gain 40 demonstrations: 0.590177

Running experiment 10/19...
Shuffled Demos: [([(0, 3), (1, 0), (1, 0), (1, 2)], 0), ([(0, 0), (0, 3), (1, 2), (0, 3)], 0), ([(0, 2), (0, 0), (0, 3), (1, 0)], 0), ([(0, 1), (5, 0), (0, 2), (0, 0)], 0), ([(0, 1), (5, 2), (5, 3), (6, 2)], 0), ([(0, 1), (5, 2), (5, 0), (0, 1)], 0), ([(0, 3), (1, 0), (1, 2), (0, 2)], 0), ([(0, 3), (1, 3), (2, 0), (2, 3)], 0), ([(0, 1), (5, 3), (6, 2), (5, 3)], 0), ([(0, 1), (5, 3), (6, 2), (5, 0)], 0), ([(0, 0), (0, 0), (0, 2), (0, 2)], 0), ([(0, 0), (0, 0), (0, 2), (0, 1)], 0), ([(0, 3), (1, 3), (2, 1), (7, 0)], 0), ([(0, 1), (5, 1), (10, 3), (11, 3)], 0), ([(0, 2), (0, 2), (0, 1), (5, 3)], 0), ([(0, 1), (5, 2), (5, 1), (10, 1)], 0), ([(0, 1), (5, 0), (0, 1), (5, 1)], 0), ([(0, 1), (5, 2), (5, 1), (10, 0)], 0), ([(0, 2), (0, 2), (0, 0), (0, 2)], 0), ([(0, 1), (5, 0), (0, 3), (1, 3)], 0), ([(0, 3), (1, 3), (2, 3), (3, 0)], 0), ([(0, 3), (1, 1), (6, 2), (5, 1)], 0), ([(0, 2), (0, 2), (0, 2), (0, 2)], 0), ([(0, 3), (1, 3), (2, 2), (1, 2)], 0), ([(0, 0), (0, 0), (0, 3), (1, 2)], 0), ([(0, 3), (1, 0), (1, 1), (6, 0)], 0), ([(0, 3), (1, 3), (2, 0), (2, 0)], 0), ([(0, 2), (0, 1), (5, 2), (5, 1)], 0), ([(0, 0), (0, 3), (1, 3), (2, 2)], 0), ([(0, 2), (0, 0), (0, 3), (1, 3)], 0), ([(0, 0), (0, 3), (1, 2), (0, 2)], 0), ([(0, 3), (1, 0), (1, 0), (1, 2)], 0), ([(0, 3), (1, 1), (6, 2), (5, 0)], 0), ([(0, 1), (5, 2), (5, 1), (10, 2)], 0), ([(0, 2), (0, 2), (0, 3), (1, 2)], 0), ([(0, 3), (1, 1), (6, 1), (11, 0)], 0), ([(0, 1), (5, 3), (6, 2), (5, 1)], 0), ([(0, 1), (5, 0), (0, 3), (1, 3)], 0), ([(0, 1), (5, 0), (0, 1), (5, 2)], 0), ([(0, 3), (1, 0), (1, 2), (0, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5926
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.44132245  0.49626357 -0.118976    0.73810682]
True reward weights: [-0.59500066 -0.16244209  0.37080191  0.69432898]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.128450

Running EBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4578
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.49037865 -0.20152718  0.23040467  0.81598362]
True reward weights: [-0.15155024  0.19191704 -0.74900194 -0.61578931]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124012

Running EBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4624
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.72068708 -0.08352105 -0.11393927 -0.67871364]
True reward weights: [-0.76775378 -0.23849996 -0.58390664  0.11280486]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127636

Running EBIRL with 4 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4078
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.15968797 -0.45578007 -0.47407007  0.73622133]
True reward weights: [-0.1121782  -0.36960135  0.85030542 -0.35747949]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.176139

Running EBIRL with 5 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.11493451 -0.59189706 -0.73365923  0.31335613]
True reward weights: [-0.29006739 -0.34230344 -0.66062299 -0.60188582]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.182546

Running EBIRL with 6 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4194
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.63303249 -0.66056086 -0.23315062 -0.32949962]
True reward weights: [-0.37043151 -0.3817927  -0.36506571 -0.76403001]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.254917

Running EBIRL with 7 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4074
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.36395608 -0.20448477 -0.27416966 -0.86634459]
True reward weights: [-0.30048937 -0.43411133 -0.57884739 -0.62144122]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.257800

Running EBIRL with 8 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4090
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.73504842 -0.11557036 -0.25393245  0.61795277]
True reward weights: [-0.26475355 -0.25993559 -0.83261039 -0.41121646]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.259965

Running EBIRL with 9 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4050
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.38963231 -0.87714086  0.08052438 -0.26893569]
True reward weights: [-0.13561124 -0.79994113 -0.25649587 -0.52527484]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.261877

Running EBIRL with 10 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.21352866 -0.59811676 -0.53159986  0.56041364]
True reward weights: [-0.4428381  -0.39710369 -0.64214142  0.48358812]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.264213

Running EBIRL with 11 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4216
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.32763109 -0.7876706   0.24173466 -0.46238215]
True reward weights: [-0.55475474 -0.43992152  0.61759255 -0.34248455]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.343188

Running EBIRL with 12 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4054
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.12011815 -0.81868092 -0.29401821  0.47842082]
True reward weights: [-0.54405077 -0.07417855 -0.81484819  0.18581908]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.426678

Running EBIRL with 13 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4062
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.75739198 -0.40793204 -0.28590833 -0.42214365]
True reward weights: [-0.7590356  -0.0547998  -0.46618266 -0.45114928]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.427722

Running EBIRL with 14 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4074
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.24095068 -0.85190544  0.36053235  0.29362616]
True reward weights: [-0.39707628 -0.86668657 -0.23708101 -0.1870225 ]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.428436

Running EBIRL with 15 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4076
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.34340934 -0.30644508 -0.39582451 -0.79465993]
True reward weights: [-0.78237303 -0.50097088  0.1998646   0.31140769]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.513267

Running EBIRL with 16 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4114
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.72168349 -0.28083506 -0.60207121  0.19446046]
True reward weights: [-0.16308147 -0.10632706  0.82489655  0.53070206]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.514224

Running EBIRL with 17 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4064
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.4624615  -0.51880065 -0.55078174  0.46218472]
True reward weights: [-0.36437834 -0.31573127 -0.71416785  0.50745095]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.599750

Running EBIRL with 18 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4072
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.14580633 -0.68912443 -0.40658849  0.5818366 ]
True reward weights: [-0.48060504 -0.68909262  0.39172937 -0.37512433]
MAP Policy for current environment:
Information gain 18 demonstrations: 0.600638

Running EBIRL with 19 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4156
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.615026   -0.37711118 -0.21388568  0.65862212]
True reward weights: [-0.51783261 -0.7722923   0.04278904  0.3654902 ]
MAP Policy for current environment:
Information gain 19 demonstrations: 0.686631

Running EBIRL with 20 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4088
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.610379   -0.73345273 -0.29911649  0.00372669]
True reward weights: [-0.48381564 -0.23843946 -0.84157884  0.02853246]
MAP Policy for current environment:
Information gain 20 demonstrations: 0.687127

Running EBIRL with 21 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4076
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.57318126 -0.2269735   0.78385098  0.07432303]
True reward weights: [-0.03515872 -0.89102857 -0.42646251  0.15153113]
MAP Policy for current environment:
Information gain 21 demonstrations: 0.687739

Running EBIRL with 22 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4032
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.22446562 -0.63544034  0.21357905  0.70725862]
True reward weights: [-0.19438775 -0.66728096  0.13567686  0.70607458]
MAP Policy for current environment:
Information gain 22 demonstrations: 0.688020

Running EBIRL with 23 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4088
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.49954209 -0.59606661  0.42946597 -0.45904387]
True reward weights: [-0.25570201 -0.2301071  -0.92611833  0.15482909]
MAP Policy for current environment:
Information gain 23 demonstrations: 0.773559

Running EBIRL with 24 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4150
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.15584783 -0.84110483 -0.2182284  -0.4697132 ]
True reward weights: [-0.00940085 -0.32101896 -0.94123271 -0.10459176]
MAP Policy for current environment:
Information gain 24 demonstrations: 0.773337

Running EBIRL with 25 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4076
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.78000955 -0.37597143 -0.24510916 -0.43606431]
True reward weights: [-0.37464123 -0.34202059  0.78003155  0.3663559 ]
MAP Policy for current environment:
Information gain 25 demonstrations: 0.773706

Running EBIRL with 26 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.3960
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.19837123 -0.71201912 -0.39764132 -0.54365338]
True reward weights: [-0.5597769  -0.60994071  0.24287334  0.50560329]
MAP Policy for current environment:
Information gain 26 demonstrations: 0.773811

Running EBIRL with 27 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.3952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.34279149 -0.51553786 -0.00235525 -0.78530832]
True reward weights: [-0.91166763 -0.24103852  0.25749339 -0.21085475]
MAP Policy for current environment:
Information gain 27 demonstrations: 0.773797

Running EBIRL with 28 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4018
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.77303196 -0.17005756 -0.38010355 -0.4785638 ]
True reward weights: [-0.60437383 -0.09169962  0.7299816   0.30569646]
MAP Policy for current environment:
Information gain 28 demonstrations: 0.859598

Running EBIRL with 29 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.65784252 -0.13509043  0.66531966 -0.32610358]
True reward weights: [-0.90776242 -0.06717276  0.08222445  0.40582552]
MAP Policy for current environment:
Information gain 29 demonstrations: 0.859961

Running EBIRL with 30 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4064
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.30005187 -0.76712022 -0.54571667  0.15391152]
True reward weights: [-0.65113811 -0.20454137  0.41887919 -0.59893424]
MAP Policy for current environment:
Information gain 30 demonstrations: 0.860394

Running EBIRL with 31 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4042
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.46560383 -0.5037446   0.72628611 -0.04430492]
True reward weights: [-0.27681061 -0.8470347  -0.45351639 -0.01519852]
MAP Policy for current environment:
Information gain 31 demonstrations: 0.860858

Running EBIRL with 32 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.3998
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.36682653 -0.31568113 -0.00964896 -0.8750375 ]
True reward weights: [-0.42413046 -0.3693665  -0.3757941  -0.7365192 ]
MAP Policy for current environment:
Information gain 32 demonstrations: 0.861168

Running EBIRL with 33 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.3994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.73570118 -0.30483381 -0.59746962 -0.09407535]
True reward weights: [-0.68400235 -0.68017548 -0.18714126 -0.18568858]
MAP Policy for current environment:
Information gain 33 demonstrations: 0.861403

Running EBIRL with 34 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4146
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.82071149 -0.25319656  0.50083411 -0.10718836]
True reward weights: [-0.07850226 -0.12667377 -0.0593479   0.98705065]
MAP Policy for current environment:
Information gain 34 demonstrations: 0.861830

Running EBIRL with 35 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4116
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.23726342 -0.47410367  0.50310683 -0.68250663]
True reward weights: [-0.59811093 -0.76869721 -0.10746594  0.19954696]
MAP Policy for current environment:
Information gain 35 demonstrations: 0.862118

Running EBIRL with 36 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4086
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.15669386 -0.26193514  0.86561227 -0.39692872]
True reward weights: [-0.10831185 -0.96500293  0.0225004  -0.23776377]
MAP Policy for current environment:
Information gain 36 demonstrations: 0.862255

Running EBIRL with 37 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.3984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.72154648 -0.42275009  0.39441565  0.38090595]
True reward weights: [-4.89156026e-01 -8.08718468e-01  6.85630112e-04  3.26650198e-01]
MAP Policy for current environment:
Information gain 37 demonstrations: 0.862911

Running EBIRL with 38 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4018
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.80562948 -0.18706285 -0.5434416  -0.14366581]
True reward weights: [-0.57875686 -0.18902102  0.67905749 -0.41011276]
MAP Policy for current environment:
Information gain 38 demonstrations: 0.863102

Running EBIRL with 39 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4116
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.51900252 -0.2160986  -0.75239199  0.34328424]
True reward weights: [-0.41077478 -0.13916826 -0.64326655  0.63095517]
MAP Policy for current environment:
Information gain 39 demonstrations: 0.948744

Running EBIRL with 40 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4084
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.21979838 -0.30655629 -0.53661129  0.75482464]
True reward weights: [-0.86725572 -0.17604399 -0.42800214  0.1835489 ]
MAP Policy for current environment:
Information gain 40 demonstrations: 0.948657

Saving results to files...
Results saved successfully.

Running experiment 11/19...
Shuffled Demos: [([(0, 2), (0, 0), (0, 2), (0, 0)], 0), ([(0, 2), (0, 2), (0, 3), (1, 1)], 0), ([(0, 1), (5, 0), (0, 0), (0, 0)], 0), ([(0, 0), (0, 1), (5, 2), (5, 1)], 0), ([(0, 0), (0, 0), (0, 2), (0, 1)], 0), ([(0, 3), (1, 1), (6, 2), (5, 3)], 2), ([(0, 1), (5, 2), (5, 0), (0, 3)], 0), ([(0, 2), (0, 2), (0, 0), (0, 2)], 0), ([(0, 2), (0, 0), (0, 1), (5, 2)], 0), ([(0, 0), (0, 0), (0, 2), (0, 2)], 0), ([(0, 3), (1, 3), (2, 2), (1, 2)], 2), ([(0, 0), (0, 0), (0, 3), (1, 3)], 0), ([(0, 3), (1, 2), (0, 0), (0, 2)], 0), ([(0, 2), (0, 3), (1, 3), (2, 0)], 3), ([(0, 0), (0, 0), (0, 1), (5, 3)], 0), ([(0, 1), (5, 2), (5, 1), (10, 3)], 0), ([(0, 3), (1, 2), (0, 0), (0, 3)], 0), ([(0, 3), (1, 2), (0, 3), (1, 2)], 0), ([(0, 0), (0, 0), (0, 3), (1, 3)], 0), ([(0, 3), (1, 3), (2, 2), (1, 0)], 2), ([(0, 2), (0, 2), (0, 3), (1, 1)], 0), ([(0, 0), (0, 3), (1, 1), (6, 2)], 3), ([(0, 1), (5, 3), (6, 1), (11, 2)], 2), ([(0, 0), (0, 0), (0, 1), (5, 3)], 0), ([(0, 0), (0, 1), (5, 0), (0, 0)], 0), ([(0, 3), (1, 3), (2, 3), (3, 3)], 2), ([(0, 0), (0, 2), (0, 0), (0, 0)], 0), ([(0, 3), (1, 0), (1, 0), (1, 2)], 0), ([(0, 3), (1, 1), (6, 1), (11, 3)], 2), ([(0, 2), (0, 3), (1, 0), (1, 0)], 0), ([(0, 1), (5, 2), (5, 2), (5, 1)], 0), ([(0, 0), (0, 0), (0, 2), (0, 2)], 0), ([(0, 3), (1, 3), (2, 3), (3, 2)], 2), ([(0, 1), (5, 2), (5, 0), (0, 2)], 0), ([(0, 0), (0, 0), (0, 0), (0, 1)], 0), ([(0, 1), (5, 2), (5, 1), (10, 3)], 0), ([(0, 3), (1, 3), (2, 0), (2, 1)], 3), ([(0, 0), (0, 3), (1, 0), (1, 2)], 0), ([(0, 2), (0, 0), (0, 3), (1, 3)], 0), ([(0, 3), (1, 2), (0, 1), (5, 2)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.90737246 -0.1209572  -0.33341032 -0.22557066]
True reward weights: [-0.0702138  -0.06544465  0.14444996  0.98484579]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5310
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.58364559  0.01918421 -0.26253627 -0.76815656]
True reward weights: [-0.18979203  0.23468161  0.39561107 -0.86740729]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.268247

Running EBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5402
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.64081116  0.65506295 -0.28119814  0.28492313]
True reward weights: [-0.63608445 -0.1968393  -0.63237075 -0.39592687]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.342452

Running EBIRL with 4 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5334
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.80371802  0.40943541 -0.25959106 -0.34498185]
True reward weights: [-0.33253498 -0.77204023 -0.50539126  0.19481798]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.419904

Running EBIRL with 5 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5416
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.68083049  0.68666949 -0.19596797 -0.16294605]
True reward weights: [-0.72602873  0.59547712 -0.26067502 -0.22436089]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.503062

Running EBIRL with 6 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5416
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-5.42317984e-01 -2.16606844e-04  3.36117662e-01 -7.70010438e-01]
True reward weights: [-0.2098221   0.26171     0.38372401  0.86037111]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.550673

Running EBIRL with 7 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5344
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.54254803  0.66980632 -0.19856896 -0.46644559]
True reward weights: [-0.70719415 -0.18637127  0.14634464 -0.66612719]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.638054

Running EBIRL with 8 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5272
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.69697971  0.35427267  0.44333906  0.43836131]
True reward weights: [-0.89427793  0.21996828  0.33612982  0.19722497]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.720470

Running EBIRL with 9 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5390
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.52348864  0.46071656 -0.65656671 -0.28743704]
True reward weights: [-0.26132383  0.19992064  0.93005827  0.16350293]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.807036

Running EBIRL with 10 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.6153911   0.06943919 -0.47826395  0.62268418]
True reward weights: [-0.80541763 -0.34167948  0.47483076  0.0953589 ]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.891749

Running EBIRL with 11 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4062
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.50665756 -0.80081149 -0.26580819 -0.17704542]
True reward weights: [-0.85693813  0.142854   -0.2081244   0.44937068]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.884626

Running EBIRL with 12 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4116
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.499163   -0.62412789  0.06077085  0.59800299]
True reward weights: [-0.04848037 -0.04622515 -0.05110359 -0.99644433]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.880325

Running EBIRL with 13 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4100
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.57336022 -0.65514258  0.49164913 -0.01809415]
True reward weights: [-0.36136189 -0.24625218 -0.17780865  0.88156765]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.876749

Running EBIRL with 14 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.37840197 -0.34573145  0.85273014 -0.10066294]
True reward weights: [-0.01249701 -0.69385932  0.59392293  0.40701182]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.958042

Running EBIRL with 15 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4072
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.37838932 -0.8519059   0.24234002 -0.26897804]
True reward weights: [-0.70491415 -0.48190417  0.1805126   0.48813892]
MAP Policy for current environment:
Information gain 15 demonstrations: 1.040400

Running EBIRL with 16 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4092
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.36278035 -0.4152751   0.82754977  0.10534891]
True reward weights: [-0.533559   -0.07340347  0.79642605 -0.27501321]
MAP Policy for current environment:
Information gain 16 demonstrations: 1.122570

Running EBIRL with 17 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4138
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.37680906 -0.35113374  0.85213715  0.09264077]
True reward weights: [-0.10912497 -0.64288866  0.02763503 -0.75764254]
MAP Policy for current environment:
Information gain 17 demonstrations: 1.120512

Running EBIRL with 18 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4122
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.35086185 -0.34876468  0.74560315  0.4464696 ]
True reward weights: [-0.36275661 -0.76302937 -0.53354949 -0.03897125]
MAP Policy for current environment:
Information gain 18 demonstrations: 1.117731

Running EBIRL with 19 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4052
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.37261655 -0.36296875  0.84572112 -0.11902258]
True reward weights: [-0.61460052 -0.33861034  0.08216448 -0.70771338]
MAP Policy for current environment:
Information gain 19 demonstrations: 1.115955

Running EBIRL with 20 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4058
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.35340333 -0.40166402  0.83085535  0.15313879]
True reward weights: [-0.19888204 -0.02884496 -0.3032618  -0.93147527]
MAP Policy for current environment:
Information gain 20 demonstrations: 1.114005

Running EBIRL with 21 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3980
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.48937974 -0.50925244  0.57923744  0.40700542]
True reward weights: [-0.59469472 -0.79810744  0.09172377 -0.0308132 ]
MAP Policy for current environment:
Information gain 21 demonstrations: 1.112202

Running EBIRL with 22 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4026
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.34256825 -0.44832657  0.82538059 -0.01992901]
True reward weights: [-0.35185756 -0.22111972  0.2196928  -0.88263095]
MAP Policy for current environment:
Information gain 22 demonstrations: 1.196526

Running EBIRL with 23 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3998
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.49717058 -0.69559789 -0.31637304 -0.41094171]
True reward weights: [-0.24497473 -0.43274696  0.16885468 -0.85100267]
MAP Policy for current environment:
Information gain 23 demonstrations: 1.196172

Running EBIRL with 24 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4158
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.34862238 -0.4423923   0.82597273 -0.02281555]
True reward weights: [-0.6174733  -0.33140734  0.60311982  0.38097557]
MAP Policy for current environment:
Information gain 24 demonstrations: 1.280304

Running EBIRL with 25 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4092
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.353775   -0.4007104   0.82378622  0.18881389]
True reward weights: [-0.08116828 -0.39360839 -0.17708672 -0.89840104]
MAP Policy for current environment:
Information gain 25 demonstrations: 1.364534

Running EBIRL with 26 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4068
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.3545637  -0.40308024  0.83043653  0.1489499 ]
True reward weights: [-0.85570823 -0.08410516  0.48782044  0.15073476]
MAP Policy for current environment:
Information gain 26 demonstrations: 1.364349

Running EBIRL with 27 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4060
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.35990002 -0.39107513  0.82833299 -0.17719109]
True reward weights: [-0.26403089 -0.05170872 -0.92287166 -0.27553911]
MAP Policy for current environment:
Information gain 27 demonstrations: 1.449047

Running EBIRL with 28 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4046
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.39631129 -0.70875772 -0.39817144 -0.42668415]
True reward weights: [-0.69710104 -0.62918865 -0.16949094 -0.2990729 ]
MAP Policy for current environment:
Information gain 28 demonstrations: 1.448461

Running EBIRL with 29 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3982
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.39255769 -0.37645903  0.83505316 -0.08284491]
True reward weights: [-0.24458026 -0.54641436 -0.77869935  0.18772096]
MAP Policy for current environment:
Information gain 29 demonstrations: 1.448090

Running EBIRL with 30 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.38199641 -0.35710668  0.84530094  0.1096352 ]
True reward weights: [-0.73661382 -0.21771451  0.45787864 -0.44760209]
MAP Policy for current environment:
Information gain 30 demonstrations: 1.447516

Running EBIRL with 31 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4028
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.34891145 -0.41506581  0.83167187  0.11959546]
True reward weights: [-0.57606216 -0.56900126  0.58554474 -0.03908078]
MAP Policy for current environment:
Information gain 31 demonstrations: 1.532603

Running EBIRL with 32 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4084
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.37070126 -0.39272852  0.84046257 -0.0443572 ]
True reward weights: [-0.17419647 -0.05876066 -0.0960446  -0.97825263]
MAP Policy for current environment:
Information gain 32 demonstrations: 1.617942

Running EBIRL with 33 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4024
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.48917084 -0.74502881  0.33559963  0.30498664]
True reward weights: [-0.47794892 -0.86795736 -0.0333379   0.13078009]
MAP Policy for current environment:
Information gain 33 demonstrations: 1.617215

Running EBIRL with 34 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4078
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.39290866 -0.36082777  0.8454518   0.02524583]
True reward weights: [-0.49897683 -0.44666902 -0.53687142 -0.51310621]
MAP Policy for current environment:
Information gain 34 demonstrations: 1.702435

Running EBIRL with 35 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3982
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.84904964 -0.37160547  0.21209373 -0.30990375]
True reward weights: [-0.94146344 -0.33306235 -0.05154633  0.00768396]
MAP Policy for current environment:
Information gain 35 demonstrations: 1.787580

Running EBIRL with 36 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4124
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.3813621  -0.34756346  0.82097363 -0.24446859]
True reward weights: [-0.61898515 -0.48087589  0.50700463 -0.35855553]
MAP Policy for current environment:
Information gain 36 demonstrations: 1.872594

Running EBIRL with 37 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3930
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.3417396  -0.35643434  0.86853804  0.04254732]
True reward weights: [-0.2096712  -0.55866803  0.73794103 -0.31523175]
MAP Policy for current environment:
Information gain 37 demonstrations: 1.958085

Running EBIRL with 38 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3982
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.34837312 -0.35060258  0.79928682 -0.34183998]
True reward weights: [-0.31283585 -0.01683221 -0.8584169  -0.40616601]
MAP Policy for current environment:
Information gain 38 demonstrations: 1.957633

Running EBIRL with 39 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4086
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.34194778 -0.35194632  0.86301123  0.12007128]
True reward weights: [-0.59930668 -0.21439201 -0.23235216  0.73544547]
MAP Policy for current environment:
Information gain 39 demonstrations: 1.957462

Running EBIRL with 40 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4024
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.50381775 -0.72370984  0.46742774 -0.06263416]
True reward weights: [-0.18322564 -0.31904184 -0.08249949 -0.92619356]
MAP Policy for current environment:
Information gain 40 demonstrations: 1.957418

Running experiment 12/19...
Shuffled Demos: [([(0, 2), (0, 3), (1, 1), (6, 2)], 3), ([(0, 3), (1, 0), (1, 3), (2, 1)], 2), ([(0, 3), (1, 1), (6, 0), (1, 0)], 3), ([(0, 3), (1, 3), (2, 0), (2, 3)], 0), ([(0, 2), (0, 0), (0, 3), (1, 3)], 0), ([(0, 2), (0, 1), (5, 3), (6, 0)], 3), ([(0, 1), (5, 2), (5, 2), (5, 2)], 3), ([(0, 0), (0, 0), (0, 3), (1, 2)], 0), ([(0, 3), (1, 0), (1, 1), (6, 2)], 3), ([(0, 2), (0, 0), (0, 3), (1, 1)], 0), ([(0, 3), (1, 2), (0, 1), (5, 0)], 1), ([(0, 2), (0, 2), (0, 3), (1, 2)], 0), ([(0, 2), (0, 3), (1, 1), (6, 0)], 3), ([(0, 0), (0, 1), (5, 1), (10, 2)], 3), ([(0, 0), (0, 2), (0, 3), (1, 3)], 0), ([(0, 0), (0, 3), (1, 3), (2, 3)], 0), ([(0, 0), (0, 0), (0, 1), (5, 2)], 0), ([(0, 3), (1, 1), (6, 3), (7, 3)], 2), ([(0, 1), (5, 0), (0, 0), (0, 0)], 0), ([(0, 3), (1, 2), (0, 1), (5, 1)], 1), ([(0, 1), (5, 3), (6, 1), (11, 3)], 2), ([(0, 1), (5, 1), (10, 0), (5, 1)], 3), ([(0, 2), (0, 2), (0, 3), (1, 1)], 0), ([(0, 3), (1, 3), (2, 0), (2, 3)], 0), ([(0, 0), (0, 1), (5, 3), (6, 2)], 3), ([(0, 2), (0, 2), (0, 1), (5, 2)], 0), ([(0, 0), (0, 2), (0, 2), (0, 3)], 0), ([(0, 3), (1, 2), (0, 2), (0, 0)], 0), ([(0, 2), (0, 0), (0, 1), (5, 0)], 0), ([(0, 1), (5, 2), (5, 0), (0, 2)], 2), ([(0, 3), (1, 3), (2, 0), (2, 3)], 0), ([(0, 1), (5, 2), (5, 0), (0, 0)], 2), ([(0, 1), (5, 2), (5, 0), (0, 3)], 2), ([(0, 1), (5, 3), (6, 1), (11, 1)], 2), ([(0, 0), (0, 1), (5, 2), (5, 0)], 3), ([(0, 1), (5, 2), (5, 0), (0, 3)], 2), ([(0, 1), (5, 3), (6, 2), (5, 0)], 3), ([(0, 2), (0, 3), (1, 2), (0, 2)], 0), ([(0, 3), (1, 2), (0, 2), (0, 3)], 0), ([(0, 3), (1, 1), (6, 0), (1, 1)], 3)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.39266301 -0.24004622  0.88645551 -0.04888975]
True reward weights: [-0.58571354 -0.27832207  0.48101309  0.59000244]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5830
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.34394334 -0.46654206  0.43836739 -0.68693196]
True reward weights: [ 0.98532654 -0.0862232  -0.11678428  0.08976969]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.192734

Running EBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5890
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.38622511  0.61250414  0.5534667  -0.41151362]
True reward weights: [-0.19535839  0.1769408   0.6483985   0.71421736]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.247237

Running EBIRL with 4 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5322
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.43349383 -0.33797997  0.35608258 -0.75568369]
True reward weights: [-0.87210822 -0.16991785  0.11815091  0.44339096]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.830676

Running EBIRL with 5 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5372
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.60781732 -0.28339859  0.62724905  0.3959823 ]
True reward weights: [-0.85291241 -0.26906336 -0.09105474  0.43801183]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.908789

Running EBIRL with 6 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5310
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.40043582 -0.12507708  0.34487683 -0.83968259]
True reward weights: [-0.59598645 -0.32501209  0.05850815  0.73194541]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.998331

Running EBIRL with 7 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5406
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.6638174   0.25896774  0.64627048 -0.27316046]
True reward weights: [-0.58213122  0.32375343 -0.71356106 -0.21711188]
MAP Policy for current environment:
Information gain 7 demonstrations: 1.086239

Running EBIRL with 8 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5292
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.54686935 -0.09879526  0.60704743 -0.56803769]
True reward weights: [-0.50652956  0.66682175  0.22027037  0.50025746]
MAP Policy for current environment:
Information gain 8 demonstrations: 1.133768

Running EBIRL with 9 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5506
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.39534211 -0.21601194  0.23603978  0.86100446]
True reward weights: [-0.06117608  0.82099904  0.0578742   0.56468454]
MAP Policy for current environment:
Information gain 9 demonstrations: 1.218102

Running EBIRL with 10 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5408
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.53169554 -0.71110703  0.38847386 -0.24640355]
True reward weights: [-0.10930079  0.0403249   0.13025958  0.98461144]
MAP Policy for current environment:
Information gain 10 demonstrations: 1.256626

Running EBIRL with 11 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.82680418 -0.12605218  0.2129734  -0.5051218 ]
True reward weights: [-0.0987847  -0.73653288 -0.26813851  0.61307638]
MAP Policy for current environment:
Information gain 11 demonstrations: 1.448454

Running EBIRL with 12 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.43597249  0.83503388  0.04259457 -0.33291458]
True reward weights: [-0.47866394 -0.30199115 -0.45822701  0.68535406]
MAP Policy for current environment:
Information gain 12 demonstrations: 1.500040

Running EBIRL with 13 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.72113063  0.4193019   0.13552081 -0.53459391]
True reward weights: [-0.62845034 -0.41151874  0.47676848  0.45650226]
MAP Policy for current environment:
Information gain 13 demonstrations: 1.587310

Running EBIRL with 14 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5100
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.79302909  0.50190157  0.2076198  -0.27585087]
True reward weights: [-0.35278342  0.85746349  0.03898431 -0.37253247]
MAP Policy for current environment:
Information gain 14 demonstrations: 1.673004

Running EBIRL with 15 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5040
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.84716588  0.16701553  0.49462386 -0.09880801]
True reward weights: [-0.13749104 -0.35306087 -0.35125342  0.85619231]
MAP Policy for current environment:
Information gain 15 demonstrations: 1.713965

Running EBIRL with 16 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.8253973   0.25388611  0.48709923  0.13036675]
True reward weights: [-0.02474373 -0.42183021 -0.28832165 -0.85925412]
MAP Policy for current environment:
Information gain 16 demonstrations: 1.746512

Running EBIRL with 17 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4960
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.59005097  0.57639867  0.11438071 -0.55364382]
True reward weights: [-0.56853833 -0.52490483 -0.60884959  0.1747606 ]
MAP Policy for current environment:
Information gain 17 demonstrations: 1.774910

Running EBIRL with 18 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.50378545  0.58857444  0.10312638  0.62381511]
True reward weights: [-0.59426252  0.74083078  0.20996695  0.23224058]
MAP Policy for current environment:
Information gain 18 demonstrations: 1.972650

Running EBIRL with 19 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5032
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.8079061   0.42994409  0.18657807 -0.35724562]
True reward weights: [-0.04870517  0.5510375  -0.47044455  0.68750811]
MAP Policy for current environment:
Information gain 19 demonstrations: 2.010097

Running EBIRL with 20 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4946
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.90791528 -0.08646029  0.37341644 -0.16963083]
True reward weights: [-0.17220982 -0.89081697 -0.09393535  0.40983539]
MAP Policy for current environment:
Information gain 20 demonstrations: 2.183237

Running EBIRL with 21 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3746
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.62947916 -0.49076349  0.13833397 -0.58631979]
True reward weights: [-0.86048905  0.41151973 -0.29508013  0.0560163 ]
MAP Policy for current environment:
Information gain 21 demonstrations: 2.317909

Running EBIRL with 22 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3690
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.5914392  -0.79325793  0.09919876  0.10536189]
True reward weights: [-0.18764211 -0.02356836 -0.44810281 -0.87374988]
MAP Policy for current environment:
Information gain 22 demonstrations: 2.396293

Running EBIRL with 23 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3804
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.62267074 -0.71959665  0.28407284 -0.11732189]
True reward weights: [-0.40264528 -0.62607666 -0.03444137  0.66687224]
MAP Policy for current environment:
Information gain 23 demonstrations: 2.435956

Running EBIRL with 24 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3756
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.6361141  -0.59376148  0.28898046  0.39911961]
True reward weights: [-0.55393557 -0.77455671  0.28835944 -0.10033004]
MAP Policy for current environment:
Information gain 24 demonstrations: 2.473411

Running EBIRL with 25 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3718
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.76508087 -0.48946826  0.19218538  0.37166768]
True reward weights: [-0.36372018 -0.02268998 -0.91586467 -0.16847759]
MAP Policy for current environment:
Information gain 25 demonstrations: 2.548926

Running EBIRL with 26 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3688
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.84696194 -0.34877652  0.36391712  0.16904064]
True reward weights: [-0.5476843  -0.65052195 -0.52617645 -0.00120226]
MAP Policy for current environment:
Information gain 26 demonstrations: 2.587707

Running EBIRL with 27 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3606
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.70149267 -0.68978234  0.15133401  0.09594988]
True reward weights: [-0.59704181 -0.57066655  0.49388599  0.2719511 ]
MAP Policy for current environment:
Information gain 27 demonstrations: 2.675850

Running EBIRL with 28 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3750
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.82373011 -0.49656519  0.20519371  0.18107253]
True reward weights: [-0.71180413 -0.27820538  0.21752269 -0.60714127]
MAP Policy for current environment:
Information gain 28 demonstrations: 2.714877

Running EBIRL with 29 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.42555708 -0.83357298  0.07610567  0.34389706]
True reward weights: [-0.40741164 -0.60712348 -0.36650249  0.5754066 ]
MAP Policy for current environment:
Information gain 29 demonstrations: 2.747497

Running EBIRL with 30 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3624
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.63486931 -0.62474184  0.13903639 -0.43279034]
True reward weights: [-0.57732207 -0.36962367 -0.52438559 -0.50507159]
MAP Policy for current environment:
Information gain 30 demonstrations: 2.925280

Running EBIRL with 31 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3746
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.35497797 -0.92381986  0.01557075  0.14249584]
True reward weights: [-0.89200802 -0.12280631  0.36634586 -0.23458691]
MAP Policy for current environment:
Information gain 31 demonstrations: 2.957621

Running EBIRL with 32 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3818
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.75360839 -0.56812441  0.3178405   0.09103   ]
True reward weights: [-0.86225342 -0.26551271 -0.42645194  0.06450421]
MAP Policy for current environment:
Information gain 32 demonstrations: 3.123198

Running EBIRL with 33 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3742
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.76265488 -0.61624432  0.18126199  0.07579294]
True reward weights: [-0.24777137 -0.68866573 -0.25617083 -0.63144705]
MAP Policy for current environment:
Information gain 33 demonstrations: 3.276373

Running EBIRL with 34 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3732
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.78344724 -0.57381204  0.19070267 -0.14346657]
True reward weights: [-0.73144366 -0.04525115  0.67953414 -0.03429077]
MAP Policy for current environment:
Information gain 34 demonstrations: 3.278802

Running EBIRL with 35 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3674
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.54550601 -0.45997036  0.11087209  0.69177875]
True reward weights: [-0.28544797 -0.65668076 -0.05062202  0.69622356]
MAP Policy for current environment:
Information gain 35 demonstrations: 3.360270

Running EBIRL with 36 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3770
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.64951082 -0.63247458  0.30012046 -0.29671418]
True reward weights: [-0.20815848 -0.87368956 -0.37807323  0.22449329]
MAP Policy for current environment:
Information gain 36 demonstrations: 3.501677

Running EBIRL with 37 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3708
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.84899533 -0.35903284  0.38665238 -0.02832465]
True reward weights: [-0.38462799 -0.19361002 -0.90133397 -0.04662131]
MAP Policy for current environment:
Information gain 37 demonstrations: 3.586012

Running EBIRL with 38 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3572
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.74677033 -0.48146406  0.31968121 -0.32913576]
True reward weights: [-0.65523867 -0.63259047 -0.15394796 -0.38313394]
MAP Policy for current environment:
Information gain 38 demonstrations: 3.636522

Running EBIRL with 39 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.83251938 -0.52509057  0.17660359 -0.00159575]
True reward weights: [-0.24067274 -0.38589976 -0.89002669  0.03178827]
MAP Policy for current environment:
Information gain 39 demonstrations: 3.685284

Running EBIRL with 40 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3626
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.79453979 -0.54133297  0.18655186  0.20214733]
True reward weights: [-0.76687877 -0.53782884  0.0522765   0.34627193]
MAP Policy for current environment:
Information gain 40 demonstrations: 3.772086

Saving results to files...
Results saved successfully.

Running experiment 13/19...
Shuffled Demos: [([(0, 3), (1, 0), (1, 1), (6, 2)], 0), ([(0, 3), (1, 0), (1, 0), (1, 3)], 0), ([(0, 1), (5, 0), (0, 2), (0, 1)], 0), ([(0, 3), (1, 0), (1, 0), (1, 1)], 0), ([(0, 0), (0, 0), (0, 1), (5, 0)], 0), ([(0, 2), (0, 1), (5, 0), (0, 3)], 0), ([(0, 1), (5, 1), (10, 1), (15, 1)], 0), ([(0, 0), (0, 1), (5, 0), (0, 2)], 0), ([(0, 2), (0, 1), (5, 3), (6, 2)], 0), ([(0, 1), (5, 0), (0, 0), (0, 1)], 0), ([(0, 1), (5, 0), (0, 3), (1, 0)], 0), ([(0, 1), (5, 1), (10, 2), (10, 1)], 0), ([(0, 3), (1, 3), (2, 2), (1, 0)], 0), ([(0, 1), (5, 3), (6, 0), (1, 2)], 0), ([(0, 0), (0, 2), (0, 0), (0, 3)], 0), ([(0, 3), (1, 1), (6, 1), (11, 1)], 0), ([(0, 3), (1, 2), (0, 2), (0, 0)], 0), ([(0, 2), (0, 2), (0, 0), (0, 3)], 0), ([(0, 2), (0, 3), (1, 1), (6, 2)], 0), ([(0, 0), (0, 3), (1, 1), (6, 1)], 0), ([(0, 2), (0, 0), (0, 1), (5, 3)], 0), ([(0, 0), (0, 2), (0, 1), (5, 0)], 0), ([(0, 0), (0, 0), (0, 2), (0, 0)], 0), ([(0, 1), (5, 0), (0, 1), (5, 2)], 0), ([(0, 0), (0, 2), (0, 2), (0, 2)], 0), ([(0, 1), (5, 1), (10, 3), (11, 3)], 0), ([(0, 2), (0, 1), (5, 2), (5, 3)], 0), ([(0, 3), (1, 3), (2, 3), (3, 2)], 0), ([(0, 3), (1, 1), (6, 1), (11, 0)], 0), ([(0, 1), (5, 3), (6, 1), (11, 1)], 0), ([(0, 0), (0, 0), (0, 2), (0, 2)], 0), ([(0, 1), (5, 3), (6, 2), (5, 3)], 0), ([(0, 2), (0, 1), (5, 1), (10, 1)], 0), ([(0, 1), (5, 1), (10, 3), (11, 0)], 0), ([(0, 0), (0, 2), (0, 1), (5, 1)], 0), ([(0, 2), (0, 1), (5, 2), (5, 1)], 0), ([(0, 3), (1, 2), (0, 0), (0, 3)], 0), ([(0, 0), (0, 0), (0, 1), (5, 2)], 0), ([(0, 2), (0, 3), (1, 1), (6, 2)], 0), ([(0, 2), (0, 1), (5, 2), (5, 3)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.8648604   0.25446718  0.41586706 -0.11965589]
True reward weights: [-0.83842198 -0.21600546  0.03565977  0.49911783]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.138974

Running EBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5390
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.54394754 -0.08561054  0.83219325  0.06516366]
True reward weights: [-0.37785139 -0.01273411  0.08152307  0.92218228]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.142179

Running EBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4184
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.31212098 -0.34489678  0.05703448 -0.88338767]
True reward weights: [-0.6145717  -0.24264729 -0.58123776  0.47495956]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.180605

Running EBIRL with 4 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4084
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.50209608 -0.27555081  0.80592919  0.14983133]
True reward weights: [-0.14323095 -0.23261382 -0.45670847 -0.84663633]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.185666

Running EBIRL with 5 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4132
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.74487978 -0.33600594  0.54716984  0.18127132]
True reward weights: [-0.29729454 -0.54924523  0.69819309  0.34996007]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.260207

Running EBIRL with 6 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4008
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.38325058 -0.7325189  -0.28359379  0.48591111]
True reward weights: [-0.47822317 -0.13094042 -0.8552169  -0.15086835]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.339947

Running EBIRL with 7 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4116
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.39842934 -0.24589974  0.8170193  -0.3365514 ]
True reward weights: [-0.48199175 -0.10187629  0.85199778 -0.17721442]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.341965

Running EBIRL with 8 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.29518821 -0.15820702 -0.47392553 -0.81438876]
True reward weights: [-0.00868599 -0.23556824 -0.52091542  0.82041409]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.424502

Running EBIRL with 9 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4108
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.68880502 -0.5468946   0.29676745  0.37199868]
True reward weights: [-0.23730251 -0.5484962   0.72804551  0.335841  ]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.508947

Running EBIRL with 10 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4074
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.94059816 -0.17036935 -0.2456671  -0.16092562]
True reward weights: [-0.24929365 -0.68667416 -0.01944437  0.68260764]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.593541

Running EBIRL with 11 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4128
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.56054914 -0.36629644 -0.69521677  0.26131441]
True reward weights: [-0.58653423 -0.06711121  0.53418242  0.60508083]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.595393

Running EBIRL with 12 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4084
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.54237718 -0.42753866  0.71691718 -0.09522314]
True reward weights: [-0.86774686 -0.45064941  0.03937097 -0.20586506]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.596357

Running EBIRL with 13 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4062
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.72581925 -0.57635607  0.34525225  0.14765152]
True reward weights: [-0.87355589 -0.21641815  0.40648874  0.15757599]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.597043

Running EBIRL with 14 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.18501198 -0.53878514  0.59979617 -0.56189474]
True reward weights: [-0.5980316  -0.79446943  0.10301258 -0.02376827]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.597948

Running EBIRL with 15 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.4014
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.25096724 -0.28079529 -0.8230271  -0.42520096]
True reward weights: [-0.2020981  -0.46989199 -0.07017437 -0.85640728]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.683552

Running EBIRL with 16 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.45095709 -0.25608563  0.10236138  0.84886984]
True reward weights: [-0.62998648 -0.75321412  0.15898945  0.10250797]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.746639

Running EBIRL with 17 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.75540442 -0.57951771  0.28250193  0.1171155 ]
True reward weights: [-0.78972451 -0.36799108 -0.47151723 -0.13634246]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.747622

Running EBIRL with 18 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3946
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.68369874 -0.57849091 -0.44482389 -0.0059999 ]
True reward weights: [-0.32489813 -0.17579609 -0.84154286 -0.39413521]
MAP Policy for current environment:
Information gain 18 demonstrations: 0.833758

Running EBIRL with 19 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.84658488 -0.52627801  0.0795258  -0.00106696]
True reward weights: [-0.25188871 -0.6830253   0.56762667 -0.3844847 ]
MAP Policy for current environment:
Information gain 19 demonstrations: 0.833515

Running EBIRL with 20 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3892
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.78045176 -0.35360129 -0.08396079 -0.50873546]
True reward weights: [-0.53757922 -0.66619035 -0.44529509  0.2625096 ]
MAP Policy for current environment:
Information gain 20 demonstrations: 0.833412

Running EBIRL with 21 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.86981642 -0.48766566  0.06817715 -0.03087835]
True reward weights: [-0.67767275 -0.70730526  0.01519889  0.20061879]
MAP Policy for current environment:
Information gain 21 demonstrations: 0.918912

Running EBIRL with 22 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.17152812 -0.23225016 -0.21104779 -0.93386123]
True reward weights: [-0.56279994 -0.04973175 -0.7069087  -0.42551506]
MAP Policy for current environment:
Information gain 22 demonstrations: 1.004367

Running EBIRL with 23 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.62757375 -0.18321605  0.12342504  0.74655832]
True reward weights: [-0.63266943 -0.10234772 -0.6395792   0.4244912 ]
MAP Policy for current environment:
Information gain 23 demonstrations: 1.090559

Running EBIRL with 24 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.5147831  -0.83890206  0.17273637  0.03746777]
True reward weights: [-0.81046821 -0.27124963 -0.50825626  0.1060212 ]
MAP Policy for current environment:
Information gain 24 demonstrations: 1.176404

Running EBIRL with 25 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3904
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.11250829 -0.66371335 -0.73099521 -0.11168025]
True reward weights: [-0.21747834 -0.58112282  0.22844038  0.75020959]
MAP Policy for current environment:
Information gain 25 demonstrations: 1.261850

Running EBIRL with 26 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3918
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.52005273 -0.6545123  -0.13616855  0.53161728]
True reward weights: [-0.00485191 -0.35355963 -0.83428976  0.4230043 ]
MAP Policy for current environment:
Information gain 26 demonstrations: 1.263061

Running EBIRL with 27 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.21224763 -0.78768448  0.5598916  -0.14500171]
True reward weights: [-0.82112857 -0.0184539   0.2689771  -0.50304934]
MAP Policy for current environment:
Information gain 27 demonstrations: 1.348680

Running EBIRL with 28 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3946
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.3598868  -0.45258287 -0.36121719 -0.73155477]
True reward weights: [-0.61939114 -0.10309717 -0.62950486  0.45765622]
MAP Policy for current environment:
Information gain 28 demonstrations: 1.348680

Running EBIRL with 29 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.21458434 -0.64510554 -0.64242313 -0.35367348]
True reward weights: [-0.03246518 -0.27986179 -0.43807517 -0.85364719]
MAP Policy for current environment:
Information gain 29 demonstrations: 1.349259

Running EBIRL with 30 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.45722926 -0.64827664 -0.07058331 -0.6047287 ]
True reward weights: [-0.74427841 -0.22933109 -0.17695351  0.60178431]
MAP Policy for current environment:
Information gain 30 demonstrations: 1.412220

Running EBIRL with 31 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3806
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.50296808 -0.13373093 -0.63031245 -0.57606022]
True reward weights: [-0.45009446 -0.39238151  0.15321288 -0.78738653]
MAP Policy for current environment:
Information gain 31 demonstrations: 1.497526

Running EBIRL with 32 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3856
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.28008666 -0.47890606 -0.33761646 -0.76040488]
True reward weights: [-0.56714361 -0.20802675 -0.32100023 -0.72940513]
MAP Policy for current environment:
Information gain 32 demonstrations: 1.583242

Running EBIRL with 33 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3928
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.43896202 -0.73597099 -0.03533521  0.51420858]
True reward weights: [-0.36134214 -0.85426002  0.26754176 -0.26094653]
MAP Policy for current environment:
Information gain 33 demonstrations: 1.582495

Running EBIRL with 34 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3812
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.60240528 -0.16489149 -0.25405866  0.73849365]
True reward weights: [-0.04851993 -0.7981981  -0.59396441  0.08793112]
MAP Policy for current environment:
Information gain 34 demonstrations: 1.581591

Running EBIRL with 35 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3890
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.35970318 -0.87140783  0.24865543 -0.22233419]
True reward weights: [-0.51724624 -0.18783618 -0.79673836 -0.24976367]
MAP Policy for current environment:
Information gain 35 demonstrations: 1.667193

Running EBIRL with 36 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3766
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.45232065 -0.69719293 -0.54179292  0.12565218]
True reward weights: [-0.57319952 -0.49785483 -0.51357711  0.39977672]
MAP Policy for current environment:
Information gain 36 demonstrations: 1.753079

Running EBIRL with 37 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3926
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.58238362 -0.77985114  0.06904491  0.21884769]
True reward weights: [-0.06086413 -0.75384792 -0.30808045  0.5771441 ]
MAP Policy for current environment:
Information gain 37 demonstrations: 1.753331

Running EBIRL with 38 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.15746943 -0.5052081  -0.3537505   0.77125141]
True reward weights: [-0.29717832 -0.12401796 -0.89731433 -0.30188009]
MAP Policy for current environment:
Information gain 38 demonstrations: 1.839284

Running EBIRL with 39 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3866
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.52217874 -0.81355721  0.19454414  0.16615234]
True reward weights: [-0.3921173  -0.15168063 -0.46094922 -0.78151317]
MAP Policy for current environment:
Information gain 39 demonstrations: 1.840292

Running EBIRL with 40 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3826
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.16979345 -0.22171386 -0.55230671 -0.78547466]
True reward weights: [-0.13461374 -0.18912628 -0.95679962  0.17505677]
MAP Policy for current environment:
Information gain 40 demonstrations: 1.925952

Running experiment 14/19...
Shuffled Demos: [([(0, 0), (0, 1), (5, 0), (0, 3)], 1), ([(0, 1), (5, 0), (0, 3), (1, 1)], 3), ([(0, 0), (0, 3), (1, 2), (0, 3)], 3), ([(0, 0), (0, 1), (5, 2), (5, 3)], 0), ([(0, 1), (5, 0), (0, 0), (0, 2)], 0), ([(0, 0), (0, 1), (5, 1), (10, 3)], 1), ([(0, 0), (0, 1), (5, 1), (10, 3)], 1), ([(0, 1), (5, 0), (0, 1), (5, 3)], 0), ([(0, 1), (5, 2), (5, 0), (0, 0)], 0), ([(0, 2), (0, 1), (5, 2), (5, 2)], 0), ([(0, 1), (5, 3), (6, 1), (11, 2)], 3), ([(0, 0), (0, 3), (1, 1), (6, 3)], 3), ([(0, 0), (0, 0), (0, 3), (1, 0)], 3), ([(0, 1), (5, 2), (5, 2), (5, 0)], 0), ([(0, 2), (0, 0), (0, 1), (5, 2)], 2), ([(0, 0), (0, 2), (0, 0), (0, 2)], 3), ([(0, 0), (0, 0), (0, 2), (0, 1)], 3), ([(0, 2), (0, 2), (0, 2), (0, 3)], 3), ([(0, 1), (5, 0), (0, 3), (1, 3)], 3), ([(0, 3), (1, 1), (6, 2), (5, 1)], 2), ([(0, 3), (1, 2), (0, 2), (0, 0)], 3), ([(0, 1), (5, 3), (6, 2), (5, 2)], 2), ([(0, 3), (1, 1), (6, 3), (7, 3)], 3), ([(0, 3), (1, 2), (0, 1), (5, 1)], 2), ([(0, 1), (5, 3), (6, 1), (11, 0)], 3), ([(0, 2), (0, 3), (1, 3), (2, 0)], 3), ([(0, 3), (1, 0), (1, 3), (2, 3)], 3), ([(0, 0), (0, 1), (5, 0), (0, 1)], 1), ([(0, 3), (1, 1), (6, 2), (5, 1)], 2), ([(0, 1), (5, 1), (10, 1), (15, 2)], 0), ([(0, 2), (0, 0), (0, 3), (1, 1)], 3), ([(0, 0), (0, 3), (1, 0), (1, 1)], 3), ([(0, 1), (5, 2), (5, 0), (0, 3)], 0), ([(0, 1), (5, 2), (5, 2), (5, 3)], 0), ([(0, 3), (1, 2), (0, 0), (0, 2)], 3), ([(0, 2), (0, 1), (5, 3), (6, 1)], 3), ([(0, 1), (5, 0), (0, 1), (5, 3)], 0), ([(0, 2), (0, 2), (0, 3), (1, 1)], 3), ([(0, 2), (0, 2), (0, 1), (5, 3)], 2), ([(0, 2), (0, 1), (5, 1), (10, 2)], 1)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5966
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.61691931  0.18368125 -0.02814717  0.76477415]
True reward weights: [-0.27410583  0.06391249  0.48081808  0.83041867]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147779

Running EBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5804
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.05556274 -0.33142145  0.41629786 -0.84484833]
True reward weights: [-0.07487944 -0.34027883  0.88675299  0.30376391]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202566

Running EBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5912
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.73762887 -0.11822267  0.6639901   0.03231414]
True reward weights: [ 0.00071113 -0.47330034 -0.62070058 -0.62507365]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.274809

Running EBIRL with 4 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5624
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.33755968 -0.25291583  0.52305729  0.74060658]
True reward weights: [-0.87599598 -0.14537657 -0.0730821   0.45404372]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.274409

Running EBIRL with 5 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5126
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.6095687  -0.1359097   0.6909087   0.36414244]
True reward weights: [-0.22647297  0.04785431 -0.25595854  0.93856549]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.273364

Running EBIRL with 6 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5120
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.22553726 -0.22563192  0.92381347 -0.21164086]
True reward weights: [-0.14936168 -0.50726888 -0.38598465 -0.75590027]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.273449

Running EBIRL with 7 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5168
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.70008289  0.07335123  0.41483801  0.57655266]
True reward weights: [-0.15680427 -0.68456782  0.56072769  0.43859296]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.273613

Running EBIRL with 8 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5156
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.39275467  0.02259528  0.5693108   0.72188533]
True reward weights: [-0.56806355 -0.73352192 -0.24262878  0.28351485]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.274052

Running EBIRL with 9 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5052
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.27046131 -0.15516558  0.78132547 -0.54065223]
True reward weights: [ 0.15841191 -0.90678611 -0.39002224  0.02296236]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.273981

Running EBIRL with 10 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5132
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.87159094  0.07841544  0.40394374 -0.26647645]
True reward weights: [-0.77468412 -0.46120331 -0.38624088  0.19486921]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.274140

Running EBIRL with 11 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5070
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.67556633 -0.1225921   0.62344291  0.3740324 ]
True reward weights: [-0.34692178  0.09612584  0.31339917  0.87874118]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.352414

Running EBIRL with 12 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5100
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.80171355  0.04147967  0.51314639  0.30367022]
True reward weights: [-0.65450921  0.2271832   0.60343967 -0.39479876]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.434491

Running EBIRL with 13 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5232
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.42318774 -0.22780605  0.8656797   0.14005427]
True reward weights: [-0.33955388 -0.67917308  0.63839635  0.12600473]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.518128

Running EBIRL with 14 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4688
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.16369942 -0.19929028  0.29494684  0.92005013]
True reward weights: [-0.27037836 -0.78848729 -0.35953335  0.41942712]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.516539

Running EBIRL with 15 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4580
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.66913578 -0.12754492  0.69782888  0.22141468]
True reward weights: [-0.22522934 -0.21745911  0.81558606  0.48662373]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.515344

Running EBIRL with 16 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4618
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.42587624  0.04038462  0.1152524  -0.89650175]
True reward weights: [-0.48627478 -0.68942754  0.28526955  0.45480522]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.598337

Running EBIRL with 17 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4568
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.44058024 -0.00092329  0.31814045 -0.83944914]
True reward weights: [-0.11014156  0.0538467  -0.91706216 -0.379429  ]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.681958

Running EBIRL with 18 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4638
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.43674588 -0.05121677  0.33104332 -0.83488933]
True reward weights: [-0.04818827 -0.41470083  0.61761604  0.66652197]
MAP Policy for current environment:
Information gain 18 demonstrations: 0.766335

Running EBIRL with 19 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4658
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.82286665  0.15867496  0.35969886 -0.41027974]
True reward weights: [-0.19106026 -0.03779835  0.91662436  0.34909462]
MAP Policy for current environment:
Information gain 19 demonstrations: 0.851037

Running EBIRL with 20 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4456
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.51082907  0.04980766  0.48531113 -0.70784601]
True reward weights: [-0.66803566  0.19119169  0.07371835  0.71535984]
MAP Policy for current environment:
Information gain 20 demonstrations: 0.871783

Running EBIRL with 21 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4498
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.53141335 -0.07196718  0.47503621 -0.69768272]
True reward weights: [-0.98680685  0.04411792 -0.13271293 -0.08156672]
MAP Policy for current environment:
Information gain 21 demonstrations: 0.956873

Running EBIRL with 22 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4660
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.60050592 -0.07961509  0.54054162  0.58383974]
True reward weights: [-0.48811288 -0.58134545  0.647961    0.06268831]
MAP Policy for current environment:
Information gain 22 demonstrations: 0.956546

Running EBIRL with 23 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4562
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.46005867  0.03358012  0.12527459 -0.87836477]
True reward weights: [-0.55320525 -0.00753239 -0.6001963   0.57764316]
MAP Policy for current environment:
Information gain 23 demonstrations: 1.041592

Running EBIRL with 24 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4622
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.65302034 -0.04842863  0.68767202  0.31357025]
True reward weights: [-0.51805152  0.06982799 -0.55900141  0.64363351]
MAP Policy for current environment:
Information gain 24 demonstrations: 1.041025

Running EBIRL with 25 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4646
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.64969319 -0.05158517  0.46274718 -0.60091828]
True reward weights: [-0.66345743 -0.14305517 -0.67478939 -0.28985985]
MAP Policy for current environment:
Information gain 25 demonstrations: 1.126090

Running EBIRL with 26 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.46724177 -0.06327184  0.36512631  0.80272323]
True reward weights: [-0.59271957 -0.16813049 -0.58809794  0.52398135]
MAP Policy for current environment:
Information gain 26 demonstrations: 1.210898

Running EBIRL with 27 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4580
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.5403673  -0.18315187  0.82125124  0.00222911]
True reward weights: [-0.15505184 -0.8420703  -0.06507372 -0.51248604]
MAP Policy for current environment:
Information gain 27 demonstrations: 1.296055

Running EBIRL with 28 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4556
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.81156796  0.07225644  0.56629509  0.12428327]
True reward weights: [-0.91837938  0.39221096 -0.01815594 -0.04919588]
MAP Policy for current environment:
Information gain 28 demonstrations: 1.295959

Running EBIRL with 29 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4608
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.78417999  0.04468888  0.50393611  0.35932304]
True reward weights: [-0.10235887 -0.78057321 -0.61395988 -0.05728344]
MAP Policy for current environment:
Information gain 29 demonstrations: 1.295703

Running EBIRL with 30 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4552
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.79712192  0.23269292  0.40510098  0.38254915]
True reward weights: [-0.33868358 -0.701733    0.58250875  0.23140396]
MAP Policy for current environment:
Information gain 30 demonstrations: 1.295098

Running EBIRL with 31 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4592
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.80753901  0.07080982  0.04572655  0.58376006]
True reward weights: [-0.50876007 -0.50572661 -0.14304263  0.68186699]
MAP Policy for current environment:
Information gain 31 demonstrations: 1.380366

Running EBIRL with 32 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4590
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.46500987  0.0406805   0.05859353 -0.88242717]
True reward weights: [-0.32649958 -0.03236388 -0.78201801 -0.52990417]
MAP Policy for current environment:
Information gain 32 demonstrations: 1.465607

Running EBIRL with 33 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4578
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.8265843  -0.06676259  0.38622783  0.40389257]
True reward weights: [-0.62822273 -0.62084345 -0.32033116  0.3424581 ]
MAP Policy for current environment:
Information gain 33 demonstrations: 1.465530

Running EBIRL with 34 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4524
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.81524847 -0.05095568 -0.05623085  0.57411806]
True reward weights: [-0.21581445 -0.8561966   0.46263977  0.07947293]
MAP Policy for current environment:
Information gain 34 demonstrations: 1.465324

Running EBIRL with 35 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4710
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.81233553  0.02145705  0.57451105 -0.09791644]
True reward weights: [-0.53850737 -0.0382902  -0.78555676 -0.30239751]
MAP Policy for current environment:
Information gain 35 demonstrations: 1.550823

Running EBIRL with 36 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4554
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.96690647  0.04540784  0.0906706   0.23411292]
True reward weights: [-0.24369781 -0.22131033 -0.55884283 -0.76113586]
MAP Policy for current environment:
Information gain 36 demonstrations: 1.636225

Running EBIRL with 37 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4520
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.45682494 -0.22129921  0.85890012  0.06803105]
True reward weights: [-0.33415813  0.01100867 -0.81608435 -0.47140586]
MAP Policy for current environment:
Information gain 37 demonstrations: 1.635994

Running EBIRL with 38 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4528
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.99486081  0.06408219  0.06549971 -0.04307228]
True reward weights: [-0.85297386 -0.23699982 -0.46088052  0.06209532]
MAP Policy for current environment:
Information gain 38 demonstrations: 1.721365

Running EBIRL with 39 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4554
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.55895785 -0.05798563  0.69650899 -0.44618272]
True reward weights: [-0.68999728 -0.06048339 -0.57138995  0.44018068]
MAP Policy for current environment:
Information gain 39 demonstrations: 1.721586

Running EBIRL with 40 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4536
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.50244801 -0.19210022  0.83137797 -0.13947822]
True reward weights: [-0.81558599 -0.20654067 -0.04542682  0.5386064 ]
MAP Policy for current environment:
Information gain 40 demonstrations: 1.721343

Saving results to files...
Results saved successfully.

Running experiment 15/19...
Shuffled Demos: [([(0, 1), (5, 0), (0, 3), (1, 1)], 0), ([(0, 2), (0, 1), (5, 1), (10, 3)], 0), ([(0, 0), (0, 1), (5, 0), (0, 2)], 0), ([(0, 1), (5, 2), (5, 2), (5, 2)], 0), ([(0, 0), (0, 3), (1, 1), (6, 1)], 0), ([(0, 1), (5, 0), (0, 3), (1, 2)], 0), ([(0, 0), (0, 1), (5, 3), (6, 1)], 0), ([(0, 0), (0, 2), (0, 3), (1, 1)], 0), ([(0, 0), (0, 0), (0, 0), (0, 2)], 0), ([(0, 1), (5, 3), (6, 0), (1, 1)], 0), ([(0, 1), (5, 2), (5, 3), (6, 2)], 0), ([(0, 0), (0, 2), (0, 1), (5, 2)], 0), ([(0, 2), (0, 3), (1, 1), (6, 1)], 0), ([(0, 3), (1, 2), (0, 3), (1, 2)], 0), ([(0, 3), (1, 2), (0, 1), (5, 1)], 0), ([(0, 1), (5, 1), (10, 0), (5, 3)], 0), ([(0, 0), (0, 1), (5, 0), (0, 0)], 0), ([(0, 1), (5, 3), (6, 1), (11, 2)], 0), ([(0, 1), (5, 1), (10, 1), (15, 3)], 0), ([(0, 1), (5, 0), (0, 3), (1, 3)], 0), ([(0, 0), (0, 2), (0, 2), (0, 2)], 0), ([(0, 2), (0, 0), (0, 0), (0, 3)], 0), ([(0, 3), (1, 0), (1, 0), (1, 0)], 0), ([(0, 0), (0, 1), (5, 1), (10, 1)], 0), ([(0, 0), (0, 3), (1, 1), (6, 0)], 0), ([(0, 3), (1, 2), (0, 2), (0, 3)], 0), ([(0, 1), (5, 2), (5, 2), (5, 0)], 0), ([(0, 2), (0, 0), (0, 1), (5, 3)], 0), ([(0, 2), (0, 1), (5, 2), (5, 1)], 0), ([(0, 2), (0, 0), (0, 3), (1, 0)], 0), ([(0, 3), (1, 0), (1, 0), (1, 2)], 0), ([(0, 2), (0, 3), (1, 2), (0, 2)], 0), ([(0, 3), (1, 2), (0, 1), (5, 1)], 0), ([(0, 1), (5, 2), (5, 2), (5, 1)], 0), ([(0, 0), (0, 1), (5, 3), (6, 1)], 0), ([(0, 1), (5, 2), (5, 0), (0, 2)], 0), ([(0, 2), (0, 3), (1, 2), (0, 2)], 0), ([(0, 0), (0, 0), (0, 2), (0, 2)], 0), ([(0, 1), (5, 1), (10, 0), (5, 3)], 0), ([(0, 2), (0, 1), (5, 1), (10, 0)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.477273   -0.43407792 -0.67122108 -0.36503303]
True reward weights: [-0.67868375 -0.62331644 -0.1171628   0.3703213 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.149239

Running EBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5022
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.45556363 -0.12043873 -0.88092357  0.04393137]
True reward weights: [-0.70373455 -0.05016669  0.6622638   0.25228487]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.165979

Running EBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5176
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.54664379  0.3224343  -0.28126577  0.71979599]
True reward weights: [-0.19672043  0.08731635 -0.80178035 -0.55751699]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.169900

Running EBIRL with 4 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.4298
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.25759393 -0.46937901 -0.65289655  0.53577497]
True reward weights: [-0.67922654  0.33751713 -0.10238169 -0.64362372]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.174872

Running EBIRL with 5 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3792
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.68871187 -0.0765906  -0.14528957 -0.7061875 ]
True reward weights: [-0.01578657  0.69319383 -0.67039251 -0.26421011]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.203961

Running EBIRL with 6 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3718
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.17870429 -0.54404844 -0.73783564 -0.35731587]
True reward weights: [-0.16218559 -0.02219909 -0.82976929 -0.5335597 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.198114

Running EBIRL with 7 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3866
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.77745568  0.17360614 -0.51821174 -0.31125578]
True reward weights: [-0.90126872  0.06723367 -0.01824239  0.42762314]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.195428

Running EBIRL with 8 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3790
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.4043333   0.00997552 -0.81626485 -0.41246426]
True reward weights: [-0.82414319 -0.28298994 -0.21088536 -0.44298088]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.186459

Running EBIRL with 9 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3606
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.84249539 -0.1085395  -0.51627391 -0.10900436]
True reward weights: [ 1.09482091e-01 -8.59080355e-01 -4.99994525e-01  3.01061991e-04]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.249100

Running EBIRL with 10 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3586
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.3394314  -0.72950352 -0.55535196  0.21022643]
True reward weights: [-0.41058144  0.09968386 -0.90599463  0.02568553]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.251592

Running EBIRL with 11 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3522
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.68635315 -0.46806016 -0.12467815  0.54248908]
True reward weights: [-0.06052561 -0.79710949 -0.54125516  0.26076038]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.253036

Running EBIRL with 12 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3512
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.31058078 -0.68187932 -0.66124016  0.03662806]
True reward weights: [-0.10890623  0.046739   -0.49147058 -0.86279289]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.255506

Running EBIRL with 13 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3598
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.12876326 -0.12888787 -0.82363631 -0.53705788]
True reward weights: [-0.84318744 -0.06907743 -0.3438353   0.40748071]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.254780

Running EBIRL with 14 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3500
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.22316321 -0.57713198 -0.28954712 -0.73025976]
True reward weights: [-0.42939202 -0.64430894 -0.23680001 -0.58686817]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.256931

Running EBIRL with 15 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3512
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.94582183 -0.12490311 -0.27904863  0.10932585]
True reward weights: [-0.66023302  0.0436778  -0.74975968  0.00671113]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.254469

Running EBIRL with 16 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3568
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.42771928 -0.2080573  -0.5281624  -0.70342935]
True reward weights: [-0.68285905 -0.12528165 -0.3889965   0.60554913]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.256106

Running EBIRL with 17 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3610
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.7634046  -0.40372361 -0.15664739  0.47925177]
True reward weights: [-0.84504628 -0.47663931 -0.23598101  0.05499743]
MAP Policy for current environment:
Information gain 17 demonstrations: 0.257115

Running EBIRL with 18 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.8000049  -0.22925231 -0.399258   -0.38474483]
True reward weights: [-0.19168262  0.05789383 -0.31406785  0.92804497]
MAP Policy for current environment:
Information gain 18 demonstrations: 0.256010

Running EBIRL with 19 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3628
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.28170124 -0.81214637 -0.27392391  0.43131007]
True reward weights: [-0.46016656 -0.49433855 -0.01278145 -0.73736882]
MAP Policy for current environment:
Information gain 19 demonstrations: 0.257545

Running EBIRL with 20 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3520
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.4627613  -0.67688009 -0.31259993  0.47954833]
True reward weights: [-0.7112864  -0.44369843 -0.52380604  0.15109797]
MAP Policy for current environment:
Information gain 20 demonstrations: 0.256492

Running EBIRL with 21 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3516
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.19039907 -0.19855586 -0.95265092  0.12953761]
True reward weights: [-0.3655796  -0.7804877  -0.404127   -0.30638517]
MAP Policy for current environment:
Information gain 21 demonstrations: 0.335786

Running EBIRL with 22 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3594
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.4726547  -0.39898414 -0.54297548  0.56796727]
True reward weights: [-0.32858638  0.14854225 -0.79352791 -0.49018328]
MAP Policy for current environment:
Information gain 22 demonstrations: 0.418706

Running EBIRL with 23 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3394
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.37096874 -0.46105241 -0.39117676 -0.70483587]
True reward weights: [-0.82001659 -0.43034346 -0.13329647 -0.35300048]
MAP Policy for current environment:
Information gain 23 demonstrations: 0.419246

Running EBIRL with 24 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3496
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.60663145 -0.51050922 -0.60914775  0.01782251]
True reward weights: [-0.5759025  -0.55816725 -0.4347925  -0.40956209]
MAP Policy for current environment:
Information gain 24 demonstrations: 0.418988

Running EBIRL with 25 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3500
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.73712893 -0.40848755 -0.53491892  0.06033748]
True reward weights: [-0.19411706 -0.90883468 -0.31969382 -0.18475377]
MAP Policy for current environment:
Information gain 25 demonstrations: 0.418282

Running EBIRL with 26 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3442
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.83857693 -0.22725222 -0.30709084 -0.38838173]
True reward weights: [-0.20740585 -0.30443895 -0.74929424 -0.55032525]
MAP Policy for current environment:
Information gain 26 demonstrations: 0.418016

Running EBIRL with 27 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3416
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.42798139 -0.88948526 -0.1449864   0.06802083]
True reward weights: [-0.67490904 -0.4328703  -0.31390319  0.5085134 ]
MAP Policy for current environment:
Information gain 27 demonstrations: 0.417483

Running EBIRL with 28 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3452
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.37549151 -0.45480471 -0.59558821 -0.54537463]
True reward weights: [-0.29521404 -0.09291896 -0.48360927 -0.81874099]
MAP Policy for current environment:
Information gain 28 demonstrations: 0.417084

Running EBIRL with 29 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3480
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.49939431 -0.72551336 -0.4730825   0.02070317]
True reward weights: [-0.35983562 -0.35920328 -0.71357063 -0.48198369]
MAP Policy for current environment:
Information gain 29 demonstrations: 0.416850

Running EBIRL with 30 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3414
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.24554646 -0.7419543  -0.62350618  0.02123182]
True reward weights: [-0.92151353 -0.09058702 -0.34102183  0.16220642]
MAP Policy for current environment:
Information gain 30 demonstrations: 0.416921

Running EBIRL with 31 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3458
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.27733651 -0.33850573 -0.69356768  0.57224314]
True reward weights: [-0.84447338 -0.11370712 -0.50173769  0.14897886]
MAP Policy for current environment:
Information gain 31 demonstrations: 0.416553

Running EBIRL with 32 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3490
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.22125936 -0.59984627 -0.12168301 -0.7592246 ]
True reward weights: [-0.12748861 -0.5924943  -0.49175883  0.6251963 ]
MAP Policy for current environment:
Information gain 32 demonstrations: 0.416669

Running EBIRL with 33 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3434
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.3672604  -0.16563551 -0.85148513 -0.33564529]
True reward weights: [-0.43681223 -0.66511792 -0.57933272 -0.17659791]
MAP Policy for current environment:
Information gain 33 demonstrations: 0.416744

Running EBIRL with 34 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3408
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.12028719 -0.7604917  -0.17482002 -0.61369482]
True reward weights: [-0.65203559 -0.74703354 -0.02630172  0.12688065]
MAP Policy for current environment:
Information gain 34 demonstrations: 0.416727

Running EBIRL with 35 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3454
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.785787   -0.60159682 -0.14357484  0.0025151 ]
True reward weights: [-0.26404244 -0.8710617  -0.14011792  0.3897436 ]
MAP Policy for current environment:
Information gain 35 demonstrations: 0.416468

Running EBIRL with 36 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3428
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.40006065 -0.13484521 -0.81094276 -0.40514205]
True reward weights: [-0.13480663 -0.44652717 -0.12089661 -0.87625605]
MAP Policy for current environment:
Information gain 36 demonstrations: 0.416447

Running EBIRL with 37 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3322
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.65583766 -0.30977577 -0.27637855 -0.63050046]
True reward weights: [-0.32855403 -0.23232794 -0.72393123  0.56035663]
MAP Policy for current environment:
Information gain 37 demonstrations: 0.416869

Running EBIRL with 38 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3390
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.60950701 -0.61848446 -0.49377362  0.04653802]
True reward weights: [-0.21424047 -0.33000593 -0.35524562 -0.8479373 ]
MAP Policy for current environment:
Information gain 38 demonstrations: 0.500061

Running EBIRL with 39 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3376
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.74055661 -0.54520563 -0.31981461 -0.22813448]
True reward weights: [-0.79321915 -0.05141424 -0.26097263  0.54777116]
MAP Policy for current environment:
Information gain 39 demonstrations: 0.499620

Running EBIRL with 40 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3398
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.34581158 -0.84521899 -0.30125144  0.27434792]
True reward weights: [-0.11913268 -0.04910917 -0.91446494 -0.3836008 ]
MAP Policy for current environment:
Information gain 40 demonstrations: 0.499562

Running experiment 16/19...
Shuffled Demos: [([(0, 1), (5, 3), (6, 3), (7, 3)], 0), ([(0, 2), (0, 2), (0, 1), (5, 2)], 3), ([(0, 2), (0, 1), (5, 0), (0, 1)], 3), ([(0, 1), (5, 3), (6, 3), (7, 2)], 0), ([(0, 2), (0, 2), (0, 2), (0, 3)], 3), ([(0, 1), (5, 1), (10, 1), (15, 2)], 0), ([(0, 1), (5, 0), (0, 0), (0, 2)], 3), ([(0, 3), (1, 3), (2, 2), (1, 3)], 3), ([(0, 0), (0, 2), (0, 3), (1, 1)], 3), ([(0, 3), (1, 3), (2, 3), (3, 0)], 3), ([(0, 3), (1, 3), (2, 1), (7, 1)], 0), ([(0, 0), (0, 2), (0, 0), (0, 2)], 3), ([(0, 2), (0, 0), (0, 3), (1, 0)], 3), ([(0, 2), (0, 0), (0, 3), (1, 2)], 3), ([(0, 1), (5, 3), (6, 2), (5, 1)], 0), ([(0, 0), (0, 3), (1, 1), (6, 3)], 0), ([(0, 1), (5, 2), (5, 0), (0, 3)], 3), ([(0, 0), (0, 0), (0, 1), (5, 0)], 3), ([(0, 0), (0, 2), (0, 2), (0, 2)], 3), ([(0, 3), (1, 1), (6, 2), (5, 2)], 0), ([(0, 2), (0, 3), (1, 0), (1, 2)], 3), ([(0, 3), (1, 2), (0, 3), (1, 1)], 3), ([(0, 1), (5, 2), (5, 1), (10, 2)], 3), ([(0, 1), (5, 1), (10, 0), (5, 3)], 3), ([(0, 0), (0, 1), (5, 2), (5, 3)], 3), ([(0, 1), (5, 3), (6, 2), (5, 1)], 0), ([(0, 1), (5, 1), (10, 2), (10, 2)], 3), ([(0, 3), (1, 0), (1, 2), (0, 2)], 3), ([(0, 2), (0, 1), (5, 2), (5, 2)], 3), ([(0, 3), (1, 1), (6, 1), (11, 0)], 0), ([(0, 3), (1, 2), (0, 1), (5, 1)], 3), ([(0, 1), (5, 3), (6, 3), (7, 2)], 0), ([(0, 3), (1, 1), (6, 1), (11, 0)], 0), ([(0, 0), (0, 0), (0, 2), (0, 3)], 3), ([(0, 3), (1, 2), (0, 0), (0, 2)], 3), ([(0, 2), (0, 2), (0, 1), (5, 1)], 3), ([(0, 2), (0, 3), (1, 1), (6, 2)], 0), ([(0, 2), (0, 1), (5, 1), (10, 2)], 3), ([(0, 3), (1, 1), (6, 0), (1, 2)], 0), ([(0, 3), (1, 1), (6, 2), (5, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5904
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.28291358 -0.24755329  0.03888136 -0.92583234]
True reward weights: [-0.92679793  0.00735039  0.0581375   0.37096037]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.130895

Running EBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.19618401  0.45459121 -0.09016712 -0.86413457]
True reward weights: [-0.04128997 -0.52165458 -0.54858139 -0.6520967 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173165

Running EBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5926
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.3548791   0.8745176  -0.06584983  0.32394998]
True reward weights: [-0.46487316 -0.44633325 -0.4857811   0.59050512]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.236158

Running EBIRL with 4 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.56143112  0.73174985  0.36184792 -0.13565886]
True reward weights: [-0.72787486  0.45541918  0.05807967 -0.50933124]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.236032

Running EBIRL with 5 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5998
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.32524438  0.63962923 -0.32737661 -0.61474799]
True reward weights: [-0.41413393  0.84443447  0.23997213  0.24049301]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.310928

Running EBIRL with 6 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5180
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.33230477  0.38844922 -0.52825292  0.67795988]
True reward weights: [-0.44912688  0.59823492 -0.66170128  0.05051172]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.318299

Running EBIRL with 7 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.94544874  0.14521716  0.19804219 -0.21405129]
True reward weights: [-0.08852285 -0.10170825 -0.96954762  0.20444206]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.401715

Running EBIRL with 8 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.44215227  0.61168829 -0.55795028 -0.34500767]
True reward weights: [-0.64903619 -0.44351263 -0.57493369  0.22693571]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.486912

Running EBIRL with 9 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5154
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.82276811  0.4773202  -0.15645381  0.26597042]
True reward weights: [-0.72156289  0.198424   -0.40261494 -0.52713956]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.573096

Running EBIRL with 10 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.90808283  0.41188009  0.04186597 -0.06314749]
True reward weights: [-0.54641255  0.39814407 -0.40336583  0.61661222]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.659384

Running EBIRL with 11 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4762
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.9932932   0.07217709  0.04951794  0.07554508]
True reward weights: [-0.79384548 -0.54911756  0.25999339 -0.02612853]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.667820

Running EBIRL with 12 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4860
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.69031164  0.48791591 -0.18750199 -0.50025085]
True reward weights: [-0.30678444 -0.35952004 -0.76390542 -0.43940545]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.752663

Running EBIRL with 13 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4774
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.47100224  0.63026533 -0.61345005  0.06783469]
True reward weights: [-0.13575563 -0.45853461 -0.87798662  0.02135207]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.837213

Running EBIRL with 14 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4660
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.25979926  0.43906225 -0.68107869 -0.52522425]
True reward weights: [-0.55221548 -0.52157021  0.01875413  0.65013142]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.922405

Running EBIRL with 15 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4884
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.98072374 -0.0660358  -0.07966977 -0.16574965]
True reward weights: [-0.56380364  0.03156056  0.15009186 -0.81154287]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.921821

Running EBIRL with 16 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4702
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.64431993  0.3185851  -0.66277017  0.20997871]
True reward weights: [-0.84654111 -0.14180819 -0.48771129 -0.15936211]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.921455

Running EBIRL with 17 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4782
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.64017382  0.58274209 -0.47146657  0.16825101]
True reward weights: [-0.15149272  0.39776972 -0.58317952  0.69190379]
MAP Policy for current environment:
Information gain 17 demonstrations: 1.007030

Running EBIRL with 18 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4694
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.62963418  0.0516455   0.04249597  0.77400751]
True reward weights: [-0.46273077  0.6711302  -0.57392726  0.07792293]
MAP Policy for current environment:
Information gain 18 demonstrations: 1.092110

Running EBIRL with 19 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4804
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.32242901  0.63864673 -0.67911211 -0.16424563]
True reward weights: [-0.90550905  0.10924313 -0.25197769 -0.32346027]
MAP Policy for current environment:
Information gain 19 demonstrations: 1.177770

Running EBIRL with 20 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4740
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.92969567  0.31965137 -0.17808551  0.04212497]
True reward weights: [-0.76291678 -0.52488481  0.2072182  -0.31545927]
MAP Policy for current environment:
Information gain 20 demonstrations: 1.177768

Running EBIRL with 21 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.3777832   0.64373258 -0.65809533 -0.09898865]
True reward weights: [ 0.22825853  0.01731571 -0.49430959 -0.83860375]
MAP Policy for current environment:
Information gain 21 demonstrations: 1.263093

Running EBIRL with 22 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4872
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.36387888 -0.0488935  -0.07807639  0.92687953]
True reward weights: [-0.75439936  0.52226142 -0.34929662  0.1900434 ]
MAP Policy for current environment:
Information gain 22 demonstrations: 1.348385

Running EBIRL with 23 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4684
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.60239619  0.07513796  0.03362397 -0.79394115]
True reward weights: [-0.01668505 -0.10952969 -0.98608114 -0.12397117]
MAP Policy for current environment:
Information gain 23 demonstrations: 1.434020

Running EBIRL with 24 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4860
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.74131654  0.05959323  0.06519554 -0.66531795]
True reward weights: [-0.09242279  0.12373433 -0.96100247 -0.22939507]
MAP Policy for current environment:
Information gain 24 demonstrations: 1.519462

Running EBIRL with 25 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4732
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.58945681  0.52605028 -0.29969651  0.53478386]
True reward weights: [ 0.32989009 -0.13084907 -0.86980918 -0.34275798]
MAP Policy for current environment:
Information gain 25 demonstrations: 1.604885

Running EBIRL with 26 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.35703402  0.64940152 -0.67140496 -0.00444468]
True reward weights: [-0.71941407 -0.1563096  -0.53843314 -0.41000055]
MAP Policy for current environment:
Information gain 26 demonstrations: 1.604659

Running EBIRL with 27 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4712
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.63065327  0.51475292 -0.26436547 -0.51712357]
True reward weights: [-0.56976735 -0.56590606 -0.49406342  0.33319188]
MAP Policy for current environment:
Information gain 27 demonstrations: 1.690400

Running EBIRL with 28 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4796
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.57933756  0.60128015 -0.5296115   0.14947182]
True reward weights: [-0.69184307  0.37421701 -0.03158965 -0.61669838]
MAP Policy for current environment:
Information gain 28 demonstrations: 1.775911

Running EBIRL with 29 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4632
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.585988    0.51736725 -0.27627282 -0.55912657]
True reward weights: [-0.73175995 -0.65306586  0.09844353 -0.16834855]
MAP Policy for current environment:
Information gain 29 demonstrations: 1.861405

Running EBIRL with 30 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4872
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.72067573  0.53891496 -0.34304489  0.26929047]
True reward weights: [-0.06806688 -0.82090647 -0.48813105  0.28845718]
MAP Policy for current environment:
Information gain 30 demonstrations: 1.861099

Running EBIRL with 31 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4756
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.68642428  0.07022551  0.0374253   0.72283431]
True reward weights: [-0.92120715 -0.04032199  0.29848837 -0.24628484]
MAP Policy for current environment:
Information gain 31 demonstrations: 1.946555

Running EBIRL with 32 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4798
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.79317533  0.48143358 -0.16696337 -0.33349338]
True reward weights: [-0.73212808 -0.4991742   0.20639062  0.41498976]
MAP Policy for current environment:
Information gain 32 demonstrations: 1.946492

Running EBIRL with 33 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4678
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.89549191  0.04338542  0.0514606  -0.43995881]
True reward weights: [ 0.18618849 -0.79061962 -0.37056155 -0.45048707]
MAP Policy for current environment:
Information gain 33 demonstrations: 1.946424

Running EBIRL with 34 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4610
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.74885889  0.54869329 -0.37167462 -0.00200195]
True reward weights: [-0.39338341  0.38832366 -0.5683736  -0.60943062]
MAP Policy for current environment:
Information gain 34 demonstrations: 2.032132

Running EBIRL with 35 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4620
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.72927681 -0.08089601 -0.06679341  0.67612854]
True reward weights: [-0.52191483  0.68621665 -0.49306011  0.11663338]
MAP Policy for current environment:
Information gain 35 demonstrations: 2.117791

Running EBIRL with 36 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4778
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.99307703  0.03951969  0.04764789  0.09982927]
True reward weights: [ 0.02026592 -0.39508809 -0.73701816 -0.54799537]
MAP Policy for current environment:
Information gain 36 demonstrations: 2.203374

Running EBIRL with 37 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4728
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.59142252  0.55807777 -0.39648666  0.42610672]
True reward weights: [-0.26981394 -0.24992197 -0.23156533  0.90062031]
MAP Policy for current environment:
Information gain 37 demonstrations: 2.203387

Running EBIRL with 38 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4760
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.9963751   0.04430212  0.05241944 -0.05026111]
True reward weights: [-0.67423611  0.54864466 -0.44516487  0.21499521]
MAP Policy for current environment:
Information gain 38 demonstrations: 2.289003

Running EBIRL with 39 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4770
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.7914448   0.50146155 -0.22955707 -0.26354316]
True reward weights: [-0.99083423  0.03664943 -0.09696901 -0.0866104 ]
MAP Policy for current environment:
Information gain 39 demonstrations: 2.288954

Running EBIRL with 40 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.58662177 -0.0497425  -0.05799337 -0.80624894]
True reward weights: [-0.14540898 -0.34667391 -0.31865062  0.87013517]
MAP Policy for current environment:
Information gain 40 demonstrations: 2.288969

Saving results to files...
Results saved successfully.

Running experiment 17/19...
Shuffled Demos: [([(0, 3), (1, 0), (1, 2), (0, 1)], 3), ([(0, 1), (5, 3), (6, 1), (11, 2)], 3), ([(0, 2), (0, 0), (0, 2), (0, 0)], 3), ([(0, 3), (1, 2), (0, 2), (0, 3)], 3), ([(0, 1), (5, 3), (6, 0), (1, 1)], 3), ([(0, 1), (5, 2), (5, 2), (5, 1)], 3), ([(0, 0), (0, 2), (0, 2), (0, 2)], 3), ([(0, 1), (5, 1), (10, 0), (5, 0)], 3), ([(0, 3), (1, 1), (6, 1), (11, 1)], 3), ([(0, 0), (0, 2), (0, 1), (5, 3)], 3), ([(0, 0), (0, 3), (1, 2), (0, 2)], 3), ([(0, 0), (0, 1), (5, 0), (0, 2)], 3), ([(0, 2), (0, 1), (5, 0), (0, 0)], 3), ([(0, 0), (0, 2), (0, 2), (0, 3)], 3), ([(0, 3), (1, 0), (1, 3), (2, 0)], 3), ([(0, 0), (0, 2), (0, 2), (0, 3)], 3), ([(0, 0), (0, 0), (0, 2), (0, 3)], 3), ([(0, 3), (1, 3), (2, 1), (7, 0)], 3), ([(0, 2), (0, 2), (0, 0), (0, 3)], 3), ([(0, 1), (5, 2), (5, 1), (10, 2)], 3), ([(0, 3), (1, 1), (6, 2), (5, 3)], 3), ([(0, 1), (5, 2), (5, 1), (10, 0)], 3), ([(0, 2), (0, 2), (0, 2), (0, 2)], 3), ([(0, 3), (1, 2), (0, 3), (1, 2)], 3), ([(0, 0), (0, 1), (5, 2), (5, 1)], 3), ([(0, 1), (5, 3), (6, 3), (7, 0)], 3), ([(0, 0), (0, 2), (0, 1), (5, 3)], 3), ([(0, 1), (5, 1), (10, 1), (15, 2)], 3), ([(0, 0), (0, 1), (5, 3), (6, 3)], 3), ([(0, 0), (0, 0), (0, 1), (5, 2)], 3), ([(0, 1), (5, 1), (10, 3), (11, 0)], 3), ([(0, 3), (1, 0), (1, 2), (0, 0)], 3), ([(0, 0), (0, 0), (0, 1), (5, 0)], 3), ([(0, 2), (0, 0), (0, 3), (1, 3)], 3), ([(0, 3), (1, 2), (0, 0), (0, 1)], 3), ([(0, 2), (0, 2), (0, 3), (1, 0)], 3), ([(0, 3), (1, 0), (1, 3), (2, 3)], 3), ([(0, 3), (1, 1), (6, 3), (7, 0)], 3), ([(0, 1), (5, 0), (0, 0), (0, 0)], 3), ([(0, 3), (1, 0), (1, 1), (6, 2)], 3)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.00151618  0.47473885  0.80311503 -0.36003746]
True reward weights: [-0.34813921  0.02418599  0.41049361  0.84244235]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running EBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.19167005  0.37902105  0.89907152 -0.10618868]
True reward weights: [-0.63075149 -0.54744627  0.41714984  0.35838687]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running EBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.22832306 -0.53721159  0.63971598 -0.50003575]
True reward weights: [-0.22183296 -0.33702552 -0.69507128  0.5950461 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running EBIRL with 4 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.20066169  0.02622711  0.84248193  0.49927069]
True reward weights: [0.40105019 0.44061349 0.50633904 0.62340939]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.350599

Running EBIRL with 5 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.4986092  -0.01957312  0.85848958  0.11832753]
True reward weights: [-0.60923547  0.62360946  0.1356969   0.47066945]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.432452

Running EBIRL with 6 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.04000456  0.22657282  0.77746851 -0.5853265 ]
True reward weights: [ 0.88343937  0.33918682 -0.19946713  0.25436203]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.516237

Running EBIRL with 7 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [ 0.02729224 -0.63468267  0.63954998 -0.43290745]
True reward weights: [-0.72766725  0.08235887  0.02553041  0.68048923]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.600989

Running EBIRL with 8 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.17205207  0.45569184  0.69982348 -0.52248457]
True reward weights: [ 0.59363241 -0.29834458  0.02120829  0.74708854]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.686223

Running EBIRL with 9 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [ 0.71377305 -0.24519298  0.63900456  0.14859878]
True reward weights: [0.15036788 0.35058593 0.67097793 0.63582044]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.771699

Running EBIRL with 10 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.11793344  0.27925784  0.7597065   0.57528497]
True reward weights: [-0.03511498 -0.36434442  0.77557473 -0.51429945]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.857296

Running EBIRL with 11 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [0.56782431 0.1467641  0.80325252 0.10402517]
True reward weights: [-0.25622321 -0.74594692 -0.58761886  0.18060159]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.942953

Running EBIRL with 12 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [ 0.22240845 -0.67177775  0.63973935 -0.29997117]
True reward weights: [ 0.27291118  0.61576714 -0.50219141 -0.54235977]
MAP Policy for current environment:
Information gain 12 demonstrations: 1.028641

Running EBIRL with 13 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.55143784 -0.48240883  0.63849899 -0.23562058]
True reward weights: [-0.21760015 -0.42615839  0.81091196 -0.33683971]
MAP Policy for current environment:
Information gain 13 demonstrations: 1.114343

Running EBIRL with 14 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.53786813 -0.5504209   0.63846175  0.01006476]
True reward weights: [ 0.11266977 -0.41233192 -0.46535139 -0.77507161]
MAP Policy for current environment:
Information gain 14 demonstrations: 1.200053

Running EBIRL with 15 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [ 0.46506616 -0.61034326  0.63997953  0.04025882]
True reward weights: [-0.08111011  0.42629596 -0.51644551 -0.73822554]
MAP Policy for current environment:
Information gain 15 demonstrations: 1.285767

Running EBIRL with 16 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [ 0.52542546 -0.56167736  0.63866323 -0.02357764]
True reward weights: [ 0.28445574 -0.48212262  0.8088006  -0.18023403]
MAP Policy for current environment:
Information gain 16 demonstrations: 1.371482

Running EBIRL with 17 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.43264033 -0.63117858  0.63871024  0.08053057]
True reward weights: [-0.47060165 -0.18936547 -0.2502114   0.824663  ]
MAP Policy for current environment:
Information gain 17 demonstrations: 1.457199

Running EBIRL with 18 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.15117692 -0.71961182  0.63964542 -0.22396006]
True reward weights: [ 0.3276104  -0.82343625  0.10572414 -0.45105052]
MAP Policy for current environment:
Information gain 18 demonstrations: 1.542916

Running EBIRL with 19 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.56949058  0.20686155  0.63881021 -0.47414164]
True reward weights: [ 0.50953905  0.31525189 -0.40014641 -0.69344723]
MAP Policy for current environment:
Information gain 19 demonstrations: 1.628633

Running EBIRL with 20 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.75795655  0.11090392  0.63948496 -0.0652777 ]
True reward weights: [ 0.61821046  0.39288822  0.54651019 -0.40593262]
MAP Policy for current environment:
Information gain 20 demonstrations: 1.714351

Running EBIRL with 21 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.73194856 -0.0208788   0.63925527 -0.23487888]
True reward weights: [ 0.26599835 -0.94999955  0.14274671 -0.07980672]
MAP Policy for current environment:
Information gain 21 demonstrations: 1.800068

Running EBIRL with 22 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.67929558  0.34939951  0.63969737 -0.08523363]
True reward weights: [-0.91969044 -0.06123754  0.38731855 -0.02009463]
MAP Policy for current environment:
Information gain 22 demonstrations: 1.885786

Running EBIRL with 23 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.29088586  0.64983154  0.63870247  0.29182794]
True reward weights: [ 0.7421503   0.45240626  0.13250249 -0.47642901]
MAP Policy for current environment:
Information gain 23 demonstrations: 1.971503

Running EBIRL with 24 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.23150913  0.65036598  0.63839369 -0.34041314]
True reward weights: [-0.44304264 -0.60995654 -0.65671111 -0.01991886]
MAP Policy for current environment:
Information gain 24 demonstrations: 2.057221

Running EBIRL with 25 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.2460173  -0.14123684  0.639851    0.71422569]
True reward weights: [ 0.27113412  0.89626308 -0.30840921  0.16757847]
MAP Policy for current environment:
Information gain 25 demonstrations: 2.142938

Running EBIRL with 26 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [ 0.61416078 -0.65242011  0.31879641  0.30906856]
True reward weights: [-0.89159947 -0.22172309 -0.23553076 -0.31688248]
MAP Policy for current environment:
Information gain 26 demonstrations: 2.228656

Running EBIRL with 27 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [ 0.52650796 -0.28678774  0.63986389  0.48074564]
True reward weights: [ 0.04324739 -0.88756213 -0.35246044 -0.29348724]
MAP Policy for current environment:
Information gain 27 demonstrations: 2.314374

Running EBIRL with 28 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.30570686 -0.64725067  0.63852183 -0.28266545]
True reward weights: [ 0.29076592 -0.87284098 -0.30707473  0.2435342 ]
MAP Policy for current environment:
Information gain 28 demonstrations: 2.400091

Running EBIRL with 29 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [0.41953239 0.59033419 0.65622076 0.21183116]
True reward weights: [ 0.07045161  0.94180387 -0.25431373 -0.2082464 ]
MAP Policy for current environment:
Information gain 29 demonstrations: 2.485809

Running EBIRL with 30 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.74682072 -0.12400382  0.63988277 -0.13202993]
True reward weights: [-0.19195781  0.96779309 -0.07027856  0.14693419]
MAP Policy for current environment:
Information gain 30 demonstrations: 2.571526

Running EBIRL with 31 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.17551494 -0.49066527  0.63953409 -0.56518869]
True reward weights: [ 0.80217433  0.5660365   0.18026978 -0.06018171]
MAP Policy for current environment:
Information gain 31 demonstrations: 2.657244

Running EBIRL with 32 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.3676835  -0.54499029  0.6386014  -0.39997835]
True reward weights: [ 0.40556627 -0.0824224  -0.72372697  0.55221537]
MAP Policy for current environment:
Information gain 32 demonstrations: 2.742961

Running EBIRL with 33 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.15127413 -0.698756    0.63949916 -0.28266062]
True reward weights: [-0.14209518  0.00695913 -0.70761433 -0.69212895]
MAP Policy for current environment:
Information gain 33 demonstrations: 2.828679

Running EBIRL with 34 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.039731   -0.27560607  0.63973813  0.7163783 ]
True reward weights: [ 0.0722973   0.82773053  0.02064377 -0.55606574]
MAP Policy for current environment:
Information gain 34 demonstrations: 2.914396

Running EBIRL with 35 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [0.15852541 0.65326622 0.63933552 0.37331357]
True reward weights: [-0.13123109  0.3353036  -0.39906037  0.84326788]
MAP Policy for current environment:
Information gain 35 demonstrations: 3.000114

Running EBIRL with 36 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.50536171  0.57926732  0.63859412 -0.03544671]
True reward weights: [ 0.18482714  0.83339164 -0.17407475  0.4909127 ]
MAP Policy for current environment:
Information gain 36 demonstrations: 3.085831

Running EBIRL with 37 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [ 0.42745303 -0.31682913  0.63957626  0.5548382 ]
True reward weights: [ 0.20684385  0.86320464 -0.43584242 -0.14877755]
MAP Policy for current environment:
Information gain 37 demonstrations: 3.171549

Running EBIRL with 38 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.28837629  0.70388428  0.63991381  0.10907036]
True reward weights: [ 0.21522408  0.54696027  0.7919033  -0.16553618]
MAP Policy for current environment:
Information gain 38 demonstrations: 3.257266

Running EBIRL with 39 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [ 0.28299513 -0.26097629  0.63982855  0.66515003]
True reward weights: [0.70943681 0.60742859 0.3155062  0.16788616]
MAP Policy for current environment:
Information gain 39 demonstrations: 3.342984

Running EBIRL with 40 demonstrations for experiment 17
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.16541733 -0.69935285  0.63842826  0.27559401]
True reward weights: [-0.14026794 -0.38446568 -0.34743109  0.843684  ]
MAP Policy for current environment:
Information gain 40 demonstrations: 3.428702

Running experiment 18/19...
Shuffled Demos: [([(0, 3), (1, 1), (6, 3), (7, 2)], 0), ([(0, 0), (0, 3), (1, 2), (0, 3)], 0), ([(0, 0), (0, 3), (1, 3), (2, 1)], 0), ([(0, 1), (5, 0), (0, 0), (0, 3)], 0), ([(0, 1), (5, 3), (6, 3), (7, 1)], 3), ([(0, 3), (1, 2), (0, 0), (0, 0)], 0), ([(0, 0), (0, 0), (0, 3), (1, 3)], 0), ([(0, 0), (0, 2), (0, 2), (0, 1)], 0), ([(0, 1), (5, 3), (6, 0), (1, 3)], 1), ([(0, 2), (0, 2), (0, 3), (1, 2)], 0), ([(0, 2), (0, 2), (0, 2), (0, 2)], 0), ([(0, 1), (5, 0), (0, 1), (5, 0)], 1), ([(0, 3), (1, 3), (2, 1), (7, 0)], 0), ([(0, 2), (0, 1), (5, 1), (10, 2)], 0), ([(0, 2), (0, 3), (1, 2), (0, 0)], 0), ([(0, 2), (0, 3), (1, 0), (1, 2)], 0), ([(0, 3), (1, 3), (2, 1), (7, 1)], 0), ([(0, 2), (0, 2), (0, 0), (0, 1)], 0), ([(0, 0), (0, 3), (1, 2), (0, 1)], 0), ([(0, 0), (0, 3), (1, 2), (0, 2)], 0), ([(0, 1), (5, 2), (5, 1), (10, 2)], 3), ([(0, 0), (0, 2), (0, 3), (1, 0)], 0), ([(0, 1), (5, 2), (5, 2), (5, 1)], 3), ([(0, 3), (1, 0), (1, 3), (2, 2)], 0), ([(0, 3), (1, 2), (0, 1), (5, 1)], 0), ([(0, 3), (1, 0), (1, 2), (0, 0)], 0), ([(0, 0), (0, 3), (1, 0), (1, 2)], 0), ([(0, 0), (0, 0), (0, 3), (1, 1)], 0), ([(0, 3), (1, 3), (2, 1), (7, 0)], 0), ([(0, 0), (0, 2), (0, 1), (5, 2)], 0), ([(0, 2), (0, 3), (1, 3), (2, 2)], 0), ([(0, 2), (0, 1), (5, 2), (5, 0)], 0), ([(0, 3), (1, 0), (1, 3), (2, 1)], 0), ([(0, 3), (1, 2), (0, 0), (0, 1)], 0), ([(0, 1), (5, 0), (0, 3), (1, 0)], 0), ([(0, 0), (0, 0), (0, 1), (5, 0)], 0), ([(0, 1), (5, 0), (0, 1), (5, 0)], 1), ([(0, 0), (0, 1), (5, 0), (0, 2)], 0), ([(0, 0), (0, 3), (1, 0), (1, 3)], 0), ([(0, 3), (1, 1), (6, 3), (7, 1)], 0)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.5960
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.28421003 -0.65399796 -0.57071503 -0.40718016]
True reward weights: [-0.70806601  0.20943992  0.21689523  0.63854045]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.137473

Running EBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4488
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.45985881  0.34505835 -0.40663511  0.71000881]
True reward weights: [ 0.26690588  0.82299851 -0.14394286 -0.48032817]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.198629

Running EBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4404
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.60265289  0.69659223  0.07790922  0.38144321]
True reward weights: [-0.55524582 -0.04328998  0.14408078 -0.81796625]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.296197

Running EBIRL with 4 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4394
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.79489872 -0.56779334  0.18505602 -0.1072428 ]
True reward weights: [-0.55410486  0.17268537  0.19759225  0.79000308]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.308038

Running EBIRL with 5 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4282
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.59683975  0.10122987  0.09994694  0.78964893]
True reward weights: [-0.88444115 -0.25744174  0.23095597 -0.31328413]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.395037

Running EBIRL with 6 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4386
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.6491897   0.70615713  0.09993336 -0.26440151]
True reward weights: [-0.83805258 -0.1597987  -0.24538784 -0.4603445 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.481029

Running EBIRL with 7 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4266
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.81036007  0.36257379  0.09415873  0.45054517]
True reward weights: [-0.95942408  0.19357746  0.0900785  -0.18417129]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.569530

Running EBIRL with 8 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4366
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.80505692  0.59319375  0.00183263 -0.00108166]
True reward weights: [-0.43188268  0.34071563 -0.05657493 -0.83318035]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.657048

Running EBIRL with 9 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4268
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.60891828  0.70615647 -0.14940132 -0.3289997 ]
True reward weights: [-0.1455608  0.2903611 -0.5642219  0.7590495]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.799597

Running EBIRL with 10 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4408
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.50197225  0.58715305  0.03828188 -0.63388458]
True reward weights: [-0.84125442  0.31417189 -0.01990881  0.43953459]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.887657

Running EBIRL with 11 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4358
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.94448565  0.27009699  0.18646959  0.01495235]
True reward weights: [-0.33364629 -0.1870224  -0.91074598  0.15570657]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.972912

Running EBIRL with 12 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4124
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.80538298  0.38801627  0.10156707  0.43644674]
True reward weights: [-0.94596022  0.158214   -0.25387389 -0.12520237]
MAP Policy for current environment:
Information gain 12 demonstrations: 1.447256

Running EBIRL with 13 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4086
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.82632386  0.17315868  0.18538334  0.50282996]
True reward weights: [-0.55270799 -0.07841847 -0.82770173 -0.05722122]
MAP Policy for current environment:
Information gain 13 demonstrations: 1.445187

Running EBIRL with 14 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4120
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.92346932  0.301712    0.17131806 -0.16378161]
True reward weights: [-0.98487502  0.0258739   0.0525023   0.1630805 ]
MAP Policy for current environment:
Information gain 14 demonstrations: 1.440525

Running EBIRL with 15 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4102
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.92350509  0.20088254  0.2680564  -0.18689653]
True reward weights: [-0.37099979 -0.70743598 -0.06976146  0.59751722]
MAP Policy for current environment:
Information gain 15 demonstrations: 1.525287

Running EBIRL with 16 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.92493925  0.08221237  0.27822688 -0.24559786]
True reward weights: [-0.20092079 -0.0044046  -0.72420582  0.65964943]
MAP Policy for current environment:
Information gain 16 demonstrations: 1.614785

Running EBIRL with 17 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4084
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.85526486  0.31532323  0.12065709 -0.39310958]
True reward weights: [-0.74160379  0.63309464 -0.08412303  0.2052762 ]
MAP Policy for current environment:
Information gain 17 demonstrations: 1.609868

Running EBIRL with 18 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4048
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.94592656  0.12509452  0.27105788  0.12689334]
True reward weights: [-0.88487876 -0.42876612 -0.18134498 -0.01622303]
MAP Policy for current environment:
Information gain 18 demonstrations: 1.695738

Running EBIRL with 19 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.79365305  0.13730509  0.17893728  0.56501646]
True reward weights: [-0.41704695 -0.22451201 -0.57402485  0.66795334]
MAP Policy for current environment:
Information gain 19 demonstrations: 1.780902

Running EBIRL with 20 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4128
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.71341947  0.20577115  0.09114708 -0.66361367]
True reward weights: [-0.71132364  0.29388852 -0.07186066 -0.63441648]
MAP Policy for current environment:
Information gain 20 demonstrations: 1.865072

Running EBIRL with 21 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4144
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.76526272  0.16061498  0.16024292  0.60241016]
True reward weights: [-0.71757049 -0.07876549 -0.19144807  0.66500844]
MAP Policy for current environment:
Information gain 21 demonstrations: 1.951905

Running EBIRL with 22 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4180
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.88018639  0.26258182  0.17513922 -0.35446997]
True reward weights: [-0.8289717   0.37872568 -0.41031889  0.03179924]
MAP Policy for current environment:
Information gain 22 demonstrations: 2.038128

Running EBIRL with 23 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4086
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.95031751  0.23262928  0.20588054  0.01983551]
True reward weights: [-0.52133895 -0.2059654  -0.76420309  0.31902601]
MAP Policy for current environment:
Information gain 23 demonstrations: 2.124707

Running EBIRL with 24 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4158
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.93679498  0.25412813  0.22790288  0.07677453]
True reward weights: [-0.92895054 -0.0367533  -0.17707894 -0.32302188]
MAP Policy for current environment:
Information gain 24 demonstrations: 2.176534

Running EBIRL with 25 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4098
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.52297143  0.09475489  0.06781949  0.84434762]
True reward weights: [-0.36613286 -0.26953005 -0.64528892  0.61392384]
MAP Policy for current environment:
Information gain 25 demonstrations: 2.207640

Running EBIRL with 26 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4104
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.87700649  0.18405172  0.21773491  0.38675068]
True reward weights: [-0.42337245  0.31024521  0.08976325  0.84643147]
MAP Policy for current environment:
Information gain 26 demonstrations: 2.290511

Running EBIRL with 27 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4244
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.84871643  0.22060932  0.16037894 -0.45309   ]
True reward weights: [-0.67445603  0.20930096 -0.62770899 -0.32754176]
MAP Policy for current environment:
Information gain 27 demonstrations: 2.376345

Running EBIRL with 28 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4146
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.62125452  0.14656331  0.09036716 -0.76445784]
True reward weights: [-0.52714202 -0.75821953  0.04643486 -0.38087825]
MAP Policy for current environment:
Information gain 28 demonstrations: 2.459850

Running EBIRL with 29 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4118
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.91092155  0.20866812  0.21276124 -0.28532824]
True reward weights: [-0.25054749 -0.92443652 -0.15392147  0.2427988 ]
MAP Policy for current environment:
Information gain 29 demonstrations: 2.460493

Running EBIRL with 30 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4106
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.78105493  0.17119003  0.20564907  0.56423012]
True reward weights: [-0.69403102  0.66140237 -0.18145238 -0.21895862]
MAP Policy for current environment:
Information gain 30 demonstrations: 2.488336

Running EBIRL with 31 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4116
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.94985306  0.22098764  0.22054159  0.01746523]
True reward weights: [-0.7818084   0.59892357  0.09248519 -0.14667198]
MAP Policy for current environment:
Information gain 31 demonstrations: 2.510592

Running EBIRL with 32 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.3914
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.63062735  0.13951485  0.12503213  0.7531346 ]
True reward weights: [-0.20285335 -0.3527884  -0.80493265 -0.4318267 ]
MAP Policy for current environment:
Information gain 32 demonstrations: 2.517865

Running EBIRL with 33 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.3966
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.947263    0.24012442  0.21119202  0.02076048]
True reward weights: [-0.69071994 -0.69887472 -0.15513297  0.10204827]
MAP Policy for current environment:
Information gain 33 demonstrations: 2.536909

Running EBIRL with 34 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.3856
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.85605461  0.17920658  0.21353295 -0.4352691 ]
True reward weights: [-0.68082078 -0.29891989  0.06699271  0.66531342]
MAP Policy for current environment:
Information gain 34 demonstrations: 2.624718

Running EBIRL with 35 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.3914
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.66050217  0.13279907  0.11090055  0.73061779]
True reward weights: [-0.53750143 -0.83602317 -0.1022971  -0.04114325]
MAP Policy for current environment:
Information gain 35 demonstrations: 2.640503

Running EBIRL with 36 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.3976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.93360454  0.23627627  0.2491574  -0.10235565]
True reward weights: [-0.72042952 -0.68162298 -0.12687023  0.01659409]
MAP Policy for current environment:
Information gain 36 demonstrations: 2.655651

Running EBIRL with 37 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.3846
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.82969846  0.20373202  0.22174648 -0.47002366]
True reward weights: [-0.84039369  0.09486752  0.272647   -0.45869622]
MAP Policy for current environment:
Information gain 37 demonstrations: 2.941358

Running EBIRL with 38 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.3992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.9302994   0.24047748  0.20058474  0.19099572]
True reward weights: [-0.61505666 -0.6571834   0.08612758 -0.42708   ]
MAP Policy for current environment:
Information gain 38 demonstrations: 2.966963

Running EBIRL with 39 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.3950
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.89571798  0.19917274  0.2373654  -0.31886859]
True reward weights: [-0.27194561 -0.51082674 -0.02000704  0.81529218]
MAP Policy for current environment:
Information gain 39 demonstrations: 3.053691

Running EBIRL with 40 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.3994
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.94046467  0.2490445   0.22332668  0.06023492]
True reward weights: [-0.20379567 -0.21576284 -0.14085727 -0.94449614]
MAP Policy for current environment:
Information gain 40 demonstrations: 3.054136

Saving results to files...
Results saved successfully.

Running experiment 19/19...
Shuffled Demos: [([(0, 3), (1, 0), (1, 3), (2, 3)], 0), ([(0, 0), (0, 0), (0, 2), (0, 0)], 3), ([(0, 1), (5, 0), (0, 2), (0, 2)], 3), ([(0, 2), (0, 2), (0, 2), (0, 2)], 3), ([(0, 2), (0, 0), (0, 3), (1, 1)], 2), ([(0, 2), (0, 2), (0, 1), (5, 2)], 3), ([(0, 1), (5, 1), (10, 2), (10, 2)], 0), ([(0, 3), (1, 3), (2, 1), (7, 0)], 3), ([(0, 2), (0, 3), (1, 2), (0, 2)], 1), ([(0, 0), (0, 3), (1, 0), (1, 2)], 0), ([(0, 3), (1, 0), (1, 2), (0, 1)], 0), ([(0, 2), (0, 2), (0, 2), (0, 3)], 3), ([(0, 1), (5, 2), (5, 3), (6, 2)], 3), ([(0, 0), (0, 1), (5, 0), (0, 1)], 3), ([(0, 3), (1, 2), (0, 2), (0, 0)], 3), ([(0, 2), (0, 0), (0, 1), (5, 3)], 3), ([(0, 0), (0, 1), (5, 2), (5, 0)], 3), ([(0, 0), (0, 2), (0, 0), (0, 1)], 3), ([(0, 2), (0, 2), (0, 2), (0, 2)], 3), ([(0, 1), (5, 3), (6, 1), (11, 1)], 3), ([(0, 1), (5, 2), (5, 2), (5, 2)], 3), ([(0, 2), (0, 1), (5, 2), (5, 3)], 3), ([(0, 2), (0, 2), (0, 1), (5, 2)], 3), ([(0, 3), (1, 3), (2, 2), (1, 1)], 0), ([(0, 2), (0, 2), (0, 0), (0, 0)], 3), ([(0, 0), (0, 2), (0, 1), (5, 2)], 3), ([(0, 3), (1, 2), (0, 2), (0, 0)], 3), ([(0, 2), (0, 0), (0, 1), (5, 2)], 3), ([(0, 2), (0, 0), (0, 0), (0, 3)], 3), ([(0, 3), (1, 2), (0, 2), (0, 2)], 3), ([(0, 2), (0, 1), (5, 1), (10, 1)], 1), ([(0, 1), (5, 0), (0, 3), (1, 1)], 2), ([(0, 0), (0, 3), (1, 0), (1, 2)], 0), ([(0, 2), (0, 0), (0, 2), (0, 0)], 3), ([(0, 2), (0, 3), (1, 3), (2, 3)], 1), ([(0, 3), (1, 0), (1, 3), (2, 2)], 0), ([(0, 2), (0, 1), (5, 0), (0, 2)], 3), ([(0, 2), (0, 0), (0, 1), (5, 0)], 3), ([(0, 2), (0, 2), (0, 1), (5, 3)], 3), ([(0, 0), (0, 0), (0, 0), (0, 0)], 3)]
Maximum entropy: 8.0864

Running EBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5950
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.33452978  0.88240277 -0.1079263   0.31274127]
True reward weights: [-0.55653737  0.11873141  0.39462253  0.72141671]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.137554

Running EBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5902
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.41027069 -0.25831703  0.07009779  0.87180076]
True reward weights: [-0.65947789  0.72365081  0.1262465  -0.15962533]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.187113

Running EBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5936
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.72085391  0.26650034  0.63972379  0.01003367]
True reward weights: [-0.40567514 -0.53336705  0.72652523 -0.15201434]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.255233

Running EBIRL with 4 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.89586556 -0.04175559  0.43948071  0.05037935]
True reward weights: [-0.4167587   0.23354703 -0.2441798  -0.84388636]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.331125

Running EBIRL with 5 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5438
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.59638517 -0.18758026  0.63952242 -0.44738066]
True reward weights: [-0.68208347 -0.36469718  0.06627863  0.63036914]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.328242

Running EBIRL with 6 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5394
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.82219604 -0.14929112  0.3190759   0.44709776]
True reward weights: [-0.58119341  0.49658963  0.46396495  0.44760416]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.405086

Running EBIRL with 7 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.55654723 -0.25925443  0.6397685   0.46231871]
True reward weights: [-0.73215442 -0.62757214  0.07344378 -0.2543799 ]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.420141

Running EBIRL with 8 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4968
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.66908024  0.20090997  0.31994223  0.64000296]
True reward weights: [-0.25592851  0.15627897  0.15668818  0.94102407]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.500976

Running EBIRL with 9 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4622
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.60737775  0.68278533 -0.01639268 -0.40574345]
True reward weights: [-0.8208771  -0.48605653 -0.02656811  0.29867034]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.499639

Running EBIRL with 10 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4704
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.66654858 -0.63770396  0.31946656 -0.21676663]
True reward weights: [-0.22822749  0.29015687 -0.78855222  0.49184002]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.498137

Running EBIRL with 11 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4746
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 11
MAP Solution: [-0.64869922 -0.72908034  0.15989217  0.14854519]
True reward weights: [-0.91344529  0.20050876  0.35015132 -0.05299056]
MAP Policy for current environment:
Information gain 11 demonstrations: 0.497251

Running EBIRL with 12 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4738
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 12
MAP Solution: [-0.61890052  0.48049121  0.26573258 -0.56167297]
True reward weights: [-0.82407616  0.27277126 -0.33069219 -0.37032011]
MAP Policy for current environment:
Information gain 12 demonstrations: 0.581128

Running EBIRL with 13 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4708
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 13
MAP Solution: [-0.66696482 -0.45879538  0.31834919  0.49327327]
True reward weights: [-0.5882122   0.78156038 -0.20186927 -0.04917898]
MAP Policy for current environment:
Information gain 13 demonstrations: 0.665857

Running EBIRL with 14 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4738
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 14
MAP Solution: [-0.63224894  0.0810709   0.04276089 -0.76932457]
True reward weights: [-0.04018095 -0.28040923 -0.79801427  0.53191107]
MAP Policy for current environment:
Information gain 14 demonstrations: 0.750829

Running EBIRL with 15 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4746
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 15
MAP Solution: [-0.74399807  0.22047571  0.35203189  0.52338407]
True reward weights: [-0.2846496  -0.53585816  0.08732868 -0.79006603]
MAP Policy for current environment:
Information gain 15 demonstrations: 0.836220

Running EBIRL with 16 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4720
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 16
MAP Solution: [-0.73900699  0.2780303   0.33252029 -0.51575001]
True reward weights: [-0.16630558  0.12361962 -0.59533078 -0.77630014]
MAP Policy for current environment:
Information gain 16 demonstrations: 0.921668

Running EBIRL with 17 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4668
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 17
MAP Solution: [-0.82016105  0.34433756  0.31956911 -0.32656251]
True reward weights: [-0.27242009 -0.88170989 -0.0616736  -0.38022537]
MAP Policy for current environment:
Information gain 17 demonstrations: 1.007484

Running EBIRL with 18 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4712
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 18
MAP Solution: [-0.70667877  0.67404857  0.20020656  0.07861916]
True reward weights: [-0.77859058 -0.20586771  0.35981861 -0.47111119]
MAP Policy for current environment:
Information gain 18 demonstrations: 1.093141

Running EBIRL with 19 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4724
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 19
MAP Solution: [-0.72043426  0.62260826  0.21720919 -0.21483386]
True reward weights: [-0.31232584  0.4176706  -0.84475635 -0.11996059]
MAP Policy for current environment:
Information gain 19 demonstrations: 1.178259

Running EBIRL with 20 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4790
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 20
MAP Solution: [-0.69787311  0.71166902 -0.07271834  0.0348192 ]
True reward weights: [-0.42963298 -0.70163336 -0.33020108  0.46270225]
MAP Policy for current environment:
Information gain 20 demonstrations: 1.263901

Running EBIRL with 21 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4676
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 21
MAP Solution: [-0.41708729  0.37340392 -0.05397537 -0.82685813]
True reward weights: [-0.73214109 -0.51438908 -0.23295964 -0.38092401]
MAP Policy for current environment:
Information gain 21 demonstrations: 1.349475

Running EBIRL with 22 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4774
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 22
MAP Solution: [-0.7893381  -0.56810427  0.15882918  0.17022393]
True reward weights: [-0.61316421  0.2736574  -0.69935671 -0.24503362]
MAP Policy for current environment:
Information gain 22 demonstrations: 1.435046

Running EBIRL with 23 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4674
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 23
MAP Solution: [-0.86681883  0.11613261  0.38697029 -0.29221966]
True reward weights: [-0.74474581  0.59640842 -0.14637516  0.26119914]
MAP Policy for current environment:
Information gain 23 demonstrations: 1.520455

Running EBIRL with 24 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4620
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 24
MAP Solution: [-0.49828249 -0.08515995 -0.05626948 -0.86098553]
True reward weights: [-0.85563076 -0.40275207  0.08936401 -0.31257135]
MAP Policy for current environment:
Information gain 24 demonstrations: 1.520497

Running EBIRL with 25 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4722
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 25
MAP Solution: [-0.64268027  0.09980966  0.31827451 -0.68971113]
True reward weights: [-0.3314685   0.63277337  0.01466416 -0.69965095]
MAP Policy for current environment:
Information gain 25 demonstrations: 1.605970

Running EBIRL with 26 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4756
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 26
MAP Solution: [-0.94539176 -0.17802813  0.27263527 -0.01450558]
True reward weights: [-0.04592652 -0.18867556 -0.82143047  0.53623155]
MAP Policy for current environment:
Information gain 26 demonstrations: 1.691654

Running EBIRL with 27 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4676
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 27
MAP Solution: [-0.75543766 -0.04769798  0.15994642  0.63360555]
True reward weights: [-0.76285167 -0.51399131  0.07779455  0.38447141]
MAP Policy for current environment:
Information gain 27 demonstrations: 1.777401

Running EBIRL with 28 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4766
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 28
MAP Solution: [-0.8685024   0.36209197  0.30438824 -0.14812419]
True reward weights: [-0.5816044   0.46788103 -0.42302984 -0.51368221]
MAP Policy for current environment:
Information gain 28 demonstrations: 1.862987

Running EBIRL with 29 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4702
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 29
MAP Solution: [-0.70046442 -0.49385247  0.31938063  0.4042961 ]
True reward weights: [-0.47301591  0.2979344  -0.56836477 -0.60369904]
MAP Policy for current environment:
Information gain 29 demonstrations: 1.948383

Running EBIRL with 30 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4720
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 30
MAP Solution: [-0.67145584 -0.66351916  0.31984548 -0.08116801]
True reward weights: [-0.3228681   0.22855556 -0.04536475 -0.91731161]
MAP Policy for current environment:
Information gain 30 demonstrations: 2.034120

Running EBIRL with 31 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4510
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 31
MAP Solution: [-0.78856584  0.19435683  0.35966429 -0.45938103]
True reward weights: [-0.87017768 -0.32335632  0.33448119  0.16233863]
MAP Policy for current environment:
Information gain 31 demonstrations: 2.046252

Running EBIRL with 32 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4442
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 32
MAP Solution: [-0.49841707 -0.06177564 -0.057265   -0.8628354 ]
True reward weights: [-0.65189411 -0.52986595  0.32800439 -0.43207553]
MAP Policy for current environment:
Information gain 32 demonstrations: 2.045908

Running EBIRL with 33 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4392
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 33
MAP Solution: [-0.81941943 -0.21873608  0.49882627  0.178546  ]
True reward weights: [-0.545487   -0.11688492 -0.65644836 -0.50779662]
MAP Policy for current environment:
Information gain 33 demonstrations: 2.045636

Running EBIRL with 34 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4366
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 34
MAP Solution: [-0.6833952  -0.28931201  0.31916326 -0.58941019]
True reward weights: [-0.14221664 -0.40821468 -0.13980526  0.89083651]
MAP Policy for current environment:
Information gain 34 demonstrations: 2.130977

Running EBIRL with 35 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4462
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 35
MAP Solution: [-0.67867052 -0.65594158  0.31885779  0.08646783]
True reward weights: [-0.24868737 -0.38293568 -0.21743026 -0.86269284]
MAP Policy for current environment:
Information gain 35 demonstrations: 2.130624

Running EBIRL with 36 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4398
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 36
MAP Solution: [-0.99223543  0.08067561  0.0556347   0.07658378]
True reward weights: [-0.34111359  0.25532897 -0.16748613 -0.88904276]
MAP Policy for current environment:
Information gain 36 demonstrations: 2.130504

Running EBIRL with 37 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4388
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 37
MAP Solution: [-0.7517059  -0.45368627  0.15884992 -0.45152377]
True reward weights: [-0.62680664 -0.58122805 -0.46997547 -0.22002377]
MAP Policy for current environment:
Information gain 37 demonstrations: 2.216191

Running EBIRL with 38 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4406
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 38
MAP Solution: [-0.53604868  0.06183374  0.04848838  0.84052203]
True reward weights: [-0.40267336 -0.84564029 -0.3470114   0.04826743]
MAP Policy for current environment:
Information gain 38 demonstrations: 2.301709

Running EBIRL with 39 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4362
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 39
MAP Solution: [-0.80464693  0.16782037  0.31987075 -0.47123491]
True reward weights: [-0.34960566 -0.26826682 -0.89452542 -0.07505377]
MAP Policy for current environment:
Information gain 39 demonstrations: 2.387358

Running EBIRL with 40 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4412
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 40
MAP Solution: [-0.92896369 -0.0815986   0.31997573 -0.1672832 ]
True reward weights: [-0.62596623  0.38218293  0.54206124  0.41020982]
MAP Policy for current environment:
Information gain 40 demonstrations: 2.473022
