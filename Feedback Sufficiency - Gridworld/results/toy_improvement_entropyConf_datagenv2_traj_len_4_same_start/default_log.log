Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 1), (3, 2), (3, 2)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 1), (4, 2)]), ([(0, 1), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 1), (1, 3), (2, 0), (2, 0)]), ([(0, 1), (1, 1), (0, 1), (3, 3)], [(0, 2), (0, 3), (1, 2), (0, 2)]), ([(0, 0), (0, 1), (0, 1), (3, 3)], [(0, 0), (0, 0), (0, 3), (1, 3)]), ([(0, 2), (0, 1), (3, 3), (3, 3)], [(0, 2), (0, 0), (0, 0), (0, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6440
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.20083681 -0.96721412  0.15543944]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6692
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.52309072 -0.8191452   0.23532368]
True reward weights: [-0.326571   -0.68242607 -0.65394651]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124095

Running PBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5678
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.90613584 -0.20860192  0.36797158]
True reward weights: [-0.92786097 -0.31746911  0.19567161]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.145822

Running PBIRL with 4 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5680
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.31444782 -0.8136788   0.48892675]
True reward weights: [-0.296764   -0.56073188  0.77298828]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.164101

Running PBIRL with 5 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3022
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.85721113 -0.29122156  0.42471059]
True reward weights: [-0.27322521 -0.84116201  0.46668454]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.239923

Running PBIRL with 6 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.3144
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.79697003 -0.45445881  0.39787682]
True reward weights: [-0.93630743 -0.30474334  0.17452762]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.274802

Running experiment 2/50...
Shuffled Demos: [([(0, 1), (3, 3), (3, 3), (0, 1)], [(0, 1), (3, 0), (0, 1), (1, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 0), (0, 0)]), ([(0, 2), (0, 1), (0, 1), (3, 3)], [(0, 2), (0, 3), (3, 0), (0, 0)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 1), (3, 2)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 1), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6664
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.92653844  0.37240154  0.0533255 ]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3256
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.52133959 -0.17347575  0.83553049]
True reward weights: [-0.9774998  -0.10643777 -0.18211298]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.172957

Running PBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3268
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.88250014  0.04840269  0.4678148 ]
True reward weights: [-0.1499091   0.36171969  0.92015549]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.179400

Running PBIRL with 4 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2652
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.52655767  0.15939778  0.8350625 ]
True reward weights: [-0.85134478  0.00384282  0.52459251]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.192896

Running PBIRL with 5 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2692
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.55572582  0.31836408  0.76799292]
True reward weights: [-0.37124211  0.18223148  0.91047844]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.187776

Running PBIRL with 6 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2738
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.57075062  0.31874407  0.75673374]
True reward weights: [-0.67687471 -0.29410395  0.67479145]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.180796

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Shuffled Demos: [([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 1), (4, 1)]), ([(0, 1), (3, 3), (3, 3), (4, 3)], [(0, 1), (3, 3), (3, 1), (3, 0)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 2), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 2), (3, 3), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (4, 3)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 3), (1, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6600
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.94272706  0.29589535  0.15398583]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123671

Running PBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6664
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.74869913  0.63840834  0.17856206]
True reward weights: [-0.17002874  0.95880707  0.2275505 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148187

Running PBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6602
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.87707936  0.31828759  0.3597566 ]
True reward weights: [-0.65035501  0.21796035  0.72768926]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.203156

Running PBIRL with 4 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5378
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.71114047  0.15834125  0.68498706]
True reward weights: [-0.31970323  0.94699302 -0.03152887]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.238778

Running PBIRL with 5 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5224
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.33348374  0.01510341  0.94263486]
True reward weights: [-0.50886188 -0.72167473 -0.46930286]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.231318

Running PBIRL with 6 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4426
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.13081621  0.31896761  0.93869419]
True reward weights: [ 0.09483789 -0.89328846  0.43936489]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.256417

Running experiment 4/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 1), (0, 1)], [(0, 0), (0, 0), (0, 3), (1, 1)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 1), (3, 3), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 3), (4, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 3), (4, 1)]), ([(0, 3), (0, 1), (0, 1), (3, 3)], [(0, 3), (0, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 2), (0, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6588
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.90192405 -0.40348842 -0.1540458 ]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4412
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.63380851 -0.26747458  0.7257714 ]
True reward weights: [-0.00465533  0.67563077  0.73722547]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.158451

Running PBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3014
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.80081632  0.05977898  0.5959192 ]
True reward weights: [-0.92745527  0.3432458   0.14835443]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.181450

Running PBIRL with 4 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3258
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.80306917  0.12128129  0.58341303]
True reward weights: [-0.10186016 -0.04796143  0.99364189]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.165391

Running PBIRL with 5 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.90101727  0.06205023  0.42932232]
True reward weights: [-0.38527246  0.48261225  0.78654342]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.180614

Running PBIRL with 6 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.3046
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.59167485  0.31970955  0.74007207]
True reward weights: [0.07643275 0.2949219  0.95245951]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.175384

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 2), (3, 1)]), ([(0, 3), (1, 2), (0, 1), (3, 3)], [(0, 3), (1, 2), (0, 3), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (4, 0)]), ([(0, 3), (0, 1), (3, 3), (4, 3)], [(0, 3), (0, 2), (0, 0), (0, 1)]), ([(0, 2), (0, 3), (1, 1), (4, 3)], [(0, 2), (0, 3), (1, 2), (0, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6478
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.51917173  0.2558798   0.81546689]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6642
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.01230638  0.8572991   0.51467155]
True reward weights: [-0.4245769  -0.27713635  0.86193381]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148082

Running PBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6496
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.82224612 0.15086668 0.54877187]
True reward weights: [ 0.62810333 -0.72787946  0.27509581]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148625

Running PBIRL with 4 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5714
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.78484614 0.16294636 0.59788378]
True reward weights: [0.73818228 0.57986127 0.34474313]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.174386

Running PBIRL with 5 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5574
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.17245583 0.31903818 0.93191932]
True reward weights: [ 0.54246368 -0.41624631  0.7297069 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.223929

Running PBIRL with 6 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5638
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.51955367 -0.07854732  0.85081978]
True reward weights: [-0.54478813 -0.27525976  0.79210982]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.290736

Running experiment 6/50...
Shuffled Demos: [([(0, 0), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 1), (4, 1)]), ([(0, 3), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 3), (2, 3), (2, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 0), (2, 1)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 0), (0, 0)]), ([(0, 2), (0, 1), (3, 3), (4, 3)], [(0, 2), (0, 3), (1, 1), (4, 2)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 2), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6636
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.28578589 0.61546139 0.73452958]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5704
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.51032657 -0.3472194   0.78676901]
True reward weights: [ 0.31627103 -0.80130692  0.50781872]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.140388

Running PBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4844
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.42184065  0.28332101  0.86126632]
True reward weights: [0.3649849  0.51347086 0.77661683]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152073

Running PBIRL with 4 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.56998058  0.08232902  0.81752313]
True reward weights: [-0.13607409  0.20736665  0.96875328]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.143773

Running PBIRL with 5 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.79574562 -0.40058683  0.45422361]
True reward weights: [-0.35844796 -0.82966657  0.42798183]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.200100

Running PBIRL with 6 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.58411687 -0.16407733  0.79491264]
True reward weights: [-0.26801063  0.08532139  0.95963043]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.184707

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Shuffled Demos: [([(0, 1), (0, 1), (0, 1), (0, 1)], [(0, 3), (1, 3), (1, 1), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 2), (0, 2)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 0), (4, 1)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 1), (3, 2), (3, 1)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 3), (4, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6648
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.59386652 -0.35983215  0.71961335]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3316
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.58552688  0.05505577  0.80878126]
True reward weights: [0.12637864 0.89171667 0.43459155]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.172619

Running PBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3118
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.14535976  0.14300026  0.97899002]
True reward weights: [-0.33558028  0.56807501  0.7514497 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.162117

Running PBIRL with 4 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3092
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.11241126  0.28321733  0.95244509]
True reward weights: [-0.82537626 -0.56039167  0.06866736]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.151516

Running PBIRL with 5 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3134
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.75943885 -0.35339736  0.54622608]
True reward weights: [0.07362161 0.21296579 0.97428201]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.146649

Running PBIRL with 6 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3090
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.90296369 -0.04315138  0.42754477]
True reward weights: [-0.61505203  0.44352423  0.65191814]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.142117

Running experiment 8/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (4, 3)], [(0, 2), (0, 3), (1, 0), (1, 0)]), ([(0, 0), (0, 1), (0, 1), (3, 3)], [(0, 0), (0, 1), (0, 0), (1, 3)]), ([(0, 2), (0, 0), (0, 1), (3, 3)], [(0, 2), (0, 0), (0, 2), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 0), (0, 0), (0, 3), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 3), (4, 1)]), ([(0, 3), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 1), (2, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6606
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.22592916  0.86795785  0.44227274]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6460
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90531918 -0.40937524  0.11317729]
True reward weights: [-0.4038176   0.57080027 -0.71492545]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123668

Running PBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6610
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.52755608  0.63846357 -0.56040062]
True reward weights: [-0.56361219  0.62224165  0.5432832 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.147782

Running PBIRL with 4 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6656
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.73218359  0.63906054 -0.23560309]
True reward weights: [-0.65141433  0.16331162 -0.7409377 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.202801

Running PBIRL with 5 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4418
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.20763675  0.63841448  0.74115716]
True reward weights: [-0.2027237   0.9299517   0.30674571]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.265113

Running PBIRL with 6 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4372
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.07824394  0.87908475  0.47019983]
True reward weights: [-0.56755438  0.81602679  0.10946369]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.252488

Saving results to files...
Results saved successfully.

Running experiment 9/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 3), (0, 2)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 3), (0, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 0), (1, 3), (1, 2)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 2), (0, 3), (0, 2), (0, 0)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 3), (2, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6592
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.06583648 -0.29739035  0.95248335]
True reward weights: [-0.0505111  -0.01531811  0.99860602]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123671

Running PBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5214
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.07685933 0.15456869 0.9849879 ]
True reward weights: [0.13861592 0.70144921 0.69910989]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148556

Running PBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5052
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.62644988 -0.45526556  0.63268777]
True reward weights: [-0.36528686 -0.75294352  0.54739525]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.141647

Running PBIRL with 4 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3260
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.22489881  0.18459661  0.95673644]
True reward weights: [-0.21866765 -0.79758099  0.56218238]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.194635

Running PBIRL with 5 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.1790
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.22020404 -0.15256109  0.96344969]
True reward weights: [0.34385225 0.20380531 0.91664007]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.229051

Running PBIRL with 6 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.1800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.02000386 -0.44428885  0.89566024]
True reward weights: [0.41081056 0.16386244 0.89687446]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.205458

Running experiment 10/50...
Shuffled Demos: [([(0, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 2), (3, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 2), (3, 2)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 3)]), ([(0, 0), (0, 1), (3, 3), (3, 3)], [(0, 0), (0, 1), (3, 0), (0, 2)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 3), (1, 1)]), ([(0, 1), (3, 1), (3, 3), (3, 3)], [(0, 1), (3, 1), (3, 1), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6624
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.35575405 -0.32385035  0.87667554]
True reward weights: [-0.76518493 -0.64328377  0.02603879]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123674

Running PBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5670
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.26871017 0.09893775 0.95812638]
True reward weights: [-0.0788319  -0.93843928  0.33632908]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.140705

Running PBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5698
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.72317655 -0.49717379  0.47940995]
True reward weights: [-0.34424241 -0.76836076  0.53955436]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.134970

Running PBIRL with 4 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5586
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.71756887 0.15906944 0.67807952]
True reward weights: [-0.62746442  0.16361759  0.76126059]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.156377

Running PBIRL with 5 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4380
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.10586903  0.31992372  0.94150972]
True reward weights: [-0.52326641  0.27451734  0.8067419 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.196745

Running PBIRL with 6 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4262
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.3669214   0.31909423  0.87381208]
True reward weights: [0.30499198 0.29080624 0.90686913]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.242527

Saving results to files...
Results saved successfully.

Running experiment 11/50...
Shuffled Demos: [([(0, 0), (0, 1), (0, 1), (3, 3)], [(0, 0), (0, 0), (0, 0), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (1, 2), (0, 3), (0, 3)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 1), (3, 2), (3, 2), (3, 0)]), ([(0, 0), (0, 2), (0, 1), (3, 3)], [(0, 0), (0, 2), (0, 3), (1, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 2), (0, 0)]), ([(0, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 1), (3, 0), (0, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.21849178  0.63967917  0.73693412]
True reward weights: [-0.87829137 -0.24439244  0.41094598]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6488
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.76312271  0.63844808 -0.10013882]
True reward weights: [-0.89401311 -0.13271678  0.4279332 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182833

Running PBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.76772367  0.63969174  0.03734763]
True reward weights: [-0.98214103 -0.16734783 -0.08598658]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.225058

Running PBIRL with 4 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5196
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.94594979  0.31963247 -0.0549006 ]
True reward weights: [-0.51527226  0.11966014  0.84863181]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.252497

Running PBIRL with 5 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3354
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.80466394  0.15987543  0.57180048]
True reward weights: [-0.65704449  0.29514381  0.69367331]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.297343

Running PBIRL with 6 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3204
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.89461775  0.06599774  0.44193142]
True reward weights: [-0.82576181 -0.49052437 -0.2783941 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.287973

Running experiment 12/50...
Shuffled Demos: [([(0, 1), (0, 1), (3, 3), (3, 3)], [(0, 3), (1, 1), (0, 1), (1, 1)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 0), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (0, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 1), (3, 3)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 1), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 0), (1, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6596
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.44392699 -0.01650028  0.89591103]
True reward weights: [-0.8497926  -0.50850291  0.13884283]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.44760982  0.89392828  0.02318793]
True reward weights: [-0.26434478  0.1096328  -0.95817665]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.134526

Running PBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3220
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.90303166 -0.42804328 -0.03623217]
True reward weights: [-0.89149562  0.07116501  0.44740484]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.191879

Running PBIRL with 4 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3266
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.72254036  0.11198962  0.68219774]
True reward weights: [-0.06711775  0.51925931  0.8519771 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.171374

Running PBIRL with 5 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3342
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.2942355   0.34940026  0.8895757 ]
True reward weights: [-0.82736834  0.38022956  0.41338494]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.160048

Running PBIRL with 6 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.3294
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.53532381 -0.13928297  0.83308383]
True reward weights: [-0.62639926 -0.50326185  0.59527428]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.152794

Saving results to files...
Results saved successfully.

Running experiment 13/50...
Shuffled Demos: [([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (0, 3), (1, 0)]), ([(0, 2), (0, 1), (3, 3), (3, 3)], [(0, 2), (0, 0), (0, 0), (0, 1)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 3), (2, 3)]), ([(0, 2), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 3), (0, 1)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 3), (4, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6460
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.01074847  0.18323334  0.98301069]
True reward weights: [-0.34477193 -0.32328333  0.88126058]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123671

Running PBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5682
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.10926854 -0.16433993  0.98033299]
True reward weights: [ 0.98591643 -0.14928424  0.07538571]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.141780

Running PBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5884
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.24238976 0.31948379 0.91606621]
True reward weights: [-0.52492668  0.64695738  0.55308058]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.160012

Running PBIRL with 4 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5134
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.48283758 -0.0495351   0.87430781]
True reward weights: [-0.56039677  0.64022926  0.52541598]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.173561

Running PBIRL with 5 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3542
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.23333894  0.31979662  0.91830445]
True reward weights: [-0.88300085  0.36942138  0.28954679]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.217319

Running PBIRL with 6 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3310
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.38101836 0.31960179 0.86757116]
True reward weights: [0.30859718 0.18438916 0.93314973]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.217815

Running experiment 14/50...
Shuffled Demos: [([(0, 1), (1, 3), (2, 1), (5, None)], [(0, 1), (1, 3), (2, 2), (1, 3)]), ([(0, 0), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 2), (4, 3)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 2), (4, 3), (5, None)]), ([(0, 1), (0, 1), (3, 3), (0, 1)], [(0, 1), (0, 2), (0, 1), (3, 2)]), ([(0, 3), (1, 2), (4, 3), (5, None)], [(0, 3), (1, 2), (4, 1), (4, 2)]), ([(0, 1), (1, 0), (1, 1), (4, 3), (5, None)], [(0, 1), (1, 0), (1, 0), (1, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.6524
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.08070368 -0.83804841  0.53959409]
True reward weights: [-0.43088031 -0.17122291  0.88601629]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5690
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.78062824  0.50304901  0.37089249]
True reward weights: [-0.68401722 -0.63456221  0.35979334]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.138961

Running PBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5648
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.40615022 -0.02101722  0.9135646 ]
True reward weights: [-0.18680487 -0.96586948  0.17944383]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.158163

Running PBIRL with 4 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5564
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.63165456  0.63873608  0.43935037]
True reward weights: [0.48952466 0.01105698 0.87191935]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.210523

Running PBIRL with 5 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.11519704 0.08508054 0.98969235]
True reward weights: [-0.56741158 -0.56476051  0.59924091]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.233801

Running PBIRL with 6 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4230
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.12128909  0.21350302  0.96938404]
True reward weights: [ 0.52531839 -0.60662537  0.59669611]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.251876

Saving results to files...
Results saved successfully.

Running experiment 15/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (3, 3)], [(0, 2), (0, 2), (0, 1), (3, 3)]), ([(0, 0), (0, 2), (0, 1), (3, 3)], [(0, 0), (0, 2), (0, 2), (0, 3)]), ([(0, 1), (1, 0), (1, 1), (4, 3), (5, None)], [(0, 1), (1, 0), (1, 0), (1, 3)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 1), (3, 2)]), ([(0, 1), (3, 1), (4, 3), (5, None)], [(0, 1), (3, 1), (4, 1), (4, 3)]), ([(0, 1), (3, 3), (3, 3), (3, 3)], [(0, 3), (1, 0), (1, 1), (4, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.46536459  0.63875647 -0.61272014]
True reward weights: [-0.75433776 -0.02805593  0.65588674]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.7498307 0.6387422 0.1725176]
True reward weights: [0.74569533 0.36546757 0.55711035]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running PBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.6642
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.65624132  0.63849522  0.40208356]
True reward weights: [0.69939006 0.07975325 0.71027668]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.251542

Running PBIRL with 4 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5462
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.11610509  0.31877506  0.94069234]
True reward weights: [-0.76327637 -0.01880966 -0.64579825]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.265180

Running PBIRL with 5 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.4288
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.18339107  0.31854914  0.92999686]
True reward weights: [0.13082334 0.9904544  0.04342053]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.282313

Running PBIRL with 6 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3144
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.10173987 0.3189967  0.94227921]
True reward weights: [0.39755716 0.42230717 0.81461952]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.307695

Running experiment 16/50...
Shuffled Demos: [([(0, 3), (0, 1), (0, 1), (0, 1)], [(0, 3), (0, 2), (3, 3), (4, 2)]), ([(0, 1), (3, 3), (3, 3), (0, 1)], [(0, 1), (3, 1), (3, 1), (3, 3)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 0), (0, 2)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 3), (1, 3)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 1), (3, 0), (0, 3), (0, 0)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (3, 3), (3, 3), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.57028355  0.63861715 -0.51666702]
True reward weights: [-0.92032338 -0.11683579  0.37330186]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.23843987  0.63943944  0.7309334 ]
True reward weights: [ 0.50511273 -0.1210201   0.85452634]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running PBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.6674
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.47978274  0.63925906  0.60096288]
True reward weights: [-0.76540495  0.61032252  0.20411194]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.250382

Running PBIRL with 4 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5628
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.38810281 -0.07010343  0.91894598]
True reward weights: [0.10362037 0.48479881 0.86846585]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.260108

Running PBIRL with 5 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5650
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.5441641   0.63933759  0.54326134]
True reward weights: [-0.8531578   0.48090177  0.20212682]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.317008

Running PBIRL with 6 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4198
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.49838122 0.05551061 0.86517902]
True reward weights: [ 0.07491461 -0.0725297   0.99454876]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.356784

Saving results to files...
Results saved successfully.

Running experiment 17/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 0), (1, 3)]), ([(0, 2), (0, 1), (3, 3), (4, 3)], [(0, 2), (0, 0), (0, 1), (3, 1)]), ([(0, 2), (0, 2), (0, 1), (3, 3)], [(0, 2), (0, 2), (0, 2), (0, 1)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 3), (4, 3)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 2), (0, 3), (1, 3), (2, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6554
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.31441618 0.93812434 0.14513848]
True reward weights: [-0.76208548 -0.28664819  0.58056743]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6604
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.65235405  0.63978911  0.40633003]
True reward weights: [-0.19368551  0.41017158  0.89120436]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147723

Running PBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6594
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.03979499  0.63920926  0.76800253]
True reward weights: [-0.63182157  0.67493089 -0.38114275]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.202685

Running PBIRL with 4 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.5026
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.27052985  0.3184789   0.9085069 ]
True reward weights: [-0.93102237 -0.3621745  -0.0450219 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.242447

Running PBIRL with 5 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.4410
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.41099055  0.3193779   0.85386446]
True reward weights: [-0.26380255 -0.07621844  0.96156069]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.256935

Running PBIRL with 6 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.3186
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.50346464  0.31878281  0.80305721]
True reward weights: [-0.70747558  0.56743454  0.42130316]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.290512

Running experiment 18/50...
Shuffled Demos: [([(0, 1), (3, 1), (3, 3), (3, 3)], [(0, 1), (3, 1), (3, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 0)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 0), (0, 1), (3, 0), (3, 1)]), ([(0, 0), (0, 1), (3, 3), (3, 3)], [(0, 0), (0, 3), (3, 3), (4, 2)]), ([(0, 3), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 3), (2, 2), (1, 0)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 3), (2, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.34353887 0.63833897 0.68884281]
True reward weights: [-0.39174849 -0.30341429  0.86860399]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6586
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.04048779 0.63869425 0.76839469]
True reward weights: [-0.408972    0.76673375  0.49483458]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183201

Running PBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6562
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.85386688  0.04411281  0.51861875]
True reward weights: [0.16920366 0.27843614 0.94543294]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.225359

Running PBIRL with 4 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6686
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.93854371  0.31935722  0.13094529]
True reward weights: [-0.27149807  0.73775657  0.61806476]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.289789

Running PBIRL with 5 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6548
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.7038656   0.31945384  0.63444658]
True reward weights: [-0.67668503 -0.40056021 -0.61777738]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.286744

Running PBIRL with 6 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.5888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.70886255  0.63874647 -0.29919362]
True reward weights: [-0.71114102  0.2922506  -0.6394279 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.305247

Saving results to files...
Results saved successfully.

Running experiment 19/50...
Shuffled Demos: [([(0, 3), (1, 0), (1, 1), (4, 3)], [(0, 3), (1, 0), (1, 0), (1, 0)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 1), (4, 2)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (2, 2)]), ([(0, 2), (0, 0), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (1, 0), (1, 3)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 3), (4, 3)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 0), (0, 0), (0, 2), (0, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6552
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.94410372  0.320517    0.07705204]
True reward weights: [-0.92648321 -0.16748955  0.33701054]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123678

Running PBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.3254
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.9370531  -0.24067762  0.25299364]
True reward weights: [ 0.47691818  0.80679671 -0.34875223]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173998

Running PBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.3158
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.75083043 -0.09026312  0.65429828]
True reward weights: [-0.91794402  0.21632336  0.33254018]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.157631

Running PBIRL with 4 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.3356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.30206919  0.31437967  0.89995535]
True reward weights: [-0.04248878 -0.03602747  0.99844716]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.147262

Running PBIRL with 5 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.3140
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.36783071 -0.03584062  0.92920182]
True reward weights: [-0.81547967 -0.14249112  0.56097165]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.150733

Running PBIRL with 6 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.3150
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.53413515  0.31922671  0.78281157]
True reward weights: [0.0598829  0.5209359  0.85149271]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.170442

Running experiment 20/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (3, 3)], [(0, 2), (0, 1), (3, 0), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 2), (3, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (0, 2)]), ([(0, 1), (3, 0), (0, 1), (3, 3)], [(0, 1), (3, 0), (0, 0), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 3), (4, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 2), (0, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 20
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.25597407 0.63878397 0.72555656]
True reward weights: [-0.61012931 -0.3129562   0.72787406]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6598
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.02763516 0.31905952 0.94733169]
True reward weights: [ 0.93372753 -0.2702111   0.23481667]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183155

Running PBIRL with 3 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6478
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.66281169  0.31893473  0.67746683]
True reward weights: [-0.8210431  -0.55994944 -0.11110736]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170755

Running PBIRL with 4 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6470
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.18515744  0.31989735  0.92918373]
True reward weights: [-0.93382089 -0.29495716 -0.20243224]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.219542

Running PBIRL with 5 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6582
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.52203085 0.3195244  0.79081474]
True reward weights: [-0.05044646 -0.471928    0.88019266]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.216185

Running PBIRL with 6 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5262
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.31335357 0.15948984 0.93614771]
True reward weights: [-0.64490495  0.03846132  0.7632944 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.253369

Saving results to files...
Results saved successfully.
Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [([(0, 3), (1, 1), (4, 3), (1, 1)], [(0, 3), (1, 3), (2, 0), (2, 3)]), ([(0, 1), (3, 3), (3, 3), (3, 3)], [(0, 2), (0, 3), (1, 2), (0, 2)]), ([(0, 1), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 1), (0, 3), (1, 3), (2, 1)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 0), (2, 2), (1, 2)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 0), (0, 1), (3, 3), (4, 0)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 3), (2, 2)]), ([(0, 1), (3, 0), (0, 1), (3, 3)], [(0, 1), (3, 0), (0, 2), (0, 3)]), ([(0, 1), (0, 1), (3, 3), (4, 3)], [(0, 3), (1, 0), (1, 0), (1, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 2), (0, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6532
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.57240297 -0.31413234  0.75741383]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.0208
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.48235253 -0.48204704  0.73141417]
True reward weights: [-0.04364952 -0.07276881  0.99639321]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.312670

Running PBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.0206
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.70492894 -0.70481885  0.07940769]
True reward weights: [ 0.69552335  0.69233011 -0.19216215]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.313719

Running PBIRL with 4 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.0206
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.18079455 -0.18077844  0.96676393]
True reward weights: [-0.55675109 -0.54768322  0.62455689]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.289055

Running PBIRL with 5 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.0192
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.12427634 0.12432907 0.98442759]
True reward weights: [-0.04220475 -0.01644412  0.99897365]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.290876

Running PBIRL with 6 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.0142
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.50058678 0.49371634 0.71109567]
True reward weights: [-0.48045976 -0.48962464  0.72761675]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.349107

Running PBIRL with 7 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.0164
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.28197998 -0.28890425  0.91488886]
True reward weights: [0.06376558 0.04661337 0.99687569]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.339026

Running PBIRL with 8 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.0152
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.55148261 -0.55895578  0.61922158]
True reward weights: [0.15272141 0.11374227 0.98170203]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.410041

Running PBIRL with 9 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.0098
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.45119357 -0.44750281  0.77211761]
True reward weights: [-0.09712275 -0.13059151  0.98666764]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.546504

Running PBIRL with 10 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.0086
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.58533215 -0.58059985  0.56594177]
True reward weights: [-0.27370642 -0.26700308  0.92400982]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.532939

Running experiment 2/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 2), (1, 3)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 2)]), ([(0, 1), (1, 1), (2, 1), (5, None)], [(0, 1), (1, 1), (2, 0), (2, 3)]), ([(0, 2), (0, 1), (3, 3), (0, 1)], [(0, 2), (0, 1), (3, 2), (3, 1)]), ([(0, 0), (0, 2), (0, 1), (3, 3)], [(0, 0), (0, 2), (0, 3), (1, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 3), (0, 0)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 2), (0, 1)]), ([(0, 2), (0, 0), (0, 1), (3, 3)], [(0, 2), (0, 0), (0, 2), (0, 1)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 3), (1, 0), (0, 3), (3, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 0), (0, 3), (1, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6606
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.55889164  0.17052981 -0.81151692]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.5106
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.79702858  0.18923717  0.57352832]
True reward weights: [-0.57977652 -0.73255088 -0.35669088]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.150210

Running PBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.0731565   0.12627265  0.98929437]
True reward weights: [-0.72239678  0.50879109  0.46826757]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.165715

Running PBIRL with 4 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4210
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.22874795 0.05928184 0.97167898]
True reward weights: [0.4335745  0.37824615 0.81788936]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.178483

Running PBIRL with 5 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.6615479   0.31983306  0.67827811]
True reward weights: [-0.9202457  -0.38654498  0.06108057]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.208033

Running PBIRL with 6 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3126
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.80510546  0.15844842  0.57157615]
True reward weights: [-0.80370339  0.39692013  0.44330044]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.196790

Running PBIRL with 7 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3184
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.3827353   0.31914994  0.86698155]
True reward weights: [-0.80495247 -0.5676379   0.17273891]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.188802

Running PBIRL with 8 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.43680653  0.31978899  0.8407943 ]
True reward weights: [-0.93574343  0.2268707   0.27002579]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.238225

Running PBIRL with 9 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3246
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.78349503 -0.38318084  0.48919116]
True reward weights: [0.05654278 0.11850621 0.99134212]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.304614

Running PBIRL with 10 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3132
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.72860662 -0.35208803  0.58750865]
True reward weights: [0.36152935 0.58143492 0.72885524]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.301858

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Shuffled Demos: [([(0, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 2), (3, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 0), (1, 3)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 0), (1, 0), (1, 0), (1, 1)]), ([(0, 3), (3, 3), (4, 3), (1, 1)], [(0, 3), (3, 2), (3, 2), (3, 0)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 1), (4, 0)]), ([(0, 2), (0, 1), (3, 3), (0, 1)], [(0, 2), (0, 0), (1, 2), (0, 2)]), ([(0, 2), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 3), (1, 0)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 1), (3, 1), (3, 3), (4, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 1), (3, 3)]), ([(0, 1), (1, 0), (0, 1), (3, 3)], [(0, 1), (1, 0), (0, 2), (0, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6460
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.38653553 -0.85460723  0.34675174]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4956
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.2381765   0.55702133  0.79561246]
True reward weights: [-0.92536919 -0.27170799  0.26432297]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.151152

Running PBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4342
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.58954438  0.10119707  0.80137168]
True reward weights: [-0.35391439  0.92566899  0.13372184]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154917

Running PBIRL with 4 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.0132
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.39387793 0.39869322 0.82819315]
True reward weights: [-0.94250893  0.18659421  0.27723548]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.386900

Running PBIRL with 5 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.0144
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.1833897  -0.17919136  0.96657057]
True reward weights: [-0.01364959  0.01662589  0.99976861]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.326838

Running PBIRL with 6 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.0140
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.3683998  0.37538833 0.85050878]
True reward weights: [-0.59952462 -0.5985075   0.53137463]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.346072

Running PBIRL with 7 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.0152
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.44775652 -0.43996905  0.77842233]
True reward weights: [-0.38177149 -0.37866222  0.84312837]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.332459

Running PBIRL with 8 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.0140
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.51841496 -0.5109661   0.68568183]
True reward weights: [0.10372854 0.09188866 0.99035189]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.404602

Running PBIRL with 9 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.0108
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.32748177 -0.32022645  0.88893797]
True reward weights: [0.57575587 0.62854917 0.52290642]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.397385

Running PBIRL with 10 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.0126
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [0.62612123 0.63324461 0.45494337]
True reward weights: [0.39964417 0.40151857 0.82405545]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.474359

Running experiment 4/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 1)]), ([(0, 3), (1, 0), (1, 1), (0, 1)], [(0, 3), (1, 0), (1, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (1, 2), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 3), (0, 2), (0, 3)]), ([(0, 2), (0, 0), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (1, 2), (1, 0)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (1, 3), (2, 3)]), ([(0, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 2), (3, 2)]), ([(0, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 1), (3, 0)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 3), (0, 0), (0, 1), (3, 2)]), ([(0, 1), (1, 1), (2, 1), (5, None)], [(0, 1), (1, 3), (2, 3), (2, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6616
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.47169533  0.86954159 -0.14629062]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6668
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.48879015 0.7904058  0.36924634]
True reward weights: [-0.60283043  0.78848047 -0.12204108]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147980

Running PBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.20624808  0.84339257  0.49613576]
True reward weights: [0.66234649 0.74356328 0.09171034]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.164014

Running PBIRL with 4 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4378
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.19884963  0.0782957   0.97689744]
True reward weights: [-0.5361779  -0.72042008  0.43989564]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.203968

Running PBIRL with 5 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4380
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.28776031 -0.20442274  0.93563099]
True reward weights: [-0.00495699 -0.63231665  0.77469418]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.190776

Running PBIRL with 6 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4346
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.1588275   0.47783608  0.86397136]
True reward weights: [-0.39668004 -0.3695384   0.84028942]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.182764

Running PBIRL with 7 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4230
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.25920068  0.41093771  0.87403959]
True reward weights: [-0.91002051 -0.37671583  0.17305446]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.183024

Running PBIRL with 8 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4400
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.48271978 0.15503611 0.86194281]
True reward weights: [0.68140164 0.04658147 0.73042588]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.178737

Running PBIRL with 9 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4362
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.38935849  0.31861709  0.864224  ]
True reward weights: [-0.34240769 -0.28010828  0.89682569]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.229989

Running PBIRL with 10 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4156
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.23983032  0.31922821  0.91682865]
True reward weights: [-0.07410778 -0.34901812  0.93418113]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.227425

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (1, 1), (2, 2), (1, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 3), (3, 0)]), ([(0, 2), (0, 2), (0, 1), (3, 3)], [(0, 2), (0, 2), (0, 2), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 1), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 1), (3, 2)]), ([(0, 1), (3, 0), (0, 1), (3, 3)], [(0, 1), (3, 0), (0, 2), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 3)]), ([(0, 1), (0, 1), (3, 3), (4, 3)], [(0, 2), (0, 2), (0, 1), (3, 2)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 1), (1, 0), (0, 2), (0, 0)]), ([(0, 1), (3, 0), (4, 3), (4, 3)], [(0, 1), (3, 0), (4, 0), (1, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6578
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.92765008 -0.31001071 -0.20822748]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4332
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.35264482 -0.44977788  0.82057388]
True reward weights: [-0.38740369 -0.71073566  0.58717391]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.157493

Running PBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4558
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.29516085 0.31915666 0.90056599]
True reward weights: [ 0.32708679 -0.20511561  0.92246508]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.169974

Running PBIRL with 4 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4424
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.02753622  0.3191757   0.94729543]
True reward weights: [-0.49957853  0.33730319  0.79790216]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.164631

Running PBIRL with 5 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4426
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.95391657 -0.07550748  0.2904166 ]
True reward weights: [-0.23677745  0.51022939  0.82680252]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.160886

Running PBIRL with 6 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4422
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.44099751  0.15902853  0.88330693]
True reward weights: [-0.15969866  0.40257984  0.90134666]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.214220

Running PBIRL with 7 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4442
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.55730759  0.31997253  0.76617611]
True reward weights: [-0.99072821  0.01229206  0.13530156]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.212504

Running PBIRL with 8 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4292
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.75714504 -0.05125801  0.65123268]
True reward weights: [-0.99808246 -0.06182416 -0.00303028]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.281673

Running PBIRL with 9 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.4326
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.29276073 -0.06541954  0.9539452 ]
True reward weights: [0.25610456 0.08058775 0.963284  ]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.280996

Running PBIRL with 10 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.3362
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.61877508  0.31844191  0.71813101]
True reward weights: [-0.76704971 -0.64097457 -0.02804177]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.324526

Running experiment 6/50...
Shuffled Demos: [([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 1), (3, 3), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 1), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 3), (1, 0)]), ([(0, 0), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 0), (1, 3)]), ([(0, 1), (0, 0), (0, 1), (3, 3)], [(0, 1), (0, 0), (0, 0), (0, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 3), (1, 1)]), ([(0, 2), (0, 1), (3, 3), (0, 1)], [(0, 2), (0, 3), (1, 1), (4, 0)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 2), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 0), (2, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 2), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6698
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.28998555 -0.5361351   0.79275944]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.0424058  -0.6468796   0.76141219]
True reward weights: [0.12029022 0.87089693 0.47651737]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.140218

Running PBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4902
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.49346978 0.20974075 0.84409502]
True reward weights: [-0.32466265  0.203055    0.92377639]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151953

Running PBIRL with 4 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4320
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.73117078  0.28866397  0.61811197]
True reward weights: [ 0.01565046 -0.50818098  0.86110809]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.161619

Running PBIRL with 5 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4192
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.3321708   0.04590003  0.94210177]
True reward weights: [ 0.39798988 -0.31930014  0.86002993]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.177994

Running PBIRL with 6 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4278
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.69285348  0.15843193  0.70345816]
True reward weights: [-0.8543234  -0.39062348  0.34284811]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.172557

Running PBIRL with 7 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3106
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.7024919   0.15995653  0.69348327]
True reward weights: [-0.38746341  0.31081149  0.86791032]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.213022

Running PBIRL with 8 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3172
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.63081995  0.31964069  0.70703325]
True reward weights: [-0.94824332  0.20104098  0.24579897]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.203425

Running PBIRL with 9 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3156
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.46376509  0.15870199  0.87162814]
True reward weights: [0.2877445  0.60961883 0.73862574]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.195864

Running PBIRL with 10 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.2982
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.52484269  0.3186632   0.78929964]
True reward weights: [0.12457161 0.36154551 0.923995  ]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.190560

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 0), (0, 1)]), ([(0, 1), (3, 3), (3, 3), (4, 3)], [(0, 1), (3, 1), (3, 3), (4, 0)]), ([(0, 3), (3, 3), (4, 3), (5, None)], [(0, 3), (3, 0), (0, 1), (3, 2)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 0), (1, 0), (1, 0), (0, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 3), (0, 1), (3, 2)]), ([(0, 2), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 1), (3, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (4, 0)]), ([(0, 1), (3, 3), (0, 1), (0, 1)], [(0, 2), (0, 0), (0, 3), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (1, 2), (4, 1)]), ([(0, 1), (1, 1), (4, 3), (1, 1)], [(0, 2), (0, 3), (0, 1), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6700
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.42107304 -0.80093212  0.42568208]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6506
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.76454305 0.31904611 0.56007456]
True reward weights: [-0.35302687  0.53565932  0.76709916]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148153

Running PBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.5488
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.81182848 0.15849515 0.56197314]
True reward weights: [0.04925978 0.70270558 0.70977344]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.171841

Running PBIRL with 4 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.2972
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.64763982  0.31945353  0.6917457 ]
True reward weights: [-0.33835503 -0.86117538  0.3793321 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.235728

Running PBIRL with 5 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3130
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.63062831  0.31845954  0.70773685]
True reward weights: [-0.36237768  0.2403647   0.90050387]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.268211

Running PBIRL with 6 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3102
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.14708074  0.15847833  0.97634619]
True reward weights: [0.23338702 0.41526814 0.87925131]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.255421

Running PBIRL with 7 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3208
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [0.03977399 0.31997417 0.94659102]
True reward weights: [-0.51929637  0.16983777  0.83754786]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.246833

Running PBIRL with 8 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3110
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.73322693  0.15879218  0.66118326]
True reward weights: [-0.97051174  0.07736083  0.22830303]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.240916

Running PBIRL with 9 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3060
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.74219548 -0.06642922  0.66688307]
True reward weights: [-0.68717698 -0.01307584  0.72637237]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.236198

Running PBIRL with 10 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.0094
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.3087423  -0.30655241  0.90039092]
True reward weights: [0.33621459 0.42178644 0.8420546 ]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.738137

Running experiment 8/50...
Shuffled Demos: [([(0, 2), (0, 0), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (1, 0), (1, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (1, 1), (4, 0), (1, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 3), (4, 1), (4, 3)]), ([(0, 2), (3, 0), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 0), (3, 1), (3, 3)]), ([(0, 2), (0, 1), (3, 3), (4, 3)], [(0, 2), (0, 3), (0, 2), (0, 0)]), ([(0, 0), (0, 1), (3, 3), (0, 1)], [(0, 0), (0, 0), (0, 3), (1, 0)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 3), (1, 2)]), ([(0, 1), (0, 1), (3, 3), (3, 3)], [(0, 3), (1, 1), (0, 1), (3, 1)]), ([(0, 2), (0, 0), (0, 1), (3, 3)], [(0, 2), (0, 0), (0, 0), (0, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6448
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.00428336  0.96962334 -0.24456539]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6128
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.57837397  0.73916158 -0.34514304]
True reward weights: [-0.73547548 -0.09183531  0.67129881]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.131234

Running PBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4072
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.41736075 -0.45751202  0.78517053]
True reward weights: [-0.86574456  0.15355089  0.47634912]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.177172

Running PBIRL with 4 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4054
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.07850044  0.18380632  0.9798229 ]
True reward weights: [-0.62606501 -0.3716548   0.68550369]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.166678

Running PBIRL with 5 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4086
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.18895025 0.15880155 0.96906133]
True reward weights: [0.0012483  0.70653619 0.70767581]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.181617

Running PBIRL with 6 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3086
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.22657639  0.3190029   0.92027186]
True reward weights: [-0.99589024  0.06693539  0.06101059]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.210681

Running PBIRL with 7 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3136
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.44807178  0.15963108  0.87963037]
True reward weights: [-0.71688656  0.11431797  0.68775364]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.199565

Running PBIRL with 8 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3126
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.20474206  0.31904505  0.9253599 ]
True reward weights: [-0.31534451 -0.31048601  0.89674761]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.191826

Running PBIRL with 9 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3124
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.73792513  0.04299715  0.6735115 ]
True reward weights: [-0.56563288  0.02239557  0.82435301]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.187459

Running PBIRL with 10 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.34448385  0.31931857  0.88281738]
True reward weights: [-0.78173549 -0.32480223  0.53234681]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.238320

Saving results to files...
Results saved successfully.

Running experiment 9/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 3), (4, 0)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 1), (1, 1), (4, 1), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 2), (0, 0)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 2), (0, 3)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 1), (3, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 3), (4, 0)]), ([(0, 0), (0, 1), (0, 1), (3, 3)], [(0, 0), (0, 3), (1, 2), (1, 2)]), ([(0, 2), (0, 2), (0, 1), (3, 3)], [(0, 2), (0, 2), (0, 0), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 0), (0, 1), (1, 1)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (0, 3), (1, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6630
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.33557119  0.4655097   0.8189583 ]
True reward weights: [-0.0505111  -0.01531811  0.99860602]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6646
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.20830625 -0.94915233  0.23604736]
True reward weights: [ 7.65848334e-01 -4.24651643e-05  6.43021249e-01]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124013

Running PBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6698
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.96062933 -0.24798707  0.12527455]
True reward weights: [ 0.06293948 -0.7460461   0.66291315]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124110

Running PBIRL with 4 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5594
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.54086517 -0.7672141   0.3447425 ]
True reward weights: [ 0.78170015 -0.60115842 -0.16599224]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.148490

Running PBIRL with 5 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.26448336 -0.5636973   0.78249212]
True reward weights: [-0.700419   -0.48570488  0.52297609]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.143434

Running PBIRL with 6 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5540
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.61471792 -0.62932728  0.47546719]
True reward weights: [-0.27458497 -0.91315814  0.30123961]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.140158

Running PBIRL with 7 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3082
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.65900754  0.00837445  0.75208971]
True reward weights: [-0.21470861 -0.952825    0.21453373]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.226884

Running PBIRL with 8 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3120
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.64556738  0.31888742  0.69394061]
True reward weights: [0.22367101 0.64876242 0.7273779 ]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.232302

Running PBIRL with 9 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3042
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.18143384  0.31921914  0.93015101]
True reward weights: [-0.16033106 -0.12008005  0.97973197]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.220071

Running PBIRL with 10 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3142
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.59273336 -0.0708554   0.80227594]
True reward weights: [-0.95158463 -0.10998822  0.28703534]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.210618

Running experiment 10/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 1), (3, 3), (4, 3)], [(0, 1), (3, 1), (3, 1), (3, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 3), (2, 0)]), ([(0, 0), (1, 1), (0, 1), (3, 3)], [(0, 0), (1, 2), (0, 1), (3, 3)]), ([(0, 1), (1, 3), (4, 3), (5, None)], [(0, 1), (1, 3), (4, 0), (3, 1)]), ([(0, 3), (1, 2), (0, 1), (0, 1)], [(0, 3), (1, 2), (0, 0), (0, 3)]), ([(0, 3), (1, 2), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 2), (1, 2), (0, 3)]), ([(0, 3), (1, 2), (4, 3), (5, None)], [(0, 3), (1, 2), (4, 0), (1, 0)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 1), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6606
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.73283059 -0.46217753  0.49935083]
True reward weights: [-0.76518493 -0.64328377  0.02603879]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6542
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.55678289  0.31832816  0.76724181]
True reward weights: [0.91277874 0.21403569 0.3478846 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148002

Running PBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6480
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.45632625 0.31952271 0.83046468]
True reward weights: [ 0.80323558 -0.58338621  0.12030435]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148444

Running PBIRL with 4 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5174
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.91722976  0.05767317  0.39416161]
True reward weights: [0.24318361 0.1663708  0.95560582]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.183558

Running PBIRL with 5 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5194
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.06233018  0.31837796  0.94591248]
True reward weights: [-0.84096148 -0.50648591  0.19040957]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.230768

Running PBIRL with 6 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5364
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.40103202 0.29124437 0.86853327]
True reward weights: [-0.20882803  0.50869107  0.83523904]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.226031

Running PBIRL with 7 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5340
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.29994845  0.15879902  0.94064542]
True reward weights: [ 0.08241442 -0.90179337  0.42423647]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.292916

Running PBIRL with 8 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4966
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.14643098  0.26112311  0.95413452]
True reward weights: [-0.5935258  -0.74069638 -0.31479517]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.303404

Running PBIRL with 9 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.15066767  0.47657815  0.866125  ]
True reward weights: [-0.09986412  0.41449841  0.90455416]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.300050

Running PBIRL with 10 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5036
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.19550015  0.49092328  0.84898411]
True reward weights: [-0.60811092 -0.03494144  0.79308272]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.297522

Saving results to files...
Results saved successfully.

Running experiment 11/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 1), (4, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (3, 0), (3, 1), (3, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 0)]), ([(0, 3), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 3), (2, 0), (2, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (1, 1), (4, 2), (3, 3)]), ([(0, 1), (3, 3), (3, 3), (0, 1)], [(0, 0), (0, 1), (3, 2), (3, 1)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 2), (0, 1), (0, 1), (3, 3)], [(0, 2), (0, 2), (0, 3), (1, 2)]), ([(0, 2), (3, 3), (0, 1), (3, 3)], [(0, 2), (3, 2), (3, 1), (4, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6482
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.71092295 -0.54240859  0.4476399 ]
True reward weights: [-0.87829137 -0.24439244  0.41094598]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6550
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.35525614 -0.898682    0.25722313]
True reward weights: [-0.73024177 -0.67758809 -0.08730029]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123972

Running PBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6516
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.09989688 -0.97368649 -0.20482976]
True reward weights: [-0.46166482 -0.8824231   0.09052655]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123995

Running PBIRL with 4 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6406
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.66972976 -0.45381136  0.5878072 ]
True reward weights: [ 0.75027999 -0.59296406  0.29235861]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.123970

Running PBIRL with 5 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5256
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.4934953   0.25730932  0.83081545]
True reward weights: [-0.4467333  -0.88019692 -0.16025834]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.161645

Running PBIRL with 6 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5374
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.38344212  0.31963211  0.86649146]
True reward weights: [-0.32897468 -0.9443371   0.0017645 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.179139

Running PBIRL with 7 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.4942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.35451875  0.15895413  0.92143911]
True reward weights: [-0.97063896  0.06564049  0.23141162]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.187372

Running PBIRL with 8 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3056
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.78328493  0.06015785  0.61874531]
True reward weights: [0.33804615 0.25806199 0.90505735]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.246721

Running PBIRL with 9 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3098
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.40829488  0.31897364  0.85530761]
True reward weights: [-0.74608054 -0.63886696  0.18765083]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.232240

Running PBIRL with 10 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3110
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.6186732   0.15900583  0.76938977]
True reward weights: [0.18745235 0.2758814  0.94273595]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.276024

Running experiment 12/50...
Shuffled Demos: [([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (3, 1), (3, 3)]), ([(0, 3), (0, 1), (3, 3), (0, 1)], [(0, 3), (0, 2), (0, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 0), (0, 2)]), ([(0, 1), (3, 1), (3, 3), (0, 1)], [(0, 1), (3, 1), (3, 0), (0, 2)]), ([(0, 3), (3, 3), (4, 3), (1, 1)], [(0, 3), (3, 3), (4, 1), (4, 1)]), ([(0, 1), (3, 3), (3, 3), (0, 1)], [(0, 2), (0, 0), (0, 2), (0, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 3), (4, 1)]), ([(0, 1), (3, 3), (0, 1), (1, 1)], [(0, 3), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (4, 2), (1, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6532
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.14393008  0.93505331  0.32397444]
True reward weights: [-0.8497926  -0.50850291  0.13884283]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123671

Running PBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6706
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.56062741 0.63931247 0.52628554]
True reward weights: [0.32713417 0.80585913 0.49353247]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148188

Running PBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5764
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.3494817  -0.04808068  0.93570871]
True reward weights: [-0.36000462 -0.75214713  0.55197043]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170916

Running PBIRL with 4 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5640
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.26915767 0.31851877 0.9089004 ]
True reward weights: [0.26717374 0.09693846 0.9587602 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.165402

Running PBIRL with 5 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5722
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.68357322 -0.05894976  0.72749747]
True reward weights: [ 0.87973673 -0.31650638  0.35480556]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.216706

Running PBIRL with 6 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4332
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.8035895 0.1590988 0.5735255]
True reward weights: [-0.90907824 -0.38958248  0.14765587]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.260367

Running PBIRL with 7 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4256
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [ 0.92115228 -0.06041506  0.38448472]
True reward weights: [ 0.25669726 -0.10112421  0.96118698]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.320989

Running PBIRL with 8 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4296
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.52234208 0.15843689 0.83788812]
True reward weights: [ 0.5397578  -0.62175735  0.56752032]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.314180

Running PBIRL with 9 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.2172
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [0.86243042 0.06835659 0.50153877]
True reward weights: [ 0.86535459 -0.32417079  0.38219725]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.407831

Running PBIRL with 10 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.0290
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [0.73812076 0.36701789 0.56610566]
True reward weights: [ 0.55014182 -0.65674116  0.51578583]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.539506

Saving results to files...
Results saved successfully.

Running experiment 13/50...
Shuffled Demos: [([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 1), (4, 1)]), ([(0, 1), (0, 1), (3, 3), (4, 3)], [(0, 1), (0, 3), (0, 2), (0, 1)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 1), (4, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 1), (4, 3)]), ([(0, 0), (0, 0), (0, 1), (1, 1)], [(0, 0), (0, 0), (0, 2), (0, 2)]), ([(0, 0), (0, 1), (3, 3), (0, 1)], [(0, 0), (0, 2), (0, 1), (3, 2)]), ([(0, 1), (0, 1), (0, 1), (3, 3)], [(0, 2), (0, 0), (0, 0), (0, 2)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (0, 3)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (0, 3)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 0), (2, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6590
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.45245349 0.42506314 0.78396886]
True reward weights: [-0.34477193 -0.32328333  0.88126058]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6484
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.51373426  0.63979726 -0.57160877]
True reward weights: [-0.59520219 -0.27517571  0.75499184]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147896

Running PBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6280
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.64695918  0.63842545  0.41696136]
True reward weights: [-0.85965825  0.34954765  0.37256427]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155616

Running PBIRL with 4 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3824
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.56811422  0.31966895  0.75832579]
True reward weights: [-0.983656   -0.16444417 -0.07334164]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.218793

Running PBIRL with 5 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.1202
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.05312897 -0.26619499  0.96245392]
True reward weights: [-0.53556237  0.53300568  0.65504038]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.293735

Running PBIRL with 6 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.1196
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.15100134 -0.16607374  0.97448351]
True reward weights: [-0.64513914 -0.72455828  0.24250935]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.311487

Running PBIRL with 7 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.1258
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [ 0.19071113 -0.13346414  0.97253102]
True reward weights: [ 0.08545271 -0.20019639  0.97602215]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.360277

Running PBIRL with 8 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.1282
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [ 0.16942022 -0.15445856  0.97336496]
True reward weights: [-0.68563298 -0.65740487  0.31261199]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.346086

Running PBIRL with 9 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.1216
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [ 0.26704043 -0.05274059  0.96224105]
True reward weights: [ 0.05129509 -0.26647316  0.96247643]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.336262

Running PBIRL with 10 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.1192
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [ 0.1778715  -0.14253614  0.97367612]
True reward weights: [-0.35452586 -0.64109128  0.68067127]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.328977

Running experiment 14/50...
Shuffled Demos: [([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 1), (3, 2), (0, 1), (3, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (0, 2), (0, 0)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 2)]), ([(0, 1), (3, 1), (3, 3), (4, 3)], [(0, 1), (3, 1), (3, 0), (0, 1)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 1), (3, 1)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 3), (3, 3), (4, 1), (4, 2)]), ([(0, 3), (1, 0), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 3), (2, 1)]), ([(0, 2), (3, 3), (4, 3), (1, 1)], [(0, 2), (3, 3), (4, 0), (1, 1)]), ([], [(0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 3), (4, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.8150588  0.31867237 0.4838668 ]
True reward weights: [-0.43088031 -0.17122291  0.88601629]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.6572
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.35471777  0.31951457  0.8786841 ]
True reward weights: [ 0.61297096 -0.21239144  0.76102331]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.181877

Running PBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5734
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.15928384 0.1584873  0.97442826]
True reward weights: [ 0.33914544 -0.58020906  0.74049836]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.189973

Running PBIRL with 4 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5704
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.00151102 0.15983409 0.98714274]
True reward weights: [ 0.44824027 -0.27677623  0.84998563]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.233399

Running PBIRL with 5 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5490
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.16524794 0.31895338 0.93325338]
True reward weights: [-0.87316594 -0.07625223  0.48142169]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.227230

Running PBIRL with 6 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.13647857  0.07509022  0.98779302]
True reward weights: [-0.55808248 -0.70814133  0.43252723]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.242430

Running PBIRL with 7 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5074
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [0.6918962  0.31857037 0.64791401]
True reward weights: [ 0.16266633 -0.88030374  0.4456512 ]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.236307

Running PBIRL with 8 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4998
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.03045373 -0.0500161   0.99828401]
True reward weights: [-0.27857919  0.30343818  0.91121836]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.302043

Running PBIRL with 9 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.3558
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [ 0.77554605 -0.34044896  0.53162282]
True reward weights: [-0.51100601 -0.52883111  0.67765073]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.347310

Running PBIRL with 10 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.3722
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [ 0.76521799 -0.34462204  0.54376197]
True reward weights: [-0.64974238 -0.75523825  0.08631348]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.337779

Saving results to files...
Results saved successfully.

Running experiment 15/50...
Shuffled Demos: [([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 2), (0, 2)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 1), (3, 2)]), ([(0, 3), (1, 2), (4, 3), (5, None)], [(0, 3), (1, 2), (4, 1), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 0), (1, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 3), (1, 2)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 2), (3, 0)]), ([(0, 2), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 2), (0, 3), (0, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (3, 0), (0, 3), (1, 3)]), ([(0, 1), (0, 1), (3, 3), (4, 3)], [(0, 2), (0, 1), (3, 0), (0, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.6634
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.41318445  0.40360396  0.81632252]
True reward weights: [-0.75433776 -0.02805593  0.65588674]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5580
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.5402016  -0.6547151   0.52870632]
True reward weights: [-0.5373202   0.82432666  0.17824856]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.139189

Running PBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.4834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.76704695 -0.07192113  0.63754711]
True reward weights: [ 0.50555168 -0.40591463  0.76134802]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154194

Running PBIRL with 4 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5024
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.62953805 -0.26492851  0.73040724]
True reward weights: [ 0.41366668 -0.0943514   0.9055262 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.146070

Running PBIRL with 5 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3248
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.4220565   0.08317262  0.90274616]
True reward weights: [-0.14473171 -0.08364985  0.98592872]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.202075

Running PBIRL with 6 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3168
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.20187366  0.47291297  0.85767147]
True reward weights: [-0.90169056 -0.43091953  0.03553157]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.183991

Running PBIRL with 7 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3084
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.730262    0.23540746  0.64132732]
True reward weights: [-0.08250428  0.27868587  0.95683187]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.174116

Running PBIRL with 8 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3070
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.75511398  0.01297382  0.65546514]
True reward weights: [-0.17680625  0.3097022   0.93425055]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.168637

Running PBIRL with 9 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3116
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.71838168 -0.26790804  0.64199147]
True reward weights: [-0.65797748  0.27831516  0.69971873]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.161873

Running PBIRL with 10 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3118
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.61332718  0.31959957  0.72227826]
True reward weights: [-0.40136439  0.24307661  0.88307439]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.182373

Running experiment 16/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 3), (3, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 0), (0, 3)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 0), (0, 2)]), ([(0, 2), (0, 3), (0, 1), (3, 3)], [(0, 2), (0, 3), (0, 2), (0, 0)]), ([(0, 0), (0, 1), (1, 1), (2, 1), (5, None)], [(0, 0), (0, 3), (1, 1), (4, 0)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (1, 3), (2, 0)]), ([(0, 3), (1, 2), (1, 1), (2, 1)], [(0, 3), (1, 2), (1, 2), (0, 2)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 3), (1, 2), (0, 3), (1, 3)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 2), (0, 0), (0, 1), (3, 1)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 1), (3, 1), (4, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.6588
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.293398    0.14438065  0.94502478]
True reward weights: [-0.92032338 -0.11683579  0.37330186]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5718
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.8183314  -0.03144247  0.57388596]
True reward weights: [-0.00382769 -0.98154991  0.1911678 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.137074

Running PBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5538
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.42180259 -0.62329276  0.65847453]
True reward weights: [ 0.19075178 -0.1855967   0.96393341]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.132681

Running PBIRL with 4 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5650
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.15623498  0.31918923  0.93472395]
True reward weights: [-0.73504429  0.14080757  0.66323685]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.154901

Running PBIRL with 5 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5608
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.48840348  0.31990319  0.81186451]
True reward weights: [-0.29261507 -0.33086542  0.8971647 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.153916

Running PBIRL with 6 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4374
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.33044734 0.31878222 0.88835942]
True reward weights: [-0.60801692  0.04231099  0.79279581]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.196238

Running PBIRL with 7 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4382
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [0.18423007 0.45278203 0.87238049]
True reward weights: [-0.88747108  0.29722169  0.3522135 ]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.243338

Running PBIRL with 8 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.3230
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.50132736  0.31854379  0.80448787]
True reward weights: [-0.54164793  0.28927326  0.78926453]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.285208

Running PBIRL with 9 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.3116
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [0.13167437 0.50680966 0.85194239]
True reward weights: [-0.10347468 -0.02819256  0.99423245]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.343535

Running PBIRL with 10 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.3246
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.52983331  0.31948591  0.78562422]
True reward weights: [-0.51840151  0.01556745  0.85499563]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.334635

Saving results to files...
Results saved successfully.

Running experiment 17/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 1), (3, 3)], [(0, 1), (3, 0), (0, 3), (1, 0)]), ([(0, 2), (0, 2), (0, 1), (3, 3)], [(0, 2), (0, 2), (0, 3), (1, 0)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 2), (0, 0), (1, 0), (1, 1)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 1)]), ([(0, 0), (1, 1), (2, 1), (1, 1)], [(0, 0), (1, 0), (0, 1), (3, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 0), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 1), (3, 2), (3, 1), (3, 3)]), ([(0, 2), (0, 0), (0, 1), (3, 3)], [(0, 2), (0, 0), (0, 3), (1, 3)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 2), (0, 1), (3, 2), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6690
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.22262059 0.9692473  0.10487965]
True reward weights: [-0.76208548 -0.28664819  0.58056743]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6630
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.8093339  -0.03444085 -0.58633818]
True reward weights: [ 0.56893852  0.82122226 -0.04362287]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124013

Running PBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6692
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.88015091 -0.20839763 -0.426503  ]
True reward weights: [-0.73947389  0.23452496 -0.63101221]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124094

Running PBIRL with 4 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.4196
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.9249559   0.08776781  0.36980183]
True reward weights: [ 0.12121587  0.82125103 -0.55754233]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.182185

Running PBIRL with 5 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.0174
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.32092245 0.33136828 0.88724509]
True reward weights: [-0.57299114  0.55564039  0.6024491 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.512772

Running PBIRL with 6 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.0162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.61549608 0.62883464 0.47511217]
True reward weights: [-0.30953142 -0.29157254  0.90508329]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.481176

Running PBIRL with 7 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.0132
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.06476572 -0.05156394  0.99656739]
True reward weights: [0.4665769  0.48550831 0.73931568]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.450735

Running PBIRL with 8 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.0076
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.32851574 -0.32113808  0.88822731]
True reward weights: [-0.55513496 -0.53131964  0.6399411 ]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.605187

Running PBIRL with 9 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.0112
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.6363178  -0.62681739  0.44966611]
True reward weights: [-0.67410873 -0.67985103  0.28875594]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.628775

Running PBIRL with 10 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.0048
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.09237077 -0.08606352  0.99199834]
True reward weights: [-0.7052812  -0.70018139  0.11101556]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.764455

Running experiment 18/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 0), (2, 2), (1, 0)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 1), (4, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 2), (0, 2)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 2), (0, 1)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (0, 0)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 1), (3, 0)]), ([], [(0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 3), (3, 3), (4, 3), (1, 1)], [(0, 3), (3, 0), (0, 2), (0, 3)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 0), (0, 1)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (3, 3), (4, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6606
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.50591434 -0.72647829  0.46505911]
True reward weights: [-0.39174849 -0.30341429  0.86860399]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6254
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.92697093  0.37350773 -0.0348836 ]
True reward weights: [-0.98082492  0.14115529  0.13437878]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.130982

Running PBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4096
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.45412748  0.02439914  0.89060256]
True reward weights: [-0.75374423 -0.34574389  0.55886563]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.178025

Running PBIRL with 4 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4116
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.85628581 -0.31989982  0.40551044]
True reward weights: [-0.8454613  -0.47862888 -0.2368746 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.166775

Running PBIRL with 5 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.3964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.00355789  0.45699532  0.88946198]
True reward weights: [-0.16298471 -0.72896735  0.66486283]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.156900

Running PBIRL with 6 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4056
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.72195396 -0.60938536  0.32776817]
True reward weights: [0.11745433 0.09610205 0.98841736]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.151057

Running PBIRL with 7 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.2002
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.67673929 -0.61353362  0.40694032]
True reward weights: [-0.08782125  0.65492808  0.75057087]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.236138

Running PBIRL with 8 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.1134
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.22650053 -0.5628028   0.79495315]
True reward weights: [-0.12416966 -0.63096404  0.76581086]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.273855

Running PBIRL with 9 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.1112
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [ 0.05348644 -0.43767459  0.89754117]
True reward weights: [-0.49285816 -0.50630591  0.70763349]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.244863

Running PBIRL with 10 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.1222
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.26645602 -0.64126782  0.71956708]
True reward weights: [-0.50040534 -0.7998866   0.33132451]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.225807

Saving results to files...
Results saved successfully.

Running experiment 19/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 2), (0, 2)]), ([(0, 1), (3, 2), (3, 3), (3, 3)], [(0, 1), (3, 2), (3, 0), (0, 1)]), ([(0, 3), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 3), (2, 3), (2, 1)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 0), (1, 1)]), ([(0, 0), (1, 3), (2, 1), (1, 1)], [(0, 0), (1, 3), (2, 2), (1, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 3), (3, 0)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 2), (3, 2)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 2), (0, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 0), (0, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 2), (0, 0), (0, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6464
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.88972336 -0.45646374 -0.00576183]
True reward weights: [-0.92648321 -0.16748955  0.33701054]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6634
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.68125048 0.15852288 0.71468054]
True reward weights: [ 0.47317231 -0.67026874 -0.57170603]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147850

Running PBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6560
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.56344307  0.31878158  0.76217466]
True reward weights: [-0.19374055 -0.95601964  0.22020685]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148265

Running PBIRL with 4 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4042
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.09491621 0.31958797 0.94279077]
True reward weights: [ 0.26159822 -0.85989816  0.43833951]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.215703

Running PBIRL with 5 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.3824
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.49118474  0.31850427  0.81073582]
True reward weights: [-0.42919045 -0.33314268  0.83953053]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.254289

Running PBIRL with 6 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.3862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.10088211 -0.0468081   0.99379666]
True reward weights: [-0.99113231 -0.13173404 -0.01740365]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.244605

Running PBIRL with 7 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.3734
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [0.31163984 0.32536796 0.8927577 ]
True reward weights: [0.57500498 0.48025139 0.66236536]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.246206

Running PBIRL with 8 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.2530
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.04218635 0.06578313 0.99694177]
True reward weights: [-0.3076816  -0.57353403  0.75920403]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.284964

Running PBIRL with 9 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.2594
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [0.41358908 0.2260215  0.88196278]
True reward weights: [-0.69123561 -0.31675257  0.64950838]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.272876

Running PBIRL with 10 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.2732
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.1287858   0.15881979  0.97887205]
True reward weights: [-0.28963936 -0.56830587  0.77015419]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.264101

Running experiment 20/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 3)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 0), (0, 0)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 0), (1, 3)]), ([(0, 0), (0, 1), (0, 1), (3, 3)], [(0, 0), (0, 3), (1, 1), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 2), (3, 3)]), ([(0, 2), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 2), (3, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (1, 1)]), ([(0, 2), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 2), (3, 3), (4, 1), (4, 0)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 0), (0, 1), (3, 3), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (0, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6526
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.18037851 0.1352263  0.97425738]
True reward weights: [-0.61012931 -0.3129562   0.72787406]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6554
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.64431681 -0.55938059  0.52148749]
True reward weights: [-0.66536647 -0.07261062  0.74297723]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123834

Running PBIRL with 3 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5704
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.268748   -0.63236582  0.726559  ]
True reward weights: [ 0.83130598 -0.35701986  0.42598965]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148547

Running PBIRL with 4 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.4252
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.93891038  0.08859918  0.33256201]
True reward weights: [-0.00283911 -0.39915966  0.91687704]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.181724

Running PBIRL with 5 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.3082
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.05320392  0.33372339  0.94116845]
True reward weights: [-0.16257608  0.9862904   0.02828901]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.204493

Running PBIRL with 6 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.3152
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.83673882 -0.08138344  0.5415209 ]
True reward weights: [-0.13407549  0.50116096  0.85490435]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.187096

Running PBIRL with 7 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.3186
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.92639772 -0.01851169  0.37609119]
True reward weights: [0.09957531 0.6976052  0.70952924]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.175161

Running PBIRL with 8 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.3156
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.82601099 -0.42904044  0.36555457]
True reward weights: [-0.84781748 -0.47038653  0.24483061]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.167594

Running PBIRL with 9 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.3226
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.22978564  0.31872347  0.91957268]
True reward weights: [0.12813145 0.67814535 0.72367204]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.187521

Running PBIRL with 10 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.3248
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.53061422 -0.07504282  0.84428498]
True reward weights: [-0.23281517  0.39013063  0.8908396 ]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.183509

Saving results to files...
Results saved successfully.

Running experiment 21/50...
Shuffled Demos: [([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 1), (1, 1), (4, 1), (4, 3)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 3), (1, 1)]), ([(0, 2), (0, 1), (3, 3), (0, 1)], [(0, 2), (0, 2), (0, 2), (3, 3)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 2), (1, 3)]), ([(0, 3), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (0, 3), (1, 3), (4, 0)]), ([(0, 2), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 1), (3, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 1), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 1), (3, 0)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 3), (3, 3)]), ([(0, 3), (1, 2), (0, 1), (3, 3)], [(0, 3), (1, 2), (0, 0), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6572
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.7385639  -0.41234159  0.53338333]
True reward weights: [-0.3738588  -0.3262014   0.86822937]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6530
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.36451966 -0.89852428  0.24449853]
True reward weights: [-0.98973075 -0.10710196  0.094669  ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123798

Running PBIRL with 3 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6472
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.02594291  0.31925092  0.94731506]
True reward weights: [-0.04499995 -0.86002587  0.50826225]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148061

Running PBIRL with 4 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5272
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.10964801  0.31828029  0.9416342 ]
True reward weights: [ 0.81901426 -0.30720104  0.4846062 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.185085

Running PBIRL with 5 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.4912
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.13731357 0.31950317 0.93758344]
True reward weights: [-0.37014421 -0.45191844  0.81164215]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.188604

Running PBIRL with 6 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.4902
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.11576609 -0.05291926  0.9918658 ]
True reward weights: [-0.07041182 -0.42224552  0.90374272]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.180876

Running PBIRL with 7 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.4906
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.10659961  0.31933046  0.94162869]
True reward weights: [-0.85090829 -0.16295343  0.4994009 ]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.175715

Running PBIRL with 8 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.4936
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.46337442 0.31879511 0.82683362]
True reward weights: [0.03535317 0.55405919 0.83172626]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.172403

Running PBIRL with 9 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.4942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.34211353  0.31929902  0.88374571]
True reward weights: [0.19417364 0.53468462 0.82244085]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.169323

Running PBIRL with 10 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5026
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [0.24029511 0.34428451 0.90759376]
True reward weights: [-0.22324162  0.19701198  0.95464625]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.222449

Running experiment 22/50...
Shuffled Demos: [([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 0), (0, 0)]), ([(0, 1), (3, 3), (3, 3), (3, 3)], [(0, 1), (3, 0), (0, 2), (0, 1)]), ([(0, 0), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 2), (0, 0), (0, 3)]), ([(0, 1), (0, 1), (3, 3), (3, 3)], [(0, 2), (0, 0), (0, 2), (0, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 3), (1, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 3), (4, 2)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (3, 3)]), ([(0, 3), (1, 0), (1, 1), (2, 1), (5, None)], [(0, 3), (1, 0), (1, 3), (2, 3)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 1), (3, 2), (0, 0), (0, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6602
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.61767553 0.14455081 0.77303428]
True reward weights: [-0.70965126 -0.6051058   0.36089066]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6646
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.26936214  0.31991096  0.90835071]
True reward weights: [ 0.31615189 -0.31514649  0.89483556]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147845

Running PBIRL with 3 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6564
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.84902057 0.15987106 0.50359241]
True reward weights: [ 0.3305856  -0.93395647  0.1357883 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.202956

Running PBIRL with 4 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6540
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.59393583 0.31996329 0.73814885]
True reward weights: [-0.64305779  0.40404282  0.65055829]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.273334

Running PBIRL with 5 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6594
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.24360608  0.0667606   0.96757382]
True reward weights: [ 0.46175267 -0.21072725  0.86161389]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.273699

Running PBIRL with 6 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5250
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.23870527 0.31965512 0.9169735 ]
True reward weights: [-0.4914958   0.55653209  0.66985365]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.313629

Running PBIRL with 7 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.7079588   0.15908115  0.6881043 ]
True reward weights: [0.50042106 0.02838299 0.8653168 ]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.307186

Running PBIRL with 8 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5038
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.68664364  0.3188117   0.65336025]
True reward weights: [-0.48792364 -0.12566194  0.86379372]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.316445

Running PBIRL with 9 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.4908
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.48800628 -0.04817646  0.87150955]
True reward weights: [-0.58034518 -0.78763824  0.20694316]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.311308

Running PBIRL with 10 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.5000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.58689526  0.15940537  0.79381603]
True reward weights: [ 0.33001265 -0.26715146  0.90538486]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.385059

Saving results to files...
Results saved successfully.

Running experiment 23/50...
Shuffled Demos: [([(0, 3), (1, 0), (0, 1), (3, 3)], [(0, 3), (1, 0), (0, 0), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 2), (0, 0), (0, 2), (3, 2)]), ([(0, 1), (3, 3), (3, 3), (4, 3)], [(0, 1), (3, 2), (3, 2), (3, 1)]), ([(0, 1), (1, 1), (0, 1), (1, 1)], [(0, 2), (0, 3), (1, 0), (1, 0)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (0, 2)]), ([(0, 2), (0, 2), (0, 1), (3, 3)], [(0, 2), (0, 2), (0, 3), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 2), (0, 3)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (0, 3), (0, 0), (0, 0)]), ([(0, 1), (3, 3), (3, 3), (4, 3)], [(0, 0), (0, 2), (0, 1), (3, 3)]), ([(0, 2), (0, 1), (1, 1), (4, 3)], [(0, 2), (0, 2), (0, 1), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 23
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.45536536  0.69940919 -0.55088036]
True reward weights: [-0.74528522 -0.55533186  0.36899385]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6640
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.98376532 -0.11600092 -0.13692913]
True reward weights: [ 0.64804262  0.41773177 -0.63682096]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183923

Running PBIRL with 3 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6666
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.99186664  0.09487362 -0.08485024]
True reward weights: [0.72277796 0.54754607 0.42164597]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.225827

Running PBIRL with 4 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6550
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.90984474  0.40372088 -0.09587495]
True reward weights: [-0.41477535 -0.57176629  0.70784512]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.290053

Running PBIRL with 5 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.2168
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.53180173 0.18090365 0.82732145]
True reward weights: [ 0.71163609 -0.64936878 -0.26813106]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.409413

Running PBIRL with 6 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.0170
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.06411481 0.06415928 0.99587794]
True reward weights: [-0.09992169 -0.67910971  0.727204  ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.696461

Running PBIRL with 7 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.0192
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.5741836  -0.57425203  0.58356474]
True reward weights: [0.3882636  0.41653161 0.82204184]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.647293

Running PBIRL with 8 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.0196
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.13162758 0.13185483 0.98249096]
True reward weights: [-0.29878627 -0.32270781  0.89810157]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.611800

Running PBIRL with 9 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.0210
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.51111767 -0.51112128  0.6910237 ]
True reward weights: [-0.68785071 -0.69862322  0.19694413]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.680091

Running PBIRL with 10 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.0170
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.65568039 -0.66254465  0.36210195]
True reward weights: [0.39645752 0.41003505 0.82139679]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.747365

Running experiment 24/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (1, 2), (1, 1)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 0), (0, 0), (0, 0), (0, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 2), (0, 1)]), ([], [(0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 3), (2, 0)]), ([(0, 1), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 3), (3, 0), (3, 3), (4, 0)]), ([(0, 3), (0, 3), (1, 1), (0, 1)], [(0, 3), (0, 3), (1, 3), (2, 3)]), ([(0, 3), (1, 2), (0, 1), (3, 3)], [(0, 3), (1, 2), (0, 0), (0, 0)]), ([(0, 3), (1, 0), (1, 1), (0, 1)], [(0, 3), (1, 0), (1, 0), (1, 1)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6628
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.29355734  0.30466098  0.90609369]
True reward weights: [-0.68144434 -0.29187893  0.6711485 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6644
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.03010342  0.63882207  0.76876534]
True reward weights: [-0.06377214  0.98984576 -0.12703736]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147652

Running PBIRL with 3 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.4968
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.55929183  0.6389298   0.52816793]
True reward weights: [-0.78859992  0.14384765 -0.59784448]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.187927

Running PBIRL with 4 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.1996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.56938122 -0.41504415  0.70960791]
True reward weights: [0.50853056 0.81591539 0.2750977 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.267723

Running PBIRL with 5 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.1988
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.71376292 -0.50501432  0.48528654]
True reward weights: [-0.36280961 -0.54031656  0.75922803]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.233029

Running PBIRL with 6 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.1482
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.56712923 -0.47496286  0.67288537]
True reward weights: [-0.69758424 -0.59785096  0.39490563]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.242909

Running PBIRL with 7 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.1520
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.31042448 -0.51472554  0.7991835 ]
True reward weights: [-0.58772815 -0.78569863  0.19301111]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.280363

Running PBIRL with 8 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.1488
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.61226948 -0.554146    0.56395771]
True reward weights: [-0.52334742 -0.30502641  0.79565468]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.338152

Running PBIRL with 9 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.0720
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.66899    -0.35114246  0.65509645]
True reward weights: [-0.64456535 -0.47937308  0.59559799]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.416903

Running PBIRL with 10 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.0616
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.65984712 -0.34446068  0.66779384]
True reward weights: [-0.73804719 -0.54619237  0.39618207]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.394192

Saving results to files...
Results saved successfully.

Running experiment 25/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 2), (3, 0)]), ([(0, 2), (3, 0), (0, 1), (3, 3)], [(0, 2), (3, 0), (0, 0), (0, 0)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (3, 2), (3, 0)]), ([(0, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 2), (3, 2), (3, 1)]), ([(0, 3), (3, 3), (4, 3), (5, None)], [(0, 3), (3, 3), (4, 1), (4, 2)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 2), (0, 2)]), ([(0, 2), (0, 3), (0, 1), (0, 1)], [(0, 2), (0, 3), (0, 2), (0, 2)]), ([(0, 1), (1, 1), (4, 3), (1, 1)], [(0, 3), (1, 2), (1, 2), (4, 1)]), ([(0, 0), (0, 1), (0, 1), (3, 3)], [(0, 0), (0, 0), (0, 2), (0, 1)]), ([(0, 3), (1, 2), (0, 1), (3, 3)], [(0, 3), (1, 2), (0, 0), (0, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6520
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.19043182  0.39989925  0.89655804]
True reward weights: [-0.44714936 -0.30119858  0.84222139]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123671

Running PBIRL with 2 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6640
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.36556765 0.31898056 0.87442078]
True reward weights: [-0.31903537 -0.00898833  0.94770019]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148173

Running PBIRL with 3 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5718
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.519189   0.3188512  0.79295441]
True reward weights: [-0.397908   -0.89540991 -0.19977566]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.169403

Running PBIRL with 4 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5672
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.5132314   0.31991109  0.79639841]
True reward weights: [ 0.65400819 -0.67236249  0.34670155]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.164321

Running PBIRL with 5 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.07158995 0.31894891 0.94506427]
True reward weights: [-0.19340216 -0.45599078  0.8687163 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.161186

Running PBIRL with 6 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5748
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.49813692  0.15846817  0.85249483]
True reward weights: [ 0.82098091 -0.55962967  0.11315909]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.159177

Running PBIRL with 7 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5558
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.88916483  0.04362625  0.45550265]
True reward weights: [ 0.70447974 -0.03910581  0.70864591]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.212742

Running PBIRL with 8 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5708
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.64798015 -0.68020489  0.34269961]
True reward weights: [-0.97444791 -0.00634676  0.22452394]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.281903

Running PBIRL with 9 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5678
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.79648619 -0.50625727  0.33062566]
True reward weights: [-0.48324473 -0.26932589  0.83302947]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.358952

Running PBIRL with 10 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5650
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.62891118 -0.69577978  0.34693115]
True reward weights: [-0.30039027 -0.95025809  0.08231188]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.440081

Running experiment 26/50...
Shuffled Demos: [([(0, 2), (0, 0), (0, 1), (3, 3)], [(0, 2), (0, 0), (0, 2), (0, 2)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 3), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 1), (0, 3)]), ([(0, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 1), (3, 2)]), ([(0, 3), (1, 1), (4, 3), (1, 1)], [(0, 3), (1, 2), (0, 2), (0, 0)]), ([(0, 2), (0, 1), (1, 1), (4, 3)], [(0, 2), (0, 3), (3, 2), (3, 1)]), ([(0, 1), (1, 2), (1, 1), (0, 1)], [(0, 1), (1, 2), (1, 0), (0, 0)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (1, 2)]), ([], [(0, 1), (0, 1), (3, 3), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 2), (4, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 26
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.11356075  0.63836694  0.76130914]
True reward weights: [-0.7333898  -0.47830978  0.48307263]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6476
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.23262077  0.63992489  0.73238222]
True reward weights: [-0.80501737  0.50078416 -0.31806015]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.181798

Running PBIRL with 3 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6564
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.93224165 -0.07293904  0.35440853]
True reward weights: [-0.69970415 -0.56573068  0.43630597]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170177

Running PBIRL with 4 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6558
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.58019343  0.63934075  0.50459786]
True reward weights: [-0.1204378   0.40262409  0.90740762]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.164431

Running PBIRL with 5 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.4240
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.88762561 -0.05418965  0.45736665]
True reward weights: [0.4132568  0.85033958 0.32579352]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.220663

Running PBIRL with 6 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.4384
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.8932339  0.15910954 0.42049655]
True reward weights: [ 0.72991186 -0.56747296  0.38105527]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.205634

Running PBIRL with 7 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.4440
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [ 0.92015235 -0.05927742  0.38704759]
True reward weights: [ 0.12837004 -0.96368242  0.2341737 ]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.251131

Running PBIRL with 8 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.2078
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [ 0.36872331 -0.04994388  0.92819649]
True reward weights: [-0.4868626  -0.68301357  0.5444789 ]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.328739

Running PBIRL with 9 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.1586
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [ 0.31945965 -0.22756031  0.91987055]
True reward weights: [-0.30157684 -0.78331909  0.54356473]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.345757

Running PBIRL with 10 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.1676
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [ 0.16847123 -0.31561199  0.93381289]
True reward weights: [ 0.53255942 -0.11080067  0.83910886]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.321910

Saving results to files...
Results saved successfully.

Running experiment 27/50...
Shuffled Demos: [([(0, 2), (0, 2), (0, 1), (3, 3)], [(0, 2), (0, 2), (0, 2), (0, 2)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (1, 1), (4, 3), (1, 1)], [(0, 2), (0, 3), (1, 2), (4, 2)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (0, 2), (0, 1), (3, 0)]), ([(0, 0), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 1), (3, 2), (0, 0)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (3, 1), (4, 2)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 1), (3, 3)]), ([(0, 3), (1, 1), (2, 1), (5, None)], [(0, 3), (1, 0), (1, 3), (2, 1)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (3, 3), (0, 3), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (3, 3), (4, 2), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 27
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.00449324  0.63865793 -0.76947765]
True reward weights: [-0.65557811 -0.01374154  0.75500233]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6510
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.08075757 0.63875297 0.76516198]
True reward weights: [-0.08974693  0.39132042 -0.9158678 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182882

Running PBIRL with 3 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.1324
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.82958942  0.5566737  -0.0435406 ]
True reward weights: [-0.10451804  0.46189575  0.88075439]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.267991

Running PBIRL with 4 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.0830
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.79445634 0.53726964 0.28316153]
True reward weights: [ 0.86504105  0.49174088 -0.09947304]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.264449

Running PBIRL with 5 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.0796
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.79660815 0.53444267 0.28246501]
True reward weights: [0.46516058 0.44052318 0.76783134]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.288531

Running PBIRL with 6 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.0788
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.78247712 0.52010669 0.34237198]
True reward weights: [0.73839647 0.39700523 0.54512154]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.270831

Running PBIRL with 7 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.0808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [0.79165484 0.52369794 0.31467933]
True reward weights: [0.67619575 0.49395988 0.54659212]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.260210

Running PBIRL with 8 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.0338
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.52738306 0.3669217  0.76631298]
True reward weights: [0.82700692 0.55503322 0.08942972]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.350812

Running PBIRL with 9 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.0356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [0.54206767 0.3694581  0.75476046]
True reward weights: [0.32969562 0.19192161 0.92437378]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.326387

Running PBIRL with 10 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.0356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [0.54750754 0.379294   0.74590318]
True reward weights: [0.32361347 0.18978254 0.92696111]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.309020

Running experiment 28/50...
Shuffled Demos: [([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 3), (4, 1)]), ([(0, 0), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 1), (3, 1)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 3)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 1), (3, 3), (0, 0), (0, 0)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 1), (1, 0)]), ([(0, 0), (0, 3), (0, 1), (0, 1)], [(0, 0), (0, 3), (0, 2), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 1), (3, 1)]), ([(0, 2), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 2), (0, 3)]), ([(0, 1), (0, 1), (1, 1), (4, 3)], [(0, 2), (0, 0), (0, 0), (0, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (0, 2), (0, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6496
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.03872987 -0.72562712  0.68699729]
True reward weights: [-0.866301   -0.34967581  0.35672034]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.4470
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.37448629 -0.92652594 -0.03618972]
True reward weights: [-0.04195921 -0.83114028  0.55447747]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.160897

Running PBIRL with 3 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.3532
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.25607704 -0.25522818  0.93235354]
True reward weights: [ 0.50941101 -0.11729092  0.85249238]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.171302

Running PBIRL with 4 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.3550
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.15314687 0.31904962 0.93528251]
True reward weights: [-0.38620517  0.28720162  0.87656192]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.181803

Running PBIRL with 5 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.3444
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.262602   0.63977379 0.72230859]
True reward weights: [-0.60376337  0.14043662  0.78469571]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.174640

Running PBIRL with 6 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.3594
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.2903536  0.04875292 0.95567669]
True reward weights: [ 0.20264005 -0.53161408  0.82238889]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.225485

Running PBIRL with 7 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.3610
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [0.24262856 0.63970062 0.72932468]
True reward weights: [-0.13385756 -0.69871876  0.70276187]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.222176

Running PBIRL with 8 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.3498
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.42035642 0.318507   0.84961978]
True reward weights: [-0.40423667 -0.48833275  0.77338467]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.219967

Running PBIRL with 9 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.1938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [ 0.35639527 -0.07521282  0.93130309]
True reward weights: [-0.32233316 -0.43089096  0.84287265]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.294856

Running PBIRL with 10 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.1958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [ 0.42135033 -0.05938751  0.90495139]
True reward weights: [ 0.09268636 -0.59953755  0.79496162]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.282234

Saving results to files...
Results saved successfully.

Running experiment 29/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 1), (3, 0), (0, 2), (0, 2)]), ([(0, 1), (3, 0), (0, 1), (3, 3)], [(0, 1), (3, 0), (0, 2), (0, 3)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 3), (2, 1)]), ([(0, 0), (1, 1), (4, 3), (5, None)], [(0, 0), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 1), (3, 1), (3, 0), (3, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 0), (0, 2), (0, 3)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 3), (1, 1), (4, 1), (4, 2)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 0), (1, 2)]), ([(0, 3), (1, 1), (0, 1), (3, 3)], [(0, 3), (1, 2), (0, 0), (0, 1)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 2), (0, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6600
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.10739601 -0.42787105  0.8974366 ]
True reward weights: [-0.6535586  -0.23072892  0.72085041]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6634
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.71504106 0.1587918  0.6808094 ]
True reward weights: [ 0.10677095 -0.12961958 -0.98579852]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147887

Running PBIRL with 3 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5216
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.77583869 -0.04750755  0.62914018]
True reward weights: [ 0.98213514 -0.03943962  0.18399752]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.180743

Running PBIRL with 4 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5306
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.83888621 -0.32074774  0.43976222]
True reward weights: [-0.31632533 -0.93046201 -0.18487492]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.227068

Running PBIRL with 5 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5170
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.3520142  -0.00450429  0.93598382]
True reward weights: [-0.21341672 -0.97525376  0.05773558]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.222288

Running PBIRL with 6 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.5320
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.36836013 -0.30398554  0.87858045]
True reward weights: [ 0.19216538 -0.69206216 -0.69578908]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.219172

Running PBIRL with 7 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.0178
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.68050311 -0.6881772   0.25164987]
True reward weights: [ 0.10897284 -0.69569157  0.71002687]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.681241

Running PBIRL with 8 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.0180
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.5596077  -0.56658494  0.60483115]
True reward weights: [-0.43120989 -0.44218281  0.78646831]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.629331

Running PBIRL with 9 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.0180
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [0.32120822 0.31543406 0.89293148]
True reward weights: [-0.47695642 -0.49531214  0.72607056]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.679999

Running PBIRL with 10 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.0154
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [0.53718588 0.52979581 0.65631374]
True reward weights: [-0.67009082 -0.69226336  0.26786139]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.667712

Running experiment 30/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 2), (0, 0), (0, 2), (0, 0)]), ([(0, 3), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 3), (2, 2), (1, 3)]), ([(0, 1), (3, 0), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (3, 2), (3, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 0), (1, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 2), (0, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 2)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 1), (3, 2)]), ([(0, 0), (0, 1), (0, 1), (3, 3)], [(0, 0), (0, 2), (0, 2), (0, 1)]), ([(0, 2), (0, 1), (3, 3), (4, 3)], [(0, 2), (0, 2), (0, 2), (0, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 1), (3, 1), (3, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.6630
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.37645291 -0.92168518  0.0936997 ]
True reward weights: [-0.81326386 -0.04441834  0.5801973 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.3360
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.31737316 -0.43442822  0.84293914]
True reward weights: [0.59719838 0.46677088 0.65228755]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.176546

Running PBIRL with 3 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.3140
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.07439256 -0.55682614  0.827291  ]
True reward weights: [ 0.02999556 -0.88006278  0.47390904]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.165097

Running PBIRL with 4 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.3048
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.4235175  -0.1304325   0.89644871]
True reward weights: [-0.06575599 -0.98484247  0.16050377]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.153926

Running PBIRL with 5 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.1754
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.14392659 -0.51036105  0.84783061]
True reward weights: [-0.26551472 -0.92292572  0.27876524]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.198847

Running PBIRL with 6 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.1784
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.23601119 -0.74335648  0.62587527]
True reward weights: [0.55756137 0.48589514 0.67307595]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.185044

Running PBIRL with 7 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.1704
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.12695694 -0.63829465  0.75925087]
True reward weights: [-0.21499951 -0.64103078  0.73678678]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.173225

Running PBIRL with 8 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.1850
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.30220447 -0.70052178  0.6464841 ]
True reward weights: [ 0.22318369 -0.37334251  0.90044678]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.192572

Running PBIRL with 9 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.1760
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.2415957  -0.66809224  0.70376436]
True reward weights: [-0.59653136 -0.69758419  0.39689625]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.242484

Running PBIRL with 10 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.1780
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [ 0.16954794 -0.22237526  0.96010559]
True reward weights: [-0.05955903 -0.4138684   0.9083863 ]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.238492

Saving results to files...
Results saved successfully.

Running experiment 31/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (4, 3)], [(0, 2), (0, 3), (1, 1), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 0), (0, 3), (1, 2), (0, 1)]), ([(0, 0), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 2), (0, 0)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 1), (3, 2), (3, 3), (4, 1)]), ([(0, 0), (0, 1), (1, 1), (4, 3)], [(0, 0), (0, 0), (1, 1), (4, 0)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 0), (3, 2)]), ([(0, 1), (3, 3), (3, 3), (4, 3)], [(0, 0), (0, 1), (3, 2), (3, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 1), (4, 2)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 1), (2, 1)]), ([(0, 2), (0, 1), (3, 3), (3, 3)], [(0, 2), (0, 3), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6536
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.70337713 -0.09700849  0.70416615]
True reward weights: [-0.27534761 -0.19938474  0.94044108]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6642
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.21271216  0.96764713 -0.13569289]
True reward weights: [-0.44426434  0.13904449 -0.88504001]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148032

Running PBIRL with 3 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.4446
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.40680028 0.83296925 0.37506768]
True reward weights: [-0.91625086 -0.37471807 -0.14167123]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.195450

Running PBIRL with 4 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.0178
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.1207786  -0.12079701  0.98530229]
True reward weights: [-0.83240167  0.19707385  0.51794725]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.488075

Running PBIRL with 5 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.0218
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.36729391 0.36639251 0.85489866]
True reward weights: [-0.19466814 -0.20795647  0.95857103]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.519232

Running PBIRL with 6 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.0148
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.0570803  0.05646364 0.99677164]
True reward weights: [-0.4692379  -0.4769561   0.74318818]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.501237

Running PBIRL with 7 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.0180
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.57267486 -0.57284179  0.58642628]
True reward weights: [0.43267048 0.46598809 0.77178453]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.562905

Running PBIRL with 8 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.0210
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.2469529  0.24686671 0.93705448]
True reward weights: [-0.04693644 -0.09249948  0.99460586]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.547359

Running PBIRL with 9 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.0196
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [0.34554592 0.34578576 0.87237046]
True reward weights: [0.41976577 0.40900948 0.81025178]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.536574

Running PBIRL with 10 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.0198
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.52579023 -0.52588774  0.66857065]
True reward weights: [-0.4784895  -0.47459594  0.73878718]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.614411

Running experiment 32/50...
Shuffled Demos: [([(0, 1), (0, 1), (3, 3), (4, 3)], [(0, 1), (0, 1), (3, 1), (3, 3)]), ([], [(0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 1), (5, None)]), ([(0, 2), (0, 1), (3, 3), (4, 3)], [(0, 2), (0, 1), (3, 0), (0, 2)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 1), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 2), (0, 2)]), ([(0, 1), (3, 2), (0, 1), (3, 3)], [(0, 1), (3, 2), (0, 2), (0, 2)]), ([(0, 2), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 2), (3, 0), (0, 2), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 1), (4, 2), (3, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 2), (0, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 32
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.57143502 0.63906066 0.51484317]
True reward weights: [-0.90753571 -0.40234227  0.1204144 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6436
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.72420071 -0.05555302 -0.68734794]
True reward weights: [-0.03731767 -0.93284993  0.35832723]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182757

Running PBIRL with 3 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.2654
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.43173289  0.15869814 -0.88793109]
True reward weights: [-0.23297168 -0.74944328  0.61972491]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.242134

Running PBIRL with 4 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.2838
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.5456615  -0.07479841 -0.83466085]
True reward weights: [-0.52985281 -0.34983559 -0.77257431]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.266594

Running PBIRL with 5 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.2284
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.94689425 -0.0731447  -0.3131152 ]
True reward weights: [-0.12955776  0.05778189 -0.98988688]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.280394

Running PBIRL with 6 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.1698
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.91707807 -0.31171443  0.24859992]
True reward weights: [-0.90409135 -0.40956319 -0.12197059]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.296769

Running PBIRL with 7 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.1716
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.74960711 -0.3375266   0.56935488]
True reward weights: [-0.69344061 -0.2298673   0.68286247]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.349380

Running PBIRL with 8 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.1390
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.80810283 -0.47503139  0.34830302]
True reward weights: [-0.7971183  -0.55408387 -0.2399864 ]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.373652

Running PBIRL with 9 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.1300
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.81264871 -0.39563535  0.42787235]
True reward weights: [-0.93414301 -0.35185104  0.05981367]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.358351

Running PBIRL with 10 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.1350
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.81595353 -0.4435601   0.37077522]
True reward weights: [-0.8543989  -0.31870577  0.4104012 ]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.347351

Saving results to files...
Results saved successfully.

Running experiment 33/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 3), (1, 1), (0, 3), (1, 1)]), ([(0, 3), (1, 1), (2, 1), (5, None)], [(0, 3), (1, 1), (2, 3), (2, 3)]), ([(0, 3), (1, 1), (2, 1), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 3), (1, 1), (4, 3), (1, 1)], [(0, 3), (1, 1), (4, 1), (4, 3)]), ([(0, 1), (0, 3), (1, 1), (4, 3)], [(0, 1), (0, 3), (1, 2), (0, 0)]), ([(0, 0), (0, 3), (1, 1), (4, 3)], [(0, 0), (0, 3), (1, 0), (1, 0)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 1), (0, 1)]), ([(0, 3), (1, 2), (0, 1), (0, 1)], [(0, 3), (1, 2), (0, 0), (0, 0)]), ([(0, 1), (3, 1), (3, 3), (0, 1)], [(0, 1), (3, 1), (3, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 1), (3, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6396
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.39903898  0.48978374  0.77516435]
True reward weights: [-0.32556312 -0.30396656  0.89532842]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.3338
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.12191045  0.24443819  0.9619708 ]
True reward weights: [-0.68440238  0.28699658 -0.6702435 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173523

Running PBIRL with 3 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.3382
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.58293852 -0.20315688  0.78670831]
True reward weights: [-0.77243575 -0.59694577 -0.21679196]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.180082

Running PBIRL with 4 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.0174
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.12020516 -0.11978655  0.98549577]
True reward weights: [-0.93284112  0.19898002  0.30035711]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.472066

Running PBIRL with 5 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.0180
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.54022395 0.54065479 0.6448647 ]
True reward weights: [-0.28328726 -0.29671972  0.91197902]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.507472

Running PBIRL with 6 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.0194
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.67502479 -0.66802758  0.31317838]
True reward weights: [-0.33083548 -0.36857448  0.86873514]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.554509

Running PBIRL with 7 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.0162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.19732431 -0.19089844  0.9615721 ]
True reward weights: [-0.67155038 -0.66940895  0.3176661 ]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.549003

Running PBIRL with 8 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.0160
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.29395276 -0.28677107  0.91178623]
True reward weights: [-0.7402356  -0.67213902  0.01674486]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.617796

Running PBIRL with 9 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.0170
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.16711814 -0.16008974  0.97285292]
True reward weights: [-0.56499801 -0.56087714  0.60513972]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.693673

Running PBIRL with 10 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.0140
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [0.17519508 0.18210017 0.96754649]
True reward weights: [-0.19530606 -0.17598571  0.9648236 ]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.682251

Running experiment 34/50...
Shuffled Demos: [([(0, 1), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 0), (0, 1), (1, 0)]), ([(0, 3), (1, 0), (1, 1), (4, 3)], [(0, 3), (1, 0), (1, 3), (2, 1)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (4, 3)], [(0, 2), (0, 1), (3, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 3), (1, 2)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 1)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 1), (3, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 0), (1, 3)]), ([(0, 2), (0, 3), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (3, 0), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.6562
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.98234501 -0.02347259  0.18559987]
True reward weights: [-0.839382   -0.393236    0.37523766]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.6590
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.3389978   0.93866247 -0.06319227]
True reward weights: [-0.40115719 -0.3524402  -0.84549324]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147927

Running PBIRL with 3 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.4444
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.55621775  0.40541249  0.72543954]
True reward weights: [0.25410145 0.56308296 0.78636508]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.198168

Running PBIRL with 4 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.4428
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.57297825  0.63896641  0.5132425 ]
True reward weights: [-0.22143917  0.97503957 -0.01620263]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.239307

Running PBIRL with 5 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.4392
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.65336715  0.51999047  0.55020112]
True reward weights: [-0.5051356   0.6729406   0.54036004]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.239868

Running PBIRL with 6 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.4430
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.69604701  0.63837887  0.3286198 ]
True reward weights: [-0.86517971  0.25941248  0.42914943]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.233663

Running PBIRL with 7 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.4202
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.6453171   0.31867176  0.69427239]
True reward weights: [-0.22824039  0.33488916  0.91419668]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.299372

Running PBIRL with 8 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.3058
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.49365811  0.31952553  0.80882947]
True reward weights: [-0.94809567  0.26206511  0.18010131]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.342274

Running PBIRL with 9 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.3124
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.41499099  0.07336317  0.9068629 ]
True reward weights: [-0.77256225  0.42984506  0.4673123 ]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.332752

Running PBIRL with 10 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.3152
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.23249466  0.31858205  0.91894054]
True reward weights: [-0.25451351  0.57852557  0.77493938]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.325781

Saving results to files...
Results saved successfully.

Running experiment 35/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 1), (3, 0)]), ([(0, 1), (0, 2), (0, 1), (3, 3)], [(0, 1), (0, 2), (0, 3), (3, 3)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 2), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 3)]), ([(0, 1), (3, 3), (4, 3), (4, 3)], [(0, 0), (0, 3), (1, 3), (1, 3)]), ([(0, 2), (0, 1), (3, 3), (3, 3)], [(0, 2), (0, 1), (3, 1), (3, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 3), (1, 1)]), ([(0, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 2), (3, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 1), (1, 3)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 0), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6628
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.29720017 -0.79266624  0.53230846]
True reward weights: [-0.94121751 -0.05112011  0.33391067]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6630
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.78169804 0.0538971  0.62132381]
True reward weights: [ 0.93126821 -0.28828285 -0.22278357]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148068

Running PBIRL with 3 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5410
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.13367137  0.31896595  0.93829243]
True reward weights: [0.19796931 0.58909155 0.78344068]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.179222

Running PBIRL with 4 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.4910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.24612349 -0.051991    0.96784305]
True reward weights: [-0.86219731 -0.40021704  0.31055132]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.182013

Running PBIRL with 5 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.3084
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.32722417  0.31852373  0.8896443 ]
True reward weights: [0.26681793 0.04707326 0.96259665]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.230506

Running PBIRL with 6 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.3046
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.19539446  0.15924848  0.96770911]
True reward weights: [-0.3771368   0.63078106  0.67814681]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.267814

Running PBIRL with 7 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.3066
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.63632212 -0.04263609  0.77024433]
True reward weights: [-0.58203706 -0.05372871  0.81138529]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.256351

Running PBIRL with 8 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.3130
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.51814682  0.31917362  0.79350619]
True reward weights: [-0.98220109 -0.14156253  0.12345469]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.248789

Running PBIRL with 9 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.3080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.14022971  0.31877701  0.93739898]
True reward weights: [-0.58628058  0.0280496   0.80962232]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.242940

Running PBIRL with 10 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.3128
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.89339134  0.04443098  0.44707695]
True reward weights: [-0.60029044 -0.4807093   0.63919477]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.238577

Running experiment 36/50...
Shuffled Demos: [([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (0, 2), (0, 1), (3, 0)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 1), (3, 3)]), ([(0, 3), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (3, 1), (3, 2), (3, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 3), (4, 2)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (0, 1)]), ([(0, 3), (1, 0), (2, 1), (2, 1)], [(0, 3), (1, 0), (2, 0), (2, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (1, 1), (4, 3)]), ([(0, 2), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (3, 2)]), ([(0, 2), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 3)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 1), (3, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.6452
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.84724831 -0.15275036  0.50876087]
True reward weights: [-0.05456508 -0.040557    0.99768621]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123673

Running PBIRL with 2 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5840
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.12718764 -0.41641808  0.90023291]
True reward weights: [0.19824342 0.38812004 0.90003466]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.141302

Running PBIRL with 3 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5700
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.83045691 -0.39416013  0.39367385]
True reward weights: [ 0.7946745  -0.51011872  0.3290461 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.135407

Running PBIRL with 4 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.03769892 -0.2125053   0.97643243]
True reward weights: [ 0.41468203 -0.18807122  0.89031906]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.132631

Running PBIRL with 5 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5508
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.55366981 -0.34248486  0.759048  ]
True reward weights: [-0.71689308  0.44772379  0.53442279]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.130879

Running PBIRL with 6 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5668
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.40587257  0.03101044  0.91340342]
True reward weights: [ 0.21943925 -0.91577194  0.33646422]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.154309

Running PBIRL with 7 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5020
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.10201335  0.24507217  0.96412287]
True reward weights: [-0.06177047 -0.62570931  0.77760675]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.178240

Running PBIRL with 8 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.4252
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.15639315  0.26357077  0.95187795]
True reward weights: [-0.18520654 -0.59951382  0.77864094]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.199171

Running PBIRL with 9 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.4234
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.167351    0.26868128  0.94858   ]
True reward weights: [-0.5495283  -0.5451427   0.63311775]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.192414

Running PBIRL with 10 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.4356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [ 0.12136808 -0.14733041  0.98161272]
True reward weights: [-0.10880569 -0.73479876  0.66950138]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.187339

Saving results to files...
Results saved successfully.

Running experiment 37/50...
Shuffled Demos: [([(0, 2), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 1), (3, 0), (0, 0)]), ([(0, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 1), (3, 2), (3, 1)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 3), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (1, 1)]), ([(0, 2), (0, 3), (1, 1), (4, 3)], [(0, 2), (0, 3), (1, 0), (1, 3)]), ([(0, 1), (3, 3), (3, 3), (0, 1)], [(0, 0), (0, 0), (0, 1), (0, 2)]), ([(0, 1), (3, 3), (3, 3), (4, 3)], [(0, 3), (1, 2), (1, 2), (0, 1)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 0), (0, 3)]), ([(0, 1), (1, 1), (0, 1), (0, 1)], [(0, 1), (1, 3), (2, 0), (2, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6548
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.0682183  0.61975491 0.78182486]
True reward weights: [-0.75478167 -0.33513042  0.563908  ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123673

Running PBIRL with 2 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6580
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.45835523 -0.03604313  0.88803794]
True reward weights: [0.74148543 0.0310316  0.67025099]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124462

Running PBIRL with 3 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6448
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.8199795  0.17552478 0.54481618]
True reward weights: [0.30305469 0.71524021 0.62975336]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124224

Running PBIRL with 4 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.5626
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.56426313  0.24200597  0.78932897]
True reward weights: [0.73714325 0.33339413 0.58776542]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.148650

Running PBIRL with 5 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.5082
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.07000675  0.79485273  0.60275052]
True reward weights: [ 0.55931759 -0.16751635  0.81185104]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.162458

Running PBIRL with 6 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.4126
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.79726127  0.00839774  0.60357596]
True reward weights: [-0.56078129 -0.01682415  0.82779303]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.181045

Running PBIRL with 7 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.4168
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.80133135  0.31961856  0.50567978]
True reward weights: [-0.87817674  0.47058901  0.08574144]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.195663

Running PBIRL with 8 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.4224
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.16018136 0.63982264 0.75164414]
True reward weights: [0.41744057 0.72039014 0.55387852]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.189494

Running PBIRL with 9 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.4226
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.29993865  0.31847514  0.89922766]
True reward weights: [-0.63564321  0.67015536  0.38320948]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.184186

Running PBIRL with 10 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.4288
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.04135972  0.86702134  0.49655148]
True reward weights: [0.0113431  0.87156165 0.49015469]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.235239

Running experiment 38/50...
Shuffled Demos: [([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 1), (4, 2)]), ([(0, 2), (0, 0), (0, 1), (3, 3)], [(0, 2), (0, 0), (0, 2), (0, 1)]), ([(0, 0), (0, 1), (3, 3), (0, 1)], [(0, 0), (0, 2), (0, 1), (3, 1)]), ([(0, 1), (0, 1), (3, 3), (3, 3)], [(0, 2), (0, 0), (0, 1), (3, 2)]), ([(0, 1), (3, 1), (3, 3), (3, 3)], [(0, 1), (3, 1), (3, 1), (3, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 2), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 2), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 1)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 3), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 3), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6636
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.74548091 -0.62492585  0.23178848]
True reward weights: [-0.49248773 -0.22865786  0.83974486]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6582
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.67525879  0.63836228  0.369485  ]
True reward weights: [-0.13841956  0.23052343  0.96317131]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147989

Running PBIRL with 3 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6560
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.76616671  0.63860174 -0.07194716]
True reward weights: [ 0.55202093 -0.54343841  0.6324141 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.203033

Running PBIRL with 4 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6582
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.75621061  0.63858422  0.14267343]
True reward weights: [-0.67398646  0.54806827  0.49534172]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.273348

Running PBIRL with 5 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6674
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.57568006  0.63912533  0.51001106]
True reward weights: [-0.68772322  0.71530142  0.12401878]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.351399

Running PBIRL with 6 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.5868
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.69650321  0.31888432  0.64280329]
True reward weights: [ 0.55104316 -0.35973353  0.75295632]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.373699

Running PBIRL with 7 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.4358
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.44304463  0.15893738  0.88229835]
True reward weights: [-0.85931527  0.43750263  0.26489379]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.418833

Running PBIRL with 8 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.4384
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.08151466 0.15907557 0.98389548]
True reward weights: [-0.83073215  0.27780466  0.48239887]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.408650

Running PBIRL with 9 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.0148
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.97173553  0.1632647   0.17051304]
True reward weights: [-0.3623841   0.41376618  0.83514987]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.870310

Running PBIRL with 10 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.0186
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.22475213  0.68583766  0.69218002]
True reward weights: [0.29712785 0.67076756 0.67954833]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.831418

Saving results to files...
Results saved successfully.

Running experiment 39/50...
Shuffled Demos: [([(0, 0), (0, 3), (0, 1), (1, 1)], [(0, 0), (0, 3), (0, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 3), (1, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 2), (3, 2), (3, 1)]), ([(0, 2), (0, 0), (0, 1), (3, 3)], [(0, 2), (0, 0), (0, 0), (0, 3)]), ([(0, 1), (1, 3), (1, 1), (4, 3)], [(0, 1), (1, 3), (1, 3), (2, 2)]), ([(0, 1), (3, 0), (4, 3), (5, None)], [(0, 1), (3, 0), (4, 1), (4, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 3), (4, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 1), (3, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 0), (0, 2), (0, 0)]), ([(0, 3), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 3), (2, 0), (2, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.6520
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.77320835 0.25298906 0.58150269]
True reward weights: [-0.56481614 -0.46100063  0.68444222]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.3270
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.45929831 -0.80668396  0.37189522]
True reward weights: [-0.13569276 -0.97949112 -0.14894503]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.172414

Running PBIRL with 3 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.3310
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.44694786 -0.07221753  0.8916402 ]
True reward weights: [-0.11257136 -0.94799044  0.29772775]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.179724

Running PBIRL with 4 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.3314
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.45041728 -0.06364712  0.89054664]
True reward weights: [-0.27758903 -0.61241037  0.74020124]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.227070

Running PBIRL with 5 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.3312
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.3983056  -0.07118286  0.91448655]
True reward weights: [-0.28615458 -0.88488578  0.36754962]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.292632

Running PBIRL with 6 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.3332
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.50375959 0.04678239 0.86257619]
True reward weights: [-0.59854948 -0.79894797 -0.05848638]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.289431

Running PBIRL with 7 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.3388
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [ 0.51201478 -0.07781641  0.8554446 ]
True reward weights: [ 0.61042174 -0.22811611  0.7585172 ]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.287350

Running PBIRL with 8 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.3310
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [ 0.31997579 -0.05629527  0.94575173]
True reward weights: [ 0.67302757 -0.11950229  0.72989937]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.285834

Running PBIRL with 9 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.3334
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [0.50903487 0.06403284 0.85836082]
True reward weights: [ 0.33435814 -0.27278055  0.90210609]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.284514

Running PBIRL with 10 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.3260
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [ 0.39222201 -0.05739047  0.91807855]
True reward weights: [ 0.51527305 -0.65340066  0.55458206]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.283550

Running experiment 40/50...
Shuffled Demos: [([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 3), (2, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (0, 0)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 1), (3, 3)]), ([(0, 3), (1, 0), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 0), (1, 1)]), ([(0, 2), (0, 0), (0, 1), (3, 3)], [(0, 2), (0, 0), (0, 2), (0, 0)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (0, 2), (0, 0)]), ([(0, 1), (1, 2), (0, 1), (3, 3)], [(0, 1), (1, 2), (0, 3), (1, 2)]), ([(0, 1), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 0), (0, 3), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (3, 3), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6524
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.17469349 -0.92917985  0.32574069]
True reward weights: [-0.6667631  -0.57474284  0.47444455]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6562
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.83542926 -0.12895474  0.53425521]
True reward weights: [ 0.97746488 -0.20914906 -0.02861966]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124135

Running PBIRL with 3 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.4374
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.44179708 0.17454648 0.87997095]
True reward weights: [-0.79887821  0.30365486  0.519218  ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.172456

Running PBIRL with 4 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.4402
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.85837256 -0.4716196   0.20191954]
True reward weights: [-0.89766629  0.20270926  0.3912853 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.158531

Running PBIRL with 5 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.4522
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.08530929 0.31870289 0.94400783]
True reward weights: [-0.25561889  0.16329282  0.95288742]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.175240

Running PBIRL with 6 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.4404
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.32565321  0.31839629  0.89026613]
True reward weights: [ 0.16487909 -0.24897972  0.95437099]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.174774

Running PBIRL with 7 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.3060
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.77294142  0.159963    0.6139816 ]
True reward weights: [ 0.16595406 -0.30296349  0.93844146]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.216852

Running PBIRL with 8 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.3034
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.31768791  0.31884811  0.89297832]
True reward weights: [-0.42356021  0.51630182  0.74433137]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.206663

Running PBIRL with 9 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.3164
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.57147188  0.04223696  0.81953397]
True reward weights: [-0.20793238  0.17870449  0.96168021]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.198924

Running PBIRL with 10 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.3178
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.43516505  0.1590239   0.88619568]
True reward weights: [0.27030254 0.32600769 0.90590039]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.193325

Saving results to files...
Results saved successfully.

Running experiment 41/50...
Shuffled Demos: [([(0, 1), (3, 1), (4, 3), (5, None)], [(0, 1), (3, 1), (4, 2), (3, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 0), (5, None)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (3, 0)]), ([(0, 0), (0, 2), (0, 1), (3, 3)], [(0, 0), (0, 2), (0, 0), (0, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 1), (4, 0)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 2), (3, 1)]), ([(0, 2), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 2), (0, 1)]), ([(0, 1), (1, 1), (2, 1), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 2)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 2), (0, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (1, 0), (1, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.6646
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.03549992 -0.09835839  0.99451766]
True reward weights: [-0.46247666 -0.0267168   0.88622884]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.6650
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.04483587 0.09495332 0.99447152]
True reward weights: [-0.53781618 -0.61868686 -0.57269567]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147989

Running PBIRL with 3 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5848
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.09365625  0.09562837  0.99100137]
True reward weights: [-0.19115432 -0.95940966  0.20734787]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.169557

Running PBIRL with 4 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5588
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.54623743 0.15024373 0.82404581]
True reward weights: [0.56654501 0.13559422 0.81279823]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.219217

Running PBIRL with 5 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5656
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.24786637  0.10527079  0.9630578 ]
True reward weights: [-0.04203465 -0.37865172  0.92458421]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.216235

Running PBIRL with 6 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5642
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.46126564 0.13403863 0.87707905]
True reward weights: [-0.24185098 -0.13773034  0.96048866]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.214381

Running PBIRL with 7 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5114
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [0.09861113 0.09482172 0.99059815]
True reward weights: [-0.30449113 -0.94092894  0.14811443]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.229756

Running PBIRL with 8 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5046
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.5288865  0.31852983 0.78664975]
True reward weights: [0.61719433 0.16533227 0.76924404]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.226166

Running PBIRL with 9 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.3526
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.25431015  0.10393911  0.96152119]
True reward weights: [-0.519029   -0.84914015  0.09782593]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.281831

Running PBIRL with 10 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.3186
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.22333988  0.10329577  0.96925192]
True reward weights: [-0.12808595 -0.71198869  0.69041009]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.288241

Running experiment 42/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 0), (4, 0)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 2), (0, 0)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 3), (1, 3), (2, 3), (2, 0)]), ([(0, 3), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 1), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 1), (3, 0)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 1), (3, 0)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 0), (1, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (4, 3), (5, None)]), ([(0, 0), (1, 1), (4, 3), (5, None)], [(0, 0), (1, 3), (2, 1), (1, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 1), (3, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.6594
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.49853758  0.06370301  0.86452427]
True reward weights: [-0.71505276 -0.68463496  0.14133129]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5810
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.21875819  0.31020675  0.92515762]
True reward weights: [-0.51995686 -0.83207996 -0.19310052]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.141296

Running PBIRL with 3 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3204
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.79427283 -0.46855006  0.38677062]
True reward weights: [-0.06664416  0.57316488  0.81672552]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.193952

Running PBIRL with 4 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3124
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.69436395 -0.06839103  0.71636678]
True reward weights: [-0.37154662  0.51374504  0.77331697]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.172304

Running PBIRL with 5 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3102
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.64697312  0.22067096  0.72988363]
True reward weights: [-0.80856649 -0.58593936  0.05380792]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.160889

Running PBIRL with 6 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3094
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.16137147 0.51201252 0.84368384]
True reward weights: [-0.99443396 -0.09123245  0.0527043 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.154953

Running PBIRL with 7 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.33627509  0.19997697  0.92028706]
True reward weights: [-0.01255267  0.3672922   0.9300209 ]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.149339

Running PBIRL with 8 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3096
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.70212461 -0.02421438  0.71164225]
True reward weights: [-0.3932509  0.4303131  0.8125173]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.170993

Running PBIRL with 9 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3138
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.3587009   0.1169685   0.92609505]
True reward weights: [-0.7799335  -0.42092598  0.46316849]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.168237

Running PBIRL with 10 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3214
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.48392562 -0.0785736   0.87157454]
True reward weights: [-0.00481415  0.08163476  0.99665069]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.167064

Saving results to files...
Results saved successfully.

Running experiment 43/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 2), (3, 1)]), ([(0, 2), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 1), (3, 0), (0, 0)]), ([(0, 2), (0, 1), (3, 3), (0, 1)], [(0, 2), (0, 1), (3, 0), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 0), (3, 1)]), ([(0, 1), (3, 3), (3, 3), (4, 3)], [(0, 1), (3, 1), (3, 3), (4, 0)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 0), (1, 1)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (4, 0), (1, 2)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (3, 0)]), ([(0, 3), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 3), (3, 1), (3, 3), (4, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.6730
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.05426651 0.70423488 0.70789009]
True reward weights: [-0.94655708 -0.29794542  0.12352418]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.6504
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.64964915 -0.38821515  0.65363979]
True reward weights: [ 0.29318382 -0.83769262  0.46077578]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123555

Running PBIRL with 3 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5574
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.36749039 -0.6970946   0.61563783]
True reward weights: [0.72422646 0.6464374  0.24003068]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.146403

Running PBIRL with 4 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5596
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.05191737  0.15882974  0.98594001]
True reward weights: [0.46840238 0.39350924 0.79104342]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.164722

Running PBIRL with 5 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5728
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.31681026  0.31856959  0.89338943]
True reward weights: [-0.41056424  0.20904015  0.88754674]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.161707

Running PBIRL with 6 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5564
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.69493586  0.3182696   0.64480122]
True reward weights: [ 0.63192067 -0.68495835  0.36264077]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.214307

Running PBIRL with 7 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5036
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [0.0037162  0.31989319 0.94744632]
True reward weights: [ 0.44990503 -0.64597027  0.61669106]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.238375

Running PBIRL with 8 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.4254
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [ 0.19641297 -0.07801986  0.97741232]
True reward weights: [0.62214621 0.39005296 0.6788172 ]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.258377

Running PBIRL with 9 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.4322
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [0.26824419 0.31941468 0.90885605]
True reward weights: [0.11590346 0.55134914 0.82618431]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.250920

Running PBIRL with 10 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.4314
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.25645775  0.3188714   0.91244203]
True reward weights: [-0.75607468 -0.5550781   0.3467555 ]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.245306

Running experiment 44/50...
Shuffled Demos: [([(0, 0), (0, 2), (0, 1), (3, 3)], [(0, 0), (0, 2), (0, 3), (1, 1)]), ([(0, 3), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 0), (1, 2)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 3)]), ([], [(0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 2), (0, 1)]), ([(0, 3), (1, 1), (4, 3), (1, 1)], [(0, 3), (1, 2), (0, 0), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 1), (3, 2)]), ([(0, 1), (0, 1), (3, 3), (0, 1)], [(0, 1), (0, 3), (1, 1), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 3), (1, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.6530
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.89973411  0.43600174 -0.01951941]
True reward weights: [-0.88315502 -0.32822573  0.33511952]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.92430197  0.37935563 -0.04189483]
True reward weights: [-0.74961471  0.06804568 -0.65836735]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.137038

Running PBIRL with 3 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.4434
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.74398934 -0.00491989  0.66817338]
True reward weights: [-0.92380263 -0.3826074  -0.01415193]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.166229

Running PBIRL with 4 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.1318
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.82534051 -0.46263222  0.3237043 ]
True reward weights: [-0.28205315  0.85638137  0.43250082]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.256837

Running PBIRL with 5 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.0972
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.74631619 -0.34964259  0.56635872]
True reward weights: [-0.69317206 -0.4073151   0.59464856]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.236341

Running PBIRL with 6 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.0138
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.51222352 -0.5119748   0.68957151]
True reward weights: [-0.59733957 -0.44173966  0.6693665 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.456462

Running PBIRL with 7 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.0132
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.47330429 -0.4734823   0.74283078]
True reward weights: [-0.31100708 -0.34332623  0.88622892]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.398098

Running PBIRL with 8 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.0112
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.40089374 -0.39480997  0.82668573]
True reward weights: [-0.68318166 -0.68690359  0.24784324]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.438170

Running PBIRL with 9 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.0122
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.38266177 -0.37578242  0.84401276]
True reward weights: [-0.40089374 -0.39480997  0.82668573]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.490819

Running PBIRL with 10 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.0080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.67673649 -0.67004415  0.3050714 ]
True reward weights: [-0.56749906 -0.55556006  0.6076988 ]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.468781

Saving results to files...
Results saved successfully.

Running experiment 45/50...
Shuffled Demos: [([(0, 0), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 1), (3, 0)]), ([(0, 3), (1, 0), (2, 1), (2, 1), (5, None)], [(0, 3), (1, 0), (2, 3), (2, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 2), (1, 2)]), ([], [(0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 3)]), ([(0, 1), (3, 3), (3, 3), (4, 3)], [(0, 2), (0, 1), (3, 3), (4, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 1), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 0), (0, 2)]), ([(0, 1), (1, 0), (1, 1), (0, 1)], [(0, 1), (1, 0), (1, 3), (4, 0)]), ([(0, 1), (3, 0), (0, 1), (3, 3)], [(0, 1), (3, 0), (0, 0), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6688
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.10335242  0.4515669   0.88623113]
True reward weights: [-0.83381491 -0.37890594  0.40147601]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.5340
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.42933106 0.20686639 0.87913659]
True reward weights: [0.13376777 0.69273311 0.70867977]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.145133

Running PBIRL with 3 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.3424
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.28949753 -0.53845869  0.79136175]
True reward weights: [0.21430976 0.30354558 0.92840261]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.187678

Running PBIRL with 4 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.1952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.24731669 -0.71482401  0.65411091]
True reward weights: [-0.56068996 -0.35001857  0.75040906]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.226311

Running PBIRL with 5 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.1710
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.12108213 -0.50429168  0.85500235]
True reward weights: [ 0.17095393 -0.50169327  0.84798503]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.212334

Running PBIRL with 6 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.1766
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.64295424 -0.66220089  0.38483739]
True reward weights: [-0.77800915 -0.54839191  0.30654214]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.218638

Running PBIRL with 7 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.1670
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.25648477 -0.58747905  0.76751803]
True reward weights: [-0.09860022 -0.50916646  0.85500147]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.206019

Running PBIRL with 8 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.1712
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.3714381  -0.63242308  0.67976083]
True reward weights: [-0.71756363 -0.51051458  0.47379036]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.197021

Running PBIRL with 9 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.1770
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.63264725 -0.6594002   0.40613893]
True reward weights: [-0.56323512 -0.51880509  0.64312322]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.246650

Running PBIRL with 10 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.1676
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.51087024 -0.56864435  0.64471326]
True reward weights: [-0.16195263 -0.51384349  0.84245843]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.312454

Running experiment 46/50...
Shuffled Demos: [([(0, 0), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 1), (3, 0), (0, 0)]), ([(0, 2), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 2), (3, 2), (3, 0)]), ([(0, 3), (1, 1), (0, 1), (3, 3)], [(0, 3), (1, 3), (2, 0), (2, 3)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 3), (1, 3), (1, 1), (0, 0)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 1), (3, 1)]), ([(0, 3), (1, 0), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 3), (2, 0)]), ([(0, 1), (3, 2), (3, 3), (3, 3)], [(0, 1), (3, 2), (3, 0), (3, 3)]), ([(0, 0), (0, 2), (0, 1), (3, 3)], [(0, 0), (0, 2), (0, 2), (0, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (4, 3)], [(0, 3), (0, 2), (0, 1), (1, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 46
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.70934204 0.63839399 0.29880926]
True reward weights: [-0.82098277 -0.18440172  0.54035479]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6580
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.63738304  0.6387347   0.43099981]
True reward weights: [ 0.26857087  0.32359598 -0.90727908]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.184158

Running PBIRL with 3 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6610
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.36977967  0.63995549  0.67358738]
True reward weights: [-0.33056073  0.20630951  0.92095928]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.226185

Running PBIRL with 4 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.4504
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.63321763  0.63967058  0.43572582]
True reward weights: [0.12483844 0.87496966 0.46780708]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.275481

Running PBIRL with 5 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.4458
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.03893683  0.86451512  0.50109633]
True reward weights: [-0.22282877  0.6238344   0.74911814]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.257979

Running PBIRL with 6 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.4430
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.06056034  0.87199658  0.48575139]
True reward weights: [-0.85360165 -0.413558    0.31675544]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.247373

Running PBIRL with 7 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.4472
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.8451055   0.31951072  0.42861358]
True reward weights: [-0.85463174  0.49911555  0.1431372 ]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.310419

Running PBIRL with 8 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.4396
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.47224441 0.63843771 0.60776847]
True reward weights: [-0.62255203  0.47150788  0.62458729]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.383285

Running PBIRL with 9 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.3234
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.1987253   0.15891075  0.96708615]
True reward weights: [-0.84953175  0.23099973  0.47427306]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.434221

Running PBIRL with 10 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.3020
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.41611505  0.31902011  0.85151303]
True reward weights: [-0.73782681 -0.0193718   0.67471203]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.423936

Saving results to files...
Results saved successfully.

Running experiment 47/50...
Shuffled Demos: [([(0, 1), (0, 1), (3, 3), (0, 1)], [(0, 3), (1, 1), (2, 1), (5, None)]), ([(0, 1), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 1), (1, 1), (4, 2), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 1), (3, 2), (3, 3), (4, 3)]), ([(0, 2), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (3, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 0), (1, 0)]), ([(0, 3), (1, 1), (4, 3), (1, 1)], [(0, 3), (1, 1), (4, 2), (3, 2)]), ([(0, 2), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 1), (3, 0), (0, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 3), (2, 0)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (0, 2), (0, 1), (3, 2)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 0), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6626
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.81178183 -0.31435349 -0.49213021]
True reward weights: [-0.73857451 -0.23755048  0.63093381]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.3638
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.30490313  0.8548025   0.41993662]
True reward weights: [-0.41738375  0.5413918   0.72985322]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.171583

Running PBIRL with 3 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.0334
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.7879191  0.58350083 0.19674925]
True reward weights: [0.19406536 0.70660028 0.68048121]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.251995

Running PBIRL with 4 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.0316
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.77323122 0.60532264 0.18893913]
True reward weights: [8.54342879e-01 5.19709169e-01 7.90857265e-04]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.204881

Running PBIRL with 5 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.0018
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.5511194  0.58498912 0.59502532]
True reward weights: [0.73306135 0.61442809 0.29171764]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.707166

Running PBIRL with 6 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.0036
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.57522413 0.58258575 0.57420471]
True reward weights: [0.54376666 0.56992862 0.61603505]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.670469

Running PBIRL with 7 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.0008
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [0.57314903 0.56799565 0.5906616 ]
True reward weights: [0.57522413 0.58258575 0.57420471]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.674323

Running PBIRL with 8 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.0036
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [0.57488246 0.55797055 0.5984806 ]
True reward weights: [0.57564842 0.56637563 0.58978601]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.722424

Running PBIRL with 9 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.0032
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [0.56387104 0.56996151 0.59765654]
True reward weights: [0.57488246 0.55797055 0.5984806 ]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.693979

Running PBIRL with 10 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.0016
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [0.57923354 0.57385493 0.57894648]
True reward weights: [0.55734438 0.58434526 0.58983714]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.693436

Running experiment 48/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (0, 3)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 1), (1, 2), (4, 0), (1, 1)]), ([(0, 3), (3, 3), (4, 3), (5, None)], [(0, 3), (3, 0), (0, 0), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 3), (1, 1), (4, 2), (3, 1)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 0), (0, 2)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 3), (2, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 2), (0, 3)]), ([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 2), (3, 0)]), ([(0, 0), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 1), (3, 2), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.6622
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.39921361 0.47331626 0.78523895]
True reward weights: [-0.95541883 -0.18852986  0.22722531]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5638
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.17627556  0.76965012  0.61364943]
True reward weights: [ 0.31719763 -0.04471226  0.94730485]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.139428

Running PBIRL with 3 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5012
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.23205097 -0.11663767  0.96568525]
True reward weights: [-0.23904747  0.10492268  0.9653225 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152532

Running PBIRL with 4 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.4996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.70299369 -0.62413527  0.34096193]
True reward weights: [ 0.3318058  -0.03113156  0.94283389]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.168927

Running PBIRL with 5 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.2904
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.32258993 0.33454971 0.88544465]
True reward weights: [ 0.48660902 -0.19739193  0.85102766]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.231259

Running PBIRL with 6 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.2882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.10969067 0.45988508 0.88117743]
True reward weights: [-0.89708147 -0.31996155  0.30474489]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.213490

Running PBIRL with 7 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.2834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.89161228 -0.21493168  0.39853721]
True reward weights: [-0.27087332 -0.01125595  0.96254919]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.201919

Running PBIRL with 8 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.2850
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.83877163 -0.20244779  0.50544738]
True reward weights: [-0.44004039  0.15438183  0.88460766]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.194784

Running PBIRL with 9 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.2760
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [0.10889528 0.51927205 0.84764282]
True reward weights: [-0.73107726 -0.34013156  0.59146984]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.188583

Running PBIRL with 10 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.2834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [-0.91900062 -0.16037026  0.36016556]
True reward weights: [-0.90641637 -0.17947138  0.38236029]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.239335

Saving results to files...
Results saved successfully.

Running experiment 49/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 0), (0, 0), (0, 2), (0, 1)]), ([(0, 1), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 0), (0, 1)]), ([(0, 0), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 3), (1, 3), (2, 3), (2, 3)]), ([(0, 1), (3, 0), (3, 3), (3, 3)], [(0, 1), (3, 0), (3, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 3), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (0, 1), (0, 3)]), ([(0, 2), (0, 1), (3, 3), (4, 3)], [(0, 2), (0, 2), (0, 1), (3, 0)]), ([(0, 2), (0, 3), (1, 1), (0, 1)], [(0, 2), (0, 3), (1, 0), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 0), (1, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.6598
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.99153795 -0.03784938  0.12417697]
True reward weights: [-0.98484463 -0.1489005   0.08893646]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.4338
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.59461296 -0.62917326  0.50057611]
True reward weights: [-0.09039934 -0.71175674  0.69658474]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.161079

Running PBIRL with 3 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.4372
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.93201329 -0.0643137   0.35667208]
True reward weights: [-0.36672413 -0.41247151  0.83389488]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.172529

Running PBIRL with 4 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.4476
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.29070959 -0.83384401  0.46924631]
True reward weights: [0.95618906 0.07767745 0.28225643]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.221360

Running PBIRL with 5 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.4376
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.056315   -0.90308737  0.42574855]
True reward weights: [ 0.5291511  -0.48229272  0.69813526]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.288124

Running PBIRL with 6 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.4396
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.37725162 -0.868485    0.32158205]
True reward weights: [-0.42502481 -0.56806186  0.70474083]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.294338

Running PBIRL with 7 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.4290
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.2669675  -0.83997531  0.47240854]
True reward weights: [ 0.4965495  -0.26685241  0.82597118]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.291519

Running PBIRL with 8 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.4322
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.30557117 -0.87490501  0.37572262]
True reward weights: [0.29472211 0.01736875 0.95542514]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.367391

Running PBIRL with 9 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.0158
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.59515206 -0.59611865  0.53892169]
True reward weights: [ 0.20607896 -0.65200208  0.72967442]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.816425

Running PBIRL with 10 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.0214
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [0.31530552 0.31572076 0.89493175]
True reward weights: [-0.30996881 -0.31687209  0.89638798]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.755280

Running experiment 50/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 2), (3, 2)]), ([(0, 1), (3, 3), (0, 1), (3, 3)], [(0, 3), (1, 2), (0, 1), (3, 3)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (4, 0), (1, 1)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (1, 1), (4, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 0), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 0), (0, 1)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 0), (0, 1)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (0, 0), (0, 1), (3, 1)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 1), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (3, 3), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.6638
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.05239146  0.07294039  0.99595925]
True reward weights: [-0.47840873 -0.41855607  0.77196885]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.3254
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.12782627  0.44493219  0.88639483]
True reward weights: [ 0.57300331 -0.80640902  0.14619061]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.172086

Running PBIRL with 3 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.3180
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.86735431 -0.3129505   0.38698641]
True reward weights: [-0.69825172 -0.66927758  0.25399224]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.153053

Running PBIRL with 4 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.3302
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.52228476 -0.14031285  0.84114858]
True reward weights: [-0.35452435  0.02831223  0.93461805]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.145095

Running PBIRL with 5 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.3288
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.83940732 -0.09672023  0.53482758]
True reward weights: [-0.3089039   0.03877723  0.95030243]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.140504

Running PBIRL with 6 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.3162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.15243595  0.37196156  0.91564615]
True reward weights: [-0.67263594 -0.41105529  0.61530029]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.149460

Running PBIRL with 7 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.3088
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 7
MAP Solution: [-0.33172993  0.08498811  0.93953833]
True reward weights: [-0.83317648 -0.36307399  0.41712616]
MAP Policy for current environment:
Information gain 7 demonstrations: 0.146391

Running PBIRL with 8 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.3192
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 8
MAP Solution: [-0.70781978 -0.00806655  0.70634701]
True reward weights: [-0.53632571 -0.41505125  0.73490625]
MAP Policy for current environment:
Information gain 8 demonstrations: 0.144250

Running PBIRL with 9 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.3072
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 9
MAP Solution: [-0.62002485  0.21166973  0.75548998]
True reward weights: [-0.70635015  0.13622766  0.69463047]
MAP Policy for current environment:
Information gain 9 demonstrations: 0.141350

Running PBIRL with 10 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.3100
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 10
MAP Solution: [0.1255775  0.50673859 0.85290462]
True reward weights: [-0.82488719  0.15854646  0.54260865]
MAP Policy for current environment:
Information gain 10 demonstrations: 0.139787

Saving results to files...
Results saved successfully.
