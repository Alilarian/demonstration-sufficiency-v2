Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6346
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.83062886 -0.17331849  0.52916576]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6188
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.9946196   0.03172073 -0.09861866]
True reward weights: [-0.85601928  0.37004632 -0.36096635]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124114

Running PBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6168
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.65226203 -0.23942704  0.7191863 ]
True reward weights: [-0.27935166  0.89388578  0.35061498]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124014

Running experiment 2/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6202
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.50414538  0.75886104  0.41227098]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6216
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.81012181  0.18790854 -0.55533146]
True reward weights: [ 0.65858575  0.7523716  -0.01420504]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124106

Running PBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6138
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.24130372  0.30697071 -0.92062017]
True reward weights: [0.51573806 0.83151246 0.20640079]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124069

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6318
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.07274567 -0.31309292  0.94693236]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6194
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.51710816 -0.27256419  0.81136177]
True reward weights: [-0.00565477 -0.87725854  0.47998488]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123923

Running PBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6120
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.7479614  -0.2106032   0.62944422]
True reward weights: [-0.50128885  0.50256894  0.70436777]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124061

Running experiment 4/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6254
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.98891156 -0.04700864  0.14086914]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6210
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.77298542  0.20066024 -0.60185464]
True reward weights: [0.41592308 0.90222226 0.11403066]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123768

Running PBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6142
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.80337719 -0.18665121  0.56546125]
True reward weights: [ 0.53073414  0.84540558 -0.06008895]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123872

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6320
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.36805954 -0.29293689  0.88245122]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6194
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.17381408 -0.3125214   0.93387314]
True reward weights: [-0.67798986  0.32659208  0.65853426]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124247

Running PBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6222
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.76291009 -0.20436855  0.61335282]
True reward weights: [-0.47354861 -0.78764118  0.39417392]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124075

Running experiment 6/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6342
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.20041232  0.3100551  -0.92935501]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6182
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.43383658  0.60831746  0.664632  ]
True reward weights: [-0.83233548 -0.17086959  0.52727719]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124053

Running PBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6194
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.79339322  0.19372441 -0.57705983]
True reward weights: [-0.80727669 -0.09536206 -0.58241773]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123946

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6244
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.54596511 -0.26467405  0.79490235]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6256
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.51445809 -0.26880611  0.81429488]
True reward weights: [-0.77418883  0.25162384  0.58079006]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123947

Running PBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.29457344 -0.30167599  0.90676242]
True reward weights: [ 0.3268516  -0.89773057  0.29537747]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123958

Running experiment 8/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6180
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.06902585  0.31803428 -0.94556313]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6264
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.29740607 0.74770974 0.59370008]
True reward weights: [-0.97136344 -0.0160572  -0.23705533]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123897

Running PBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6238
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.99553914 -0.02855876  0.08992337]
True reward weights: [-0.14671225  0.01332895  0.98908941]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123928

Saving results to files...
Results saved successfully.

Running experiment 9/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6240
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.91369893 -0.37323834  0.16077128]
True reward weights: [-0.0505111  -0.01531811  0.99860602]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6246
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.22099022 -0.3061547   0.92597658]
True reward weights: [-0.3776382  -0.85888231 -0.34599214]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123952

Running PBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6002
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.83786533 -0.53796939 -0.09257767]
True reward weights: [0.02110457 0.22917923 0.97315542]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124148

Running experiment 10/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6150
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.23406794 -0.30754361  0.92229557]
True reward weights: [-0.76518493 -0.64328377  0.02603879]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6306
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.75872907 -0.20460214  0.6184401 ]
True reward weights: [ 0.93278139 -0.03423453  0.35881314]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123938

Running PBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6266
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.65182904 -0.24062209  0.71918003]
True reward weights: [ 0.0013658  -0.90339445  0.42880835]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124088

Saving results to files...
Results saved successfully.

Running experiment 11/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6186
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.45076308 -0.28121449  0.84719009]
True reward weights: [-0.87829137 -0.24439244  0.41094598]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6208
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.38152233 -0.29192631  0.87705173]
True reward weights: [ 0.23284634 -0.91440903 -0.33111735]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123905

Running PBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6226
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.44929845 -0.28528501  0.84660697]
True reward weights: [-0.10963078 -0.82091261  0.56043159]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124275

Running experiment 12/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6114
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.88067211  0.15079383 -0.44908558]
True reward weights: [-0.8497926  -0.50850291  0.13884283]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6256
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.99700365  0.02490103 -0.07323703]
True reward weights: [0.40461763 0.52745515 0.7470446 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124219

Running PBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6134
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.87424145 -0.15470091  0.46018422]
True reward weights: [-0.19731824  0.90254053  0.38273502]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124037

Saving results to files...
Results saved successfully.

Running experiment 13/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6114
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.95985459  0.08617389 -0.26693299]
True reward weights: [-0.34477193 -0.32328333  0.88126058]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6384
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.42493854  0.28479237 -0.85925581]
True reward weights: [-0.48701713  0.87313659  0.02113778]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124039

Running PBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6228
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.00427697  0.89408513 -0.44787665]
True reward weights: [-0.18394517  0.50698541  0.84209855]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124043

Running experiment 14/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.6220
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.8104647   0.1854076  -0.55567166]
True reward weights: [-0.43088031 -0.17122291  0.88601629]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.6238
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.99559222 -0.02689407  0.08984899]
True reward weights: [-0.18843653  0.83918413 -0.51015848]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123825

Running PBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.6284
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.75964155 -0.20856726  0.61599059]
True reward weights: [-0.58884113 -0.04492344 -0.80699939]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124018

Saving results to files...
Results saved successfully.

Running experiment 15/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.6224
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.92135294 -0.38786388 -0.02589143]
True reward weights: [-0.75433776 -0.02805593  0.65588674]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.6184
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.10719995  0.68210512 -0.72335384]
True reward weights: [-0.94304314 -0.1124759  -0.31307955]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124149

Running PBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.6166
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.95851451  0.09281027 -0.269511  ]
True reward weights: [ 0.00091833  0.44879663 -0.89363345]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124135

Running experiment 16/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.6320
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.99780838 -0.0188277   0.06343461]
True reward weights: [-0.92032338 -0.11683579  0.37330186]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.6036
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.71211802  0.21999569 -0.66670069]
True reward weights: [-0.06897074  0.23118624  0.97046173]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123852

Running PBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.6112
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.35679514  0.29374988 -0.88679661]
True reward weights: [-0.93722384 -0.3439901   0.05729127]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123700

Saving results to files...
Results saved successfully.

Running experiment 17/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6230
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.06300277  0.31366241 -0.94744211]
True reward weights: [-0.76208548 -0.28664819  0.58056743]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6104
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.66386752  0.23386635 -0.71034248]
True reward weights: [-0.13227341  0.56795684 -0.81236   ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123870

Running PBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6304
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.96657157 -0.08202928  0.24292095]
True reward weights: [-0.55424049  0.69778903 -0.45377082]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124097

Running experiment 18/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6184
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.14646192 0.83790012 0.52580633]
True reward weights: [-0.39174849 -0.30341429  0.86860399]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6066
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.80912694 -0.18491897  0.55778003]
True reward weights: [-0.99999414 -0.0019654  -0.00280226]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123974

Running PBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6152
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.96299735 -0.08312823  0.25637044]
True reward weights: [-0.6624848  -0.45143753  0.59776087]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124091

Saving results to files...
Results saved successfully.

Running experiment 19/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6218
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.23707784  0.30945581 -0.9208861 ]
True reward weights: [-0.92648321 -0.16748955  0.33701054]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6136
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.99154692  0.04170459 -0.12286351]
True reward weights: [ 0.24387705  0.4435694  -0.86242111]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124067

Running PBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.18967091  0.98183314 -0.00535072]
True reward weights: [-0.65962444 -0.42541661  0.6196098 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123974

Running experiment 20/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6066
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.29573644 -0.30390219  0.90563978]
True reward weights: [-0.61012931 -0.3129562   0.72787406]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6238
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.8659619  -0.1578375   0.47454959]
True reward weights: [-0.80273204 -0.14957813  0.57727607]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123973

Running PBIRL with 3 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6210
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.91581612 -0.12482864  0.38170493]
True reward weights: [-0.37412701 -0.85560804  0.35772037]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124231

Saving results to files...
Results saved successfully.

Running experiment 21/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6234
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.56060467  0.26464048 -0.78465777]
True reward weights: [-0.3738588  -0.3262014   0.86822937]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6202
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.98333805 -0.05677737  0.17269226]
True reward weights: [-0.05021331  0.0497941   0.99749645]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123996

Running PBIRL with 3 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6216
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.78807212 -0.19325313  0.58446178]
True reward weights: [-0.28746644  0.51248785 -0.80914724]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123966

Running experiment 22/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6182
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.89163395 -0.14427447  0.42915473]
True reward weights: [-0.70965126 -0.6051058   0.36089066]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6200
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.23289786  0.28250132  0.9305652 ]
True reward weights: [-0.66321427 -0.60728049  0.43744397]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123947

Running PBIRL with 3 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6210
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.06383427 -0.315489    0.94677974]
True reward weights: [ 0.32285823 -0.82981818  0.45515311]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124289

Saving results to files...
Results saved successfully.

Running experiment 23/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6158
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.92468367 -0.12002626  0.3613223 ]
True reward weights: [-0.74528522 -0.55533186  0.36899385]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6196
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.98952841 -0.04375547  0.13754627]
True reward weights: [-0.59778578  0.74750575 -0.28963306]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123761

Running PBIRL with 3 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6164
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.83583094 -0.25236192  0.48754498]
True reward weights: [-0.21678165  0.21665791  0.9518745 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124025

Running experiment 24/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6394
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.29327774 -0.30403868  0.90639321]
True reward weights: [-0.68144434 -0.29187893  0.6711485 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6110
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.22907757 -0.30551445  0.92422096]
True reward weights: [0.11576045 0.37668247 0.91908097]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124011

Running PBIRL with 3 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6124
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.11430262 -0.31381785  0.94257799]
True reward weights: [ 0.66930551 -0.51274087  0.53770525]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124124

Saving results to files...
Results saved successfully.

Running experiment 25/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6304
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.89042876 -0.14365094  0.43185765]
True reward weights: [-0.44714936 -0.30119858  0.84222139]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6128
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.96667142 -0.25377875 -0.03380398]
True reward weights: [-0.54474573 -0.09727467  0.83294041]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123554

Running PBIRL with 3 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6194
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.99990348  0.0013416  -0.01382896]
True reward weights: [-0.2542697   0.84901329 -0.46316666]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124178

Running experiment 26/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6300
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.73674104 -0.21457062  0.64122702]
True reward weights: [-0.7333898  -0.47830978  0.48307263]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6268
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.08473002 -0.31267149  0.94607471]
True reward weights: [ 0.87233516 -0.04421162  0.48690523]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123774

Running PBIRL with 3 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6172
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.11176441 -0.10143479  0.98854423]
True reward weights: [-0.77652393  0.44428947  0.44678568]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124398

Saving results to files...
Results saved successfully.

Running experiment 27/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6246
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.79470388 -0.18922947  0.57674774]
True reward weights: [-0.65557811 -0.01374154  0.75500233]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123672

Running PBIRL with 2 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6252
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.09021572  0.31731961 -0.94401769]
True reward weights: [-0.84004081  0.5389676   0.062011  ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124328

Running PBIRL with 3 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6156
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.90116488 -0.13900031  0.41058589]
True reward weights: [-0.7677224  -0.24310378 -0.59287677]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124303

Running experiment 28/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6282
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.59444916  0.25474515 -0.76271561]
True reward weights: [-0.866301   -0.34967581  0.35672034]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123673

Running PBIRL with 2 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6060
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.95760952  0.09246282 -0.27282711]
True reward weights: [-0.60174791 -0.46227689  0.65130602]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124462

Running PBIRL with 3 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6136
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.92969467 -0.11602963  0.34957823]
True reward weights: [-0.24061744  0.95242353  0.18706325]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124047

Saving results to files...
Results saved successfully.

Running experiment 29/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6142
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.77434101  0.19863244 -0.60078379]
True reward weights: [-0.6535586  -0.23072892  0.72085041]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6204
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.9655068  -0.08245872  0.24697605]
True reward weights: [ 0.21131532  0.97739665 -0.00645221]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123970

Running PBIRL with 3 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.94649832 -0.10179502  0.30623308]
True reward weights: [ 0.28799962  0.57469951 -0.7660135 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124204

Running experiment 30/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.6226
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.92439956 -0.11981381  0.3621189 ]
True reward weights: [-0.81326386 -0.04441834  0.5801973 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.6252
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.2460828   0.3037193  -0.92043351]
True reward weights: [-0.73312232  0.42398636 -0.53175863]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124250

Running PBIRL with 3 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.6152
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.42857956  0.28411651 -0.85766973]
True reward weights: [-0.51496349  0.78998301  0.33277536]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124220

Saving results to files...
Results saved successfully.

Running experiment 31/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6164
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.41607072 -0.28455816  0.86366186]
True reward weights: [-0.27534761 -0.19938474  0.94044108]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6260
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.44208719 -0.28285438  0.85120639]
True reward weights: [-0.527152   -0.32789456  0.78396169]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124004

Running PBIRL with 3 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6248
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.93115273 -0.11317774  0.34661996]
True reward weights: [0.85402949 0.27403549 0.44219699]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123787

Running experiment 32/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6114
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.73481281 -0.37575162 -0.56467766]
True reward weights: [-0.90753571 -0.40234227  0.1204144 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6232
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.98313936 -0.18075873 -0.02762768]
True reward weights: [-0.59979891  0.7319632   0.32321995]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123728

Running PBIRL with 3 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6168
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.16810826  0.90776137  0.38432917]
True reward weights: [-0.88386341  0.41987444 -0.20613327]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124096

Saving results to files...
Results saved successfully.

Running experiment 33/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6234
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.76294978  0.20449628 -0.61326088]
True reward weights: [-0.32556312 -0.30396656  0.89532842]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6146
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.8099682   0.18266449 -0.55730171]
True reward weights: [ 0.51088123  0.8509792  -0.12179808]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123975

Running PBIRL with 3 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6238
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.97494455  0.07286602 -0.21017534]
True reward weights: [-0.29516754  0.36374704  0.88349545]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124022

Running experiment 34/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.6104
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.91612414 -0.12428531  0.38114266]
True reward weights: [-0.839382   -0.393236    0.37523766]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.6134
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.88993119  0.2786641   0.3610662 ]
True reward weights: [-0.65809756  0.66562896  0.35191715]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124076

Running PBIRL with 3 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.6222
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.89239895  0.14118579 -0.42859151]
True reward weights: [-0.27829829  0.01010128 -0.96044158]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124139

Saving results to files...
Results saved successfully.

Running experiment 35/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6234
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.97521544 -0.07182984  0.20927332]
True reward weights: [-0.94121751 -0.05112011  0.33391067]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6108
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.87482308 -0.33181329  0.35296531]
True reward weights: [-0.61529344  0.7842885  -0.07940737]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123876

Running PBIRL with 3 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6176
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.84165297 -0.17236614  0.51177163]
True reward weights: [-0.89807471 -0.40259065 -0.17715129]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123745

Running experiment 36/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.6274
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.96229178 -0.25911938 -0.08277485]
True reward weights: [-0.05456508 -0.040557    0.99768621]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.6262
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.99879503  0.01381853 -0.04709082]
True reward weights: [-0.7828547   0.02126341 -0.62184113]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123866

Running PBIRL with 3 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.6190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.37427199  0.223911   -0.89988018]
True reward weights: [-0.64267175  0.63744037  0.42502094]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124052

Saving results to files...
Results saved successfully.

Running experiment 37/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6218
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.97555837 -0.06911381  0.20858847]
True reward weights: [-0.75478167 -0.33513042  0.563908  ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6152
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.69797931  0.22786481 -0.67889802]
True reward weights: [-0.50564939 -0.38525014 -0.77194626]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123948

Running PBIRL with 3 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6198
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.66758023  0.23533703 -0.70636613]
True reward weights: [-0.70724657  0.70180336  0.08528969]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124046

Running experiment 38/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6262
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.62326475  0.24985257 -0.74102277]
True reward weights: [-0.49248773 -0.22865786  0.83974486]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6114
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.71393612  0.22217654 -0.66402772]
True reward weights: [-0.69221432  0.05013677 -0.71994836]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124161

Running PBIRL with 3 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6186
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.65293302 -0.24091623  0.71807928]
True reward weights: [0.02374004 0.97719574 0.21100924]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124218

Saving results to files...
Results saved successfully.

Running experiment 39/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.6108
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.87712429 -0.15180409  0.45564075]
True reward weights: [-0.56481614 -0.46100063  0.68444222]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.6226
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.89491777 -0.14394466  0.42237674]
True reward weights: [0.22755523 0.09044156 0.96955605]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123562

Running PBIRL with 3 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.6126
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.84339915 -0.17160308  0.5091466 ]
True reward weights: [ 0.92324857 -0.28272616 -0.26014995]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124088

Running experiment 40/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6182
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.32370026  0.29787856 -0.89804593]
True reward weights: [-0.6667631  -0.57474284  0.47444455]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6250
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90259733 -0.13344573  0.40928022]
True reward weights: [-0.89298096  0.20232675  0.40205582]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123852

Running PBIRL with 3 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6102
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.71614845 -0.2206206   0.66216157]
True reward weights: [-0.49539716  0.86510681  0.07856117]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124265

Saving results to files...
Results saved successfully.

Running experiment 41/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.6272
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.2865021  -0.30072537  0.90965972]
True reward weights: [-0.46247666 -0.0267168   0.88622884]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.6240
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.71493582 -0.21847477  0.66418036]
True reward weights: [-0.70578532 -0.1425381   0.69393802]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123976

Running PBIRL with 3 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.6222
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.38379357 -0.29125752  0.87628281]
True reward weights: [-0.9933883   0.00864402  0.1144769 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124200

Running experiment 42/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.6288
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.35691425 -0.29781747  0.88539086]
True reward weights: [-0.71505276 -0.68463496  0.14133129]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.6082
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.0221019  -0.31575054  0.94858479]
True reward weights: [-0.72568005 -0.59219414  0.35027784]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123632

Running PBIRL with 3 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.6318
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.30186528 -0.30346227  0.90376324]
True reward weights: [-0.61377281 -0.66502135 -0.42547567]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124546

Saving results to files...
Results saved successfully.

Running experiment 43/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.6238
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.51053555 -0.27426161  0.81494418]
True reward weights: [-0.94655708 -0.29794542  0.12352418]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.6134
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.58700732 -0.25653302  0.76786276]
True reward weights: [-0.69582683 -0.07641668  0.7141327 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123999

Running PBIRL with 3 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.6142
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.44474739 0.22447252 0.86707084]
True reward weights: [-0.66213616 -0.07061317  0.74604925]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124071

Running experiment 44/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.6252
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.99969095  0.00652837 -0.02398706]
True reward weights: [-0.88315502 -0.32822573  0.33511952]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.6140
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.82018835 -0.18033078  0.54292899]
True reward weights: [-0.96249192 -0.17242013 -0.20947696]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124082

Running PBIRL with 3 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.6226
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.90495643 -0.1367467   0.40293201]
True reward weights: [-0.83490665 -0.02626864  0.54976436]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124085

Saving results to files...
Results saved successfully.

Running experiment 45/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6220
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.59790886 -0.66587724  0.44622023]
True reward weights: [-0.83381491 -0.37890594  0.40147601]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6198
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.95050412 -0.09728811  0.29508803]
True reward weights: [ 0.87158104 -0.15420484  0.46536797]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123865

Running PBIRL with 3 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6236
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.73203659 -0.21273401  0.6471991 ]
True reward weights: [-0.674463   -0.49312924  0.5494754 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123806

Running experiment 46/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6144
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.89251742 -0.14333518  0.42763032]
True reward weights: [-0.82098277 -0.18440172  0.54035479]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6226
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.11567451 -0.31603323  0.94167001]
True reward weights: [ 0.4548739  -0.72044403  0.52349798]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124086

Running PBIRL with 3 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.77879384 -0.19689014  0.59557907]
True reward weights: [-0.45043475 -0.68450177 -0.57320665]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124172

Saving results to files...
Results saved successfully.

Running experiment 47/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6198
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.82372042 -0.18101873  0.53732383]
True reward weights: [-0.73857451 -0.23755048  0.63093381]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6242
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90118688  0.13513858 -0.41182492]
True reward weights: [-0.69440461 -0.71525896  0.07878359]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123932

Running PBIRL with 3 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6126
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.53865101  0.72293315  0.43269244]
True reward weights: [0.13735583 0.32165515 0.93684115]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123676

Running experiment 48/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.6174
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.51375387  0.30619322 -0.80143788]
True reward weights: [-0.95541883 -0.18852986  0.22722531]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.6192
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.71802174  0.22044649 -0.66018794]
True reward weights: [-0.85688437  0.49309866 -0.15034258]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124051

Running PBIRL with 3 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.6132
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.43198825  0.28551394 -0.8554928 ]
True reward weights: [0.34965333 0.44041667 0.82690731]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124063

Saving results to files...
Results saved successfully.

Running experiment 49/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.6000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.94918004 -0.09935004  0.29864163]
True reward weights: [-0.98484463 -0.1489005   0.08893646]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.6150
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.37822311  0.29488685 -0.87749019]
True reward weights: [ 0.19437836  0.69698574 -0.69023759]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124382

Running PBIRL with 3 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.6130
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.656495   -0.23742138  0.7159926 ]
True reward weights: [-0.49384245  0.84680143 -0.19760309]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123800

Running experiment 50/50...
Same Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.6264
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.79410717 -0.19474727  0.57573197]
True reward weights: [-0.47840873 -0.41855607  0.77196885]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.6306
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.99462048 -0.03167662  0.09862399]
True reward weights: [-0.90542349  0.33534303 -0.26029475]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123885

Running PBIRL with 3 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.6266
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.66283854  0.23714233 -0.71021728]
True reward weights: [0.07128831 0.93979586 0.33421808]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123717

Saving results to files...
Results saved successfully.
