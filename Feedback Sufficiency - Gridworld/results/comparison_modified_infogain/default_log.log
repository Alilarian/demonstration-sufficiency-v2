Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6206
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.05573469  0.76004915  0.64747119]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000026

Running PBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.2732
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.86865975 -0.45706396  0.19110934]
True reward weights: [-0.71901359 -0.68150582  0.13626914]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.410523

Running PBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.2888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.78009811 -0.19837903  0.59337399]
True reward weights: [-0.99709303 -0.07384555 -0.01877012]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.255152

Running experiment 2/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6302
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.50213441  0.27480859 -0.81996419]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000014

Running PBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2736
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.7931745  -0.31368398  0.52199288]
True reward weights: [-0.77350563 -0.6310631   0.05872309]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.386874

Running PBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2732
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.04960611 0.50896173 0.8593586 ]
True reward weights: [-0.13249851  0.33299189  0.93357407]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.240532

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6206
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.8842897  -0.14820966  0.44279297]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000080

Running PBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2660
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.47249531  0.297206    0.82970885]
True reward weights: [-0.0591707   0.88417655  0.4633904 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.407241

Running PBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2740
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.17499653  0.29955012  0.93789442]
True reward weights: [-0.60694264 -0.57506007 -0.54856773]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.259400

Running experiment 4/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6344
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.7180628  -0.22175278  0.65970563]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000018

Running PBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.2868
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.84730414 -0.16778241  0.50390947]
True reward weights: [0.23462106 0.02906595 0.97165227]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.418627

Running PBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.2896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.09436859  0.38188807  0.91937809]
True reward weights: [-0.88705305  0.17607917  0.42677045]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.267562

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6312
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.5807476   0.25954653 -0.77160082]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000023

Running PBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6252
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.30498389  0.30192315 -0.90323155]
True reward weights: [-0.27216645  0.49913212  0.82267402]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.001773

Running PBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2786
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.71822961  0.13967248  0.68164347]
True reward weights: [-0.98727023 -0.06458438 -0.14534907]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.572101

Running experiment 6/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6270
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.69886747 -0.22681494  0.67833565]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000078

Running PBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6146
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.17056187  0.30917792 -0.93558413]
True reward weights: [ 0.20632633  0.69590053 -0.68786038]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.005854

Running PBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.2846
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.91835784 -0.00152866  0.39574808]
True reward weights: [-0.51024105  0.85953858  0.02911192]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.605227

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6322
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.16641629  0.31206549 -0.93537198]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000025

Running PBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6288
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.76373399  0.20572499 -0.61187222]
True reward weights: [-0.68211619  0.04046158  0.73012353]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.003540

Running PBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.2816
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.9401782  -0.1096088   0.32256915]
True reward weights: [0.32559801 0.73033995 0.60049104]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.568515

Running experiment 8/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6274
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.96718087 -0.08214453  0.24044428]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000033
Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.99378019  0.03352769 -0.10619237]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000009

Running PBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6302
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.76256481 -0.10731068 -0.63794932]
True reward weights: [-0.99001857  0.09068611  0.10788537]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.999905

Running PBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.2792
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.87035676 -0.35224381  0.34409796]
True reward weights: [-0.13785721  0.65682986  0.74132984]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.592922

Running experiment 2/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6230
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.48475042  0.27732833 -0.82952157]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000043

Running PBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2830
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.8259113  -0.33922866  0.45032705]
True reward weights: [-0.06890007  0.98453049  0.16109779]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.383842

Running PBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2850
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.78945604 -0.19410157  0.58230897]
True reward weights: [-0.88273933 -0.36071443 -0.30109197]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.238309

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6228
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.8561631  -0.16176052  0.4907324 ]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000037

Running PBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.77974359 -0.19755017  0.59411603]
True reward weights: [ 0.18443415 -0.98061547 -0.06616143]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.404443

Running PBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2918
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.63101652 -0.24697474  0.73540576]
True reward weights: [-0.87057352 -0.45670908  0.18308076]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.259159

Running experiment 4/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6236
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.18126634 -0.30814507  0.93391066]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000032

Running PBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.2848
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.67935867 -0.2330565   0.69581353]
True reward weights: [ 0.54118285 -0.05798147  0.83890362]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.416280

Running PBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.2864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.85969761 -0.35974393  0.36263524]
True reward weights: [-0.89793369  0.20509749  0.38942277]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.253631

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6348
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.83938129  0.1740374  -0.51492721]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000028

Running PBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.9481058  -0.09936535  0.30202967]
True reward weights: [-0.81523009  0.02290316 -0.57868416]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.405886

Running PBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.23609585  0.45444101  0.85891915]
True reward weights: [-0.8201247  -0.48796505  0.29880695]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.251700

Running experiment 6/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6168
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.90898962 -0.1329833   0.39503584]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000034

Running PBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6090
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.92838199  0.11465054 -0.35349984]
True reward weights: [0.28919986 0.95644662 0.03966489]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.002786

Running PBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.2868
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.57918804  0.04018046  0.81420313]
True reward weights: [-0.46406248 -0.21946623 -0.85818447]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.578888

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6208
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.73523688 -0.21629662  0.64237255]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000012

Running PBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.2800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.88310091 -0.14816694  0.44517338]
True reward weights: [-0.67744171 -0.71071585 -0.18961992]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.393428

Running PBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.2854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.85833764  0.14234717  0.492944  ]
True reward weights: [-0.88441456 -0.07141243  0.4612062 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.248637

Running experiment 8/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6240
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.65992797 -0.2362604   0.71321532]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000084

Running PBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.2896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.68160728 -0.23242414  0.69382313]
True reward weights: [-0.31244377  0.74459512 -0.5898788 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.402069

Running PBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.2748
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.8509552  -0.15814703  0.50086402]
True reward weights: [-0.95529739 -0.27188051 -0.11613738]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.254153

Saving results to files...
Results saved successfully.

Running experiment 9/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6428
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.65698935 -0.23735031  0.7155626 ]
True reward weights: [-0.0505111  -0.01531811  0.99860602]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000029

Running PBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.2784
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.69225785 -0.22834425  0.68457138]
True reward weights: [ 0.36706003 -0.90916118  0.19670507]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.403195

Running PBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.2826
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.68478995 -0.22935666  0.69170677]
True reward weights: [-0.1744442  -0.15630134  0.97218265]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.246886

Running experiment 10/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6064
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.17788562  0.30962072 -0.93407265]
True reward weights: [-0.76518493 -0.64328377  0.02603879]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000035

Running PBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.2814
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.68039989 -0.23340474  0.6946785 ]
True reward weights: [ 0.44790903  0.89407234 -0.00348696]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.411726

Running PBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.2830
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.81603744  0.01295271  0.5778539 ]
True reward weights: [0.29922016 0.49744788 0.81425604]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.257492

Saving results to files...
Results saved successfully.

Running experiment 11/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6300
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.93846103 -0.10894021  0.32775437]
True reward weights: [-0.87829137 -0.24439244  0.41094598]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000021

Running PBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6232
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.94767708 -0.10033885  0.30305159]
True reward weights: [-0.39684023  0.61286279  0.68331328]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.003430

Running PBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.2824
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.86167403 -0.01557725  0.50722304]
True reward weights: [-0.25243943  0.74921437 -0.61233337]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.580431

Running experiment 12/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6294
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.84550583 -0.16898424  0.50652168]
True reward weights: [-0.8497926  -0.50850291  0.13884283]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000049

Running PBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.2784
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.5856524  -0.19485153  0.78679359]
True reward weights: [-0.46030376 -0.87561755  0.14633643]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.391885

Running PBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.2902
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.89033108 -0.14351128  0.4321054 ]
True reward weights: [-0.92307586 -0.30064756 -0.23987915]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.242045

Saving results to files...
Results saved successfully.

Running experiment 13/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6164
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.98330046  0.05789942 -0.17253365]
True reward weights: [-0.34477193 -0.32328333  0.88126058]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000015

Running PBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6298
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.25476687  0.30664409 -0.917095  ]
True reward weights: [-0.32554697  0.94491754 -0.03391174]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.001671

Running PBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.2794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.93057808 -0.11659282  0.34703106]
True reward weights: [-0.0350181   0.36746734  0.92937693]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.575249

Running experiment 14/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.6140
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.31004178 0.77752199 0.54711392]
True reward weights: [-0.43088031 -0.17122291  0.88601629]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000025

Running PBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.2858
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.78148626 -0.19740235  0.59187122]
True reward weights: [-0.65426329  0.75145396  0.0851851 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.386364

Running PBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.2788
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.93032244 -0.32584248  0.16830577]
True reward weights: [-0.82973828  0.2365898   0.50552909]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.251308

Saving results to files...
Results saved successfully.

Running experiment 15/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.6276
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.56505967 -0.26208317  0.78231706]
True reward weights: [-0.75433776 -0.02805593  0.65588674]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000011

Running PBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.2816
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.77656808 -0.19948005  0.59762005]
True reward weights: [-0.7989159   0.30909192  0.51594143]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.394378

Running PBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.2766
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.70249762 -0.22563998  0.6749694 ]
True reward weights: [-0.95382843 -0.05209085  0.2958004 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.249037

Running experiment 16/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.6230
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.72957877  0.21491855 -0.64924944]
True reward weights: [-0.92032338 -0.11683579  0.37330186]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000021

Running PBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.2774
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.94643093 -0.10380511  0.30576623]
True reward weights: [-0.18533044  0.63449734  0.75037708]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.404117

Running PBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.2774
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.91907858 -0.12320669  0.37431894]
True reward weights: [-0.82225205 -0.56727086 -0.04588403]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.255030

Saving results to files...
Results saved successfully.

Running experiment 17/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.64894616 -0.23782552  0.72270873]
True reward weights: [-0.76208548 -0.28664819  0.58056743]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000017

Running PBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.2926
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.94149832 -0.10698268  0.31958665]
True reward weights: [0.66750251 0.72040213 0.18831139]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.395670

Running PBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.2752
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.71404887 -0.22227056  0.66387499]
True reward weights: [-0.48769209  0.07751349  0.86956776]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.253125

Running experiment 18/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6166
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.90300332 -0.13609606  0.40750811]
True reward weights: [-0.39174849 -0.30341429  0.86860399]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000013

Running PBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6296
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.8657915   0.157745   -0.47489113]
True reward weights: [-0.55823265  0.42000885 -0.71552001]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.001012

Running PBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.2774
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.89350073 -0.14060615  0.42648137]
True reward weights: [-0.51539529  0.29422142  0.80486114]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.604313

Saving results to files...
Results saved successfully.

Running experiment 19/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6254
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.31400271  0.29945482 -0.9009601 ]
True reward weights: [-0.92648321 -0.16748955  0.33701054]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000018

Running PBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.2958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.74501816  0.22962698  0.62627422]
True reward weights: [-0.9871251   0.12072849 -0.10492222]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.410613

Running PBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.2760
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.7763473  -0.40128726  0.48605905]
True reward weights: [-0.46347113  0.49729131  0.73341384]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.248064

Running experiment 20/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6278
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.3097212  -0.1254912   0.94250981]
True reward weights: [-0.61012931 -0.3129562   0.72787406]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000024

Running PBIRL with 2 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.2920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90787316 -0.13233795  0.39781025]
True reward weights: [-0.95867875  0.06852142  0.2761157 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.411050

Running PBIRL with 3 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.2800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.64085283 -0.24176186  0.72860062]
True reward weights: [0.1406032  0.58823854 0.79637062]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.257198

Saving results to files...
Results saved successfully.

Running experiment 21/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6242
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.80930364 -0.18671918  0.5569233 ]
True reward weights: [-0.3738588  -0.3262014   0.86822937]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000013

Running PBIRL with 2 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.2834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.69255348 -0.22793789  0.68440777]
True reward weights: [-0.55005599 -0.79649317 -0.25107178]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.411256

Running PBIRL with 3 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.2814
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.90966058 -0.13312882  0.39343913]
True reward weights: [-0.64713111  0.16427993  0.74446856]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.254429

Running experiment 22/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6184
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.5684636  -0.26048371  0.78038284]
True reward weights: [-0.70965126 -0.6051058   0.36089066]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000020

Running PBIRL with 2 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.2750
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.71217649 -0.22465825  0.66508144]
True reward weights: [ 0.65745043 -0.26684384  0.70466538]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.404226

Running PBIRL with 3 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.2734
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.83138173 -0.17587174  0.52713712]
True reward weights: [-0.65877398  0.4248861   0.62087732]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.250551

Saving results to files...
Results saved successfully.

Running experiment 23/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6174
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.9400367  -0.10527038  0.32442126]
True reward weights: [-0.74528522 -0.55533186  0.36899385]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000021

Running PBIRL with 2 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.2864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.84101411 -0.47040613  0.26723274]
True reward weights: [-0.55606591 -0.36913132  0.74466958]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.421188

Running PBIRL with 3 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.2792
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.7437174  -0.21059837  0.6344547 ]
True reward weights: [-0.13340234  0.44901904  0.88350762]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.267042

Running experiment 24/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6186
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.94222542 -0.10313244  0.31870827]
True reward weights: [-0.68144434 -0.29187893  0.6711485 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000038

Running PBIRL with 2 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.2842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.70050664 -0.2263987   0.67678215]
True reward weights: [-0.49361515  0.4137959   0.76492943]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.419554

Running PBIRL with 3 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.2766
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.78728326 -0.20570343  0.58126687]
True reward weights: [-0.72530054  0.08712469  0.68289707]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.264391

Saving results to files...
Results saved successfully.

Running experiment 25/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6198
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.40305073 -0.29136663  0.86755726]
True reward weights: [-0.44714936 -0.30119858  0.84222139]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000028

Running PBIRL with 2 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.2808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.83798046 -0.17433011  0.51710518]
True reward weights: [-0.76142509 -0.38284403  0.52312741]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.412252

Running PBIRL with 3 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.2898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.76164367 -0.20603256  0.61436919]
True reward weights: [-0.95326716  0.08143744  0.29094615]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.257235

Running experiment 26/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6194
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.51000933 -0.27124485  0.81628225]
True reward weights: [-0.7333898  -0.47830978  0.48307263]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000012

Running PBIRL with 2 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.2784
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.87541387 -0.1548685   0.45789332]
True reward weights: [ 0.4117006  -0.896349   -0.16450253]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.396028

Running PBIRL with 3 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.2814
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.81291416 -0.18152134  0.553372  ]
True reward weights: [-0.11971829  0.68486978  0.71876346]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.248857

Saving results to files...
Results saved successfully.

Running experiment 27/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6268
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.85486172  0.1617342  -0.49300455]
True reward weights: [-0.65557811 -0.01374154  0.75500233]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000023

Running PBIRL with 2 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.2882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.67421179 -0.23274943  0.70090382]
True reward weights: [0.21074325 0.84658762 0.48875012]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.435756

Running PBIRL with 3 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.2828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.92743087 -0.12124466  0.35379614]
True reward weights: [-0.40130961 -0.22086171  0.88891546]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.266682

Running experiment 28/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6220
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.23882637  0.30820276 -0.92085451]
True reward weights: [-0.866301   -0.34967581  0.35672034]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000007

Running PBIRL with 2 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6182
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.1865333   0.31304164 -0.93124125]
True reward weights: [-0.57194965  0.81103084  0.12289255]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.999899

Running PBIRL with 3 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.2800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.80511313 -0.18896163  0.56221557]
True reward weights: [-0.14262097  0.87292891  0.46653444]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.597894

Saving results to files...
Results saved successfully.

Running experiment 29/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6198
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.22195494  0.310544   -0.92428266]
True reward weights: [-0.6535586  -0.23072892  0.72085041]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000020

Running PBIRL with 2 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.2790
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.77494734 -0.20134508  0.59909664]
True reward weights: [ 0.16622234  0.1697221  -0.97137251]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.423047

Running PBIRL with 3 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.2850
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.42261975  0.14865667  0.89403229]
True reward weights: [-0.71769751 -0.54219561  0.43696018]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.265081

Running experiment 30/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.6230
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.10959281 -0.31328986  0.94331272]
True reward weights: [-0.81326386 -0.04441834  0.5801973 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000021

Running PBIRL with 2 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.2850
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.81368105  0.02770894  0.58065081]
True reward weights: [-0.14160376 -0.47403003  0.8690477 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.417504

Running PBIRL with 3 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.2872
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.77896641 -0.19966579  0.59442821]
True reward weights: [-0.82917833  0.31898836  0.45903129]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.266771

Saving results to files...
Results saved successfully.

Running experiment 31/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6180
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.80886271  0.18625798 -0.55771774]
True reward weights: [-0.27534761 -0.19938474  0.94044108]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000042

Running PBIRL with 2 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6144
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.25386352  0.30824413 -0.91680907]
True reward weights: [0.3628525  0.92954011 0.06552289]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.004407

Running PBIRL with 3 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.2746
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.00281775 0.39911572 0.91689623]
True reward weights: [-0.55447543  0.58854202  0.58836663]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.591464

Running experiment 32/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6180
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.34596579  0.29585565 -0.89038031]
True reward weights: [-0.90753571 -0.40234227  0.1204144 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000024

Running PBIRL with 2 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6146
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.89559983  0.13785228 -0.422963  ]
True reward weights: [0.01738318 0.70315869 0.71082043]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.000884

Running PBIRL with 3 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.2792
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.94708779 -0.0573897   0.31580236]
True reward weights: [-0.74394516 -0.6284479   0.22715377]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.575816

Saving results to files...
Results saved successfully.

Running experiment 33/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6210
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.11658205  0.31654324 -0.94138675]
True reward weights: [-0.32556312 -0.30396656  0.89532842]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000028

Running PBIRL with 2 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.2750
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.91005368 -0.12886614  0.39394901]
True reward weights: [-0.37091433  0.28162251 -0.88493577]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.433734

Running PBIRL with 3 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.2800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.79758801 -0.19031201  0.57239383]
True reward weights: [-0.79013    -0.56703889 -0.2327262 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.272695

Running experiment 34/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.6278
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.85400928 -0.16206717  0.4943707 ]
True reward weights: [-0.839382   -0.393236    0.37523766]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000058

Running PBIRL with 2 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.2910
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.63764907 -0.12050911  0.76084244]
True reward weights: [ 0.94667583 -0.25739507 -0.19378506]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.417044

Running PBIRL with 3 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.2956
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.25237058  0.336897    0.90708848]
True reward weights: [0.10194136 0.50508593 0.85702752]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.258905

Saving results to files...
Results saved successfully.

Running experiment 35/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6198
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.07342087 -0.68403294  0.72574673]
True reward weights: [-0.94121751 -0.05112011  0.33391067]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000019

Running PBIRL with 2 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.2824
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-9.01624898e-01  5.54575721e-04  4.32518481e-01]
True reward weights: [-0.67988935 -0.73061395  0.06287861]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.404386

Running PBIRL with 3 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.2790
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.43486815  0.39951831  0.80701599]
True reward weights: [-0.30676195  0.48120125  0.82118357]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.253153

Running experiment 36/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.6254
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.07942671 -0.3167506   0.94517747]
True reward weights: [-0.05456508 -0.040557    0.99768621]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000021

Running PBIRL with 2 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.2750
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.88209747 -0.21980725  0.41663993]
True reward weights: [-0.67772398 -0.72148777  0.14193521]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.401380

Running PBIRL with 3 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.2812
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.92217892 -0.01800458  0.38634424]
True reward weights: [0.32359462 0.42289673 0.84643067]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.254325

Saving results to files...
Results saved successfully.

Running experiment 37/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6276
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.93036912 -0.11536074  0.34800172]
True reward weights: [-0.75478167 -0.33513042  0.563908  ]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000035

Running PBIRL with 2 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.2880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.91690414 -0.36151378  0.16909931]
True reward weights: [-0.37612045 -0.66528317 -0.64492768]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.426819

Running PBIRL with 3 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.2784
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.88077618 -0.15024951  0.44906392]
True reward weights: [-0.41740806  0.02337667  0.90841843]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.264956

Running experiment 38/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6220
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.6098855   0.25298176 -0.7510259 ]
True reward weights: [-0.49248773 -0.22865786  0.83974486]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000025

Running PBIRL with 2 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.2800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.7954411  -0.07250245  0.60167836]
True reward weights: [0.1420408  0.98444139 0.10343863]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.395173

Running PBIRL with 3 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.2842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.7664139  -0.20341042  0.60928969]
True reward weights: [-0.69724132 -0.24621489  0.67322564]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.247493

Saving results to files...
Results saved successfully.

Running experiment 39/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.6200
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.1630125  0.96984064 0.18120724]
True reward weights: [-0.56481614 -0.46100063  0.68444222]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000018

Running PBIRL with 2 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2818
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.79370782 -0.18974182  0.57794977]
True reward weights: [-0.91984574  0.17566894 -0.35074811]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.406588

Running PBIRL with 3 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.91932663 -0.12616551  0.37272082]
True reward weights: [-0.04167042  0.05651217  0.99753193]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.264217

Running experiment 40/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6100
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.11894366 0.97396165 0.19300548]
True reward weights: [-0.6667631  -0.57474284  0.47444455]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000009

Running PBIRL with 2 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.2762
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.30246086  0.27228148  0.91344415]
True reward weights: [-0.38899695  0.90516486  0.17134159]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.393399

Running PBIRL with 3 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.2854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.5028459   0.42635528  0.75190902]
True reward weights: [-0.62498287 -0.50303193  0.59695501]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.246831

Saving results to files...
Results saved successfully.

Running experiment 41/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.6234
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.54005921  0.23575474 -0.80793301]
True reward weights: [-0.46247666 -0.0267168   0.88622884]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000013

Running PBIRL with 2 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.2782
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.01652849 0.52761073 0.84932545]
True reward weights: [-0.34684071  0.19613906 -0.91718645]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.418815

Running PBIRL with 3 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.2834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.35577528  0.41269698  0.83851365]
True reward weights: [-0.71704211 -0.65373672  0.24182414]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.258968

Running experiment 42/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.6180
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.94598411  0.10392855 -0.3071041 ]
True reward weights: [-0.71505276 -0.68463496  0.14133129]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000023

Running PBIRL with 2 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.6250
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.76154552 -0.20249185  0.61566669]
True reward weights: [-0.77048948 -0.04852396  0.63560317]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.002442

Running PBIRL with 3 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.2806
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.7147399  -0.29772034  0.63285818]
True reward weights: [-0.14198028  0.90729985  0.39578856]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.594944

Saving results to files...
Results saved successfully.

Running experiment 43/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.6298
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.17711806 -0.30866289  0.9345354 ]
True reward weights: [-0.94655708 -0.29794542  0.12352418]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000025

Running PBIRL with 2 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.2852
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.64600608 -0.24282519  0.72367953]
True reward weights: [ 0.61059467 -0.61284811 -0.50158882]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.408267

Running PBIRL with 3 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.2898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.30330368  0.1617905   0.93905842]
True reward weights: [-0.38719141 -0.06966796  0.91936347]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.247147

Running experiment 44/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.6258
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.11795174 -0.31493598  0.94175512]
True reward weights: [-0.88315502 -0.32822573  0.33511952]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000032

Running PBIRL with 2 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.2856
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.745512   -0.21125463  0.63212604]
True reward weights: [-0.72216931 -0.51415922 -0.46272214]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.399350

Running PBIRL with 3 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.2884
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.65620633 -0.23852983  0.7158888 ]
True reward weights: [-0.8664473  -0.28569966  0.40944447]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.246992

Saving results to files...
Results saved successfully.

Running experiment 45/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6220
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.87596578  0.15158777 -0.4579357 ]
True reward weights: [-0.83381491 -0.37890594  0.40147601]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000035

Running PBIRL with 2 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.2782
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.49739676  0.40936902  0.76486173]
True reward weights: [-0.19940808  0.22252899 -0.95431508]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.396758

Running PBIRL with 3 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.2856
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.62727256  0.25369407  0.73632089]
True reward weights: [-0.9362114  -0.26228862  0.23390789]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.249144

Running experiment 46/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6168
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.86674506 -0.15735429  0.47327859]
True reward weights: [-0.82098277 -0.18440172  0.54035479]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000026

Running PBIRL with 2 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.2888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.95012682 -0.29460597  0.10230515]
True reward weights: [ 0.36846193 -0.92764803  0.06086827]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.417781

Running PBIRL with 3 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.2794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.29153375  0.20669125  0.93396295]
True reward weights: [-0.85051178  0.3272342   0.41176145]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.269708

Saving results to files...
Results saved successfully.

Running experiment 47/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6214
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.67006911 -0.23263311  0.7049037 ]
True reward weights: [-0.73857451 -0.23755048  0.63093381]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000006

Running PBIRL with 2 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6280
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.20731989  0.31220401 -0.92711764]
True reward weights: [-0.7783307  -0.11524027 -0.61718798]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.000128

Running PBIRL with 3 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.2822
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.88168842 -0.15104437  0.44700238]
True reward weights: [-0.26945278  0.95771157  0.10091455]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.593146

Running experiment 48/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.6244
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.70201434 -0.22268192  0.67645298]
True reward weights: [-0.95541883 -0.18852986  0.22722531]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000022

Running PBIRL with 2 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.6260
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90198557 -0.1361957   0.40972278]
True reward weights: [-0.6287372   0.71659605 -0.30195966]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.001331

Running PBIRL with 3 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.2902
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.64259894 -0.24232955  0.72687206]
True reward weights: [-0.77083167 -0.10682968 -0.62801748]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.583634

Saving results to files...
Results saved successfully.

Running experiment 49/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.6226
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.98361097  0.05623344 -0.17131044]
True reward weights: [-0.98484463 -0.1489005   0.08893646]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000018

Running PBIRL with 2 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.6218
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.94670278 -0.10370437  0.30495778]
True reward weights: [-0.90541387 -0.39749977 -0.1490626 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.001405

Running PBIRL with 3 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.2806
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.15021758  0.45668864  0.87685242]
True reward weights: [-0.17431466  0.95954005  0.22112732]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.573648

Running experiment 50/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.6232
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.84720494 -0.16691706  0.50436345]
True reward weights: [-0.47840873 -0.41855607  0.77196885]
MAP Policy for current environment:
Information gain 1 demonstrations: 1.000011

Running PBIRL with 2 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.2882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.63753113 -0.24550831  0.73026004]
True reward weights: [-0.82194024 -0.23435851 -0.51912457]
MAP Policy for current environment:
Information gain 2 demonstrations: 1.405867

Running PBIRL with 3 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.2776
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.95266344 -0.29526774  0.07245221]
True reward weights: [0.16766395 0.56514023 0.80777801]
MAP Policy for current environment:
Information gain 3 demonstrations: 1.257179

Saving results to files...
Results saved successfully.
