Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [([(0, 0), (0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 0), (0, 3), (1, 3), (2, 3), (2, 0), (2, 1), (5, None)]), ([(0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 0), (1, 2), (0, 2), (3, 3), (4, 2), (4, 3), (5, None)]), ([(0, 3), (1, 1), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 3), (2, 2), (1, 0), (1, 0), (1, 1), (4, 3), (5, None)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (1, 1), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 3), (1, 2), (0, 2), (0, 1), (1, 3), (2, 0), (2, 0), (2, 0), (2, 0)]), ([(0, 1), (3, 1), (3, 2), (3, 3), (0, 1), (3, 2), (3, 3), (3, 3), (4, 3), (1, 1)], [(0, 1), (3, 1), (3, 2), (3, 3), (0, 1), (3, 2), (3, 2), (3, 1), (3, 0), (0, 0)]), ([(0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (4, 3), (5, 0), (5, 0)], [(0, 2), (0, 0), (0, 2), (0, 1), (3, 3), (3, 1), (3, 3), (4, 2), (3, 3), (4, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6594
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.5534159   0.10676339  0.82603415]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6278
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.10169218 -0.44942703  0.88751002]
True reward weights: [-0.08523963 -0.77072789  0.63143703]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.129598

Running PBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5072
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.83736402 -0.44604343  0.31601701]
True reward weights: [-0.53202303  0.07965792  0.84297456]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155593

Running PBIRL with 4 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.5184
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.96832309 -0.23469614 -0.08525321]
True reward weights: [ 0.28039892 -0.0018456   0.95988178]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.146548

Running PBIRL with 5 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.2398
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.28363508 -0.86102233  0.42212758]
True reward weights: [-0.43486428 -0.50213493  0.7474982 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.226285

Running PBIRL with 6 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.2326
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.48478207 0.14034472 0.86330163]
True reward weights: [-0.48639338 -0.75787514  0.43479506]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.202559

Running experiment 2/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (4, 1), (4, 0), (1, 0), (1, 1), (4, 3), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 3), (4, 1), (4, 0), (1, 0), (1, 1), (4, 2), (3, 1), (3, 3)]), ([(0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 2), (0, 1), (3, 0), (0, 2), (0, 0), (0, 2), (0, 1), (3, 2), (3, 3)]), ([(0, 3), (1, 3), (2, 3), (2, 2), (1, 1), (4, 2), (3, 0), (0, 0), (0, 1), (3, 3)], [(0, 3), (1, 3), (2, 3), (2, 2), (1, 1), (4, 2), (3, 0), (0, 0), (0, 2), (3, 2)]), ([(0, 0), (0, 0), (0, 2), (0, 2), (0, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 0), (0, 0), (0, 2), (0, 2), (0, 2), (0, 1), (3, 2), (3, 1), (3, 0), (0, 1)]), ([(0, 3), (3, 1), (3, 3), (4, 1), (4, 2), (3, 2), (3, 0), (0, 1), (3, 3), (4, 3)], [(0, 3), (3, 1), (3, 3), (4, 1), (4, 2), (3, 2), (3, 0), (0, 2), (3, 2), (3, 0)]), ([(0, 3), (1, 1), (2, 1), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 1), (2, 3), (2, 0), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6548
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.01970094 -0.79231517  0.60979385]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6494
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.42928134 -0.08412191  0.8992447 ]
True reward weights: [ 0.09415964 -0.33514969  0.93744794]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123602

Running PBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6608
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.00114107 -0.87600218 -0.4823058 ]
True reward weights: [ 0.17291045 -0.95604018 -0.23683148]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.147801

Running PBIRL with 4 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6632
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.02878295 -0.92754452 -0.37260261]
True reward weights: [-0.34917569 -0.88265715 -0.31463105]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.147811

Running PBIRL with 5 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6574
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.76881211 -0.54959286 -0.32691838]
True reward weights: [0.54309286 0.12764147 0.82991434]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.202675

Running PBIRL with 6 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6592
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.20191555 -0.79702722  0.56919041]
True reward weights: [-0.79012337 -0.17792941  0.5865545 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.202729

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Shuffled Demos: [([(0, 3), (1, 3), (2, 2), (1, 1), (4, 2), (1, 2), (0, 1), (1, 3), (2, 1), (5, 0)], [(0, 3), (1, 3), (2, 2), (1, 1), (4, 2), (1, 2), (0, 1), (1, 3), (2, 3), (2, 2)]), ([(0, 2), (0, 3), (1, 2), (0, 1), (3, 3), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 3), (1, 2), (0, 1), (3, 1), (3, 2), (3, 1), (3, 3), (3, 0), (0, 1)]), ([(0, 1), (3, 0), (0, 1), (1, 1), (2, 3), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 0), (0, 1), (1, 1), (2, 3), (2, 2), (1, 3), (2, 3), (2, 2), (1, 1)]), ([(0, 0), (0, 1), (3, 3), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 0), (0, 2), (0, 2), (0, 1), (3, 2), (3, 3), (4, 2), (3, 0), (0, 0)]), ([(0, 1), (0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (0, 3), (1, 1), (4, 2), (3, 2), (3, 1), (3, 2), (3, 3), (4, 2), (3, 1)]), ([(0, 0), (0, 1), (0, 3), (1, 1), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (0, 3), (1, 2), (0, 2), (0, 0), (0, 1), (3, 2), (3, 0), (0, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6650
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.97200066 -0.1021731   0.21160193]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6520
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.74687153  0.17697758  0.64098506]
True reward weights: [-0.12567307  0.66156181  0.73928496]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124229

Running PBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5922
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.36917596 -0.33237151  0.86789302]
True reward weights: [0.18260679 0.01765619 0.98302748]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.137482

Running PBIRL with 4 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6062
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.13212772 -0.92236926 -0.36301131]
True reward weights: [-0.43441601 -0.89804661 -0.06924606]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.133868

Running PBIRL with 5 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.5958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.17979915 -0.5459258   0.81831369]
True reward weights: [ 0.12708069 -0.5575352   0.82036882]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.131780

Running PBIRL with 6 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6054
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.10673097 -0.23435842  0.96627358]
True reward weights: [ 0.19611019 -0.71333276  0.67282774]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.130440

Running experiment 4/50...
Shuffled Demos: [([(0, 1), (1, 1), (4, 2), (3, 2), (3, 1), (3, 0), (3, 0), (0, 1), (1, 1), (4, 3)], [(0, 1), (1, 1), (4, 2), (3, 2), (3, 1), (3, 0), (3, 0), (0, 0), (0, 1), (3, 3)]), ([(0, 2), (0, 1), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 0), (0, 0), (0, 3), (1, 2), (0, 0), (0, 1), (3, 2), (3, 0), (0, 3)]), ([(0, 1), (3, 2), (3, 0), (0, 1), (1, 1), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 2), (3, 0), (0, 1), (1, 2), (0, 1), (3, 3), (3, 2), (3, 1), (3, 1)]), ([(0, 1), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 0), (0, 0), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 3), (3, 2), (3, 1), (3, 0), (4, 1), (4, 0), (1, 1), (4, 3), (5, 0), (5, 0)], [(0, 3), (3, 2), (3, 1), (3, 0), (4, 1), (4, 0), (1, 1), (4, 0), (1, 2), (0, 3)]), ([(0, 2), (0, 1), (3, 2), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 2), (3, 3), (4, 2), (3, 2), (0, 2), (0, 0), (0, 0), (0, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6496
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.873856   -0.45847783  0.1617831 ]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123673

Running PBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4930
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.47085028 -0.17282092  0.86512019]
True reward weights: [ 0.44998025 -0.22008603  0.86549403]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.152120

Running PBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5106
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.76306335 -0.62757361 -0.15454994]
True reward weights: [-0.04943528 -0.81405728 -0.57867685]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.142169

Running PBIRL with 4 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.5072
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.46730415 -0.80607291 -0.36314364]
True reward weights: [-0.03248832 -0.43929402  0.89775569]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.137136

Running PBIRL with 5 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4348
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.50248823 -0.35317152  0.78916124]
True reward weights: [ 0.38399308 -0.74515419  0.54524723]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.151890

Running PBIRL with 6 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4398
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.08546972 -0.97089281 -0.22374558]
True reward weights: [ 0.10586694 -0.19114616  0.97583571]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.146351

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Shuffled Demos: [([(0, 2), (0, 0), (1, 1), (4, 1), (4, 2), (1, 1), (4, 1), (4, 0), (1, 1), (4, 3)], [(0, 2), (0, 0), (1, 1), (4, 1), (4, 2), (1, 1), (4, 1), (4, 0), (1, 2), (0, 2)]), ([(0, 3), (1, 3), (2, 3), (2, 2), (1, 0), (1, 0), (1, 1), (4, 3), (1, 1), (0, 1)], [(0, 3), (1, 3), (2, 3), (2, 2), (1, 0), (1, 0), (1, 2), (0, 3), (3, 3), (4, 0)]), ([(0, 1), (3, 1), (4, 1), (4, 1), (3, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 1), (4, 1), (4, 1), (3, 1), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 3), (4, 1), (4, 1), (4, 2), (3, 1), (3, 0), (0, 1), (3, 0), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 0), (0, 3), (1, 0), (0, 3), (1, 1), (4, 1), (3, 2), (3, 2)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 0), (1, 1), (4, 0), (1, 3), (4, 0), (1, 0), (1, 0), (1, 0), (1, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.07057998  0.96609012  0.24837137]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6656
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.05889864 -0.94994148  0.3068259 ]
True reward weights: [ 0.15545445 -0.93972603 -0.30454704]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183853

Running PBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5324
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.50412794 -0.85849421  0.09403572]
True reward weights: [ 0.35663424 -0.74311053 -0.56621441]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.198656

Running PBIRL with 4 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5262
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.31861513 -0.88906522  0.32870571]
True reward weights: [-0.67107257 -0.70604137  0.22620167]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.184032

Running PBIRL with 5 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5026
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.04494853 -0.9399414  -0.33836342]
True reward weights: [-0.33975741 -0.87041645 -0.35628655]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.184741

Running PBIRL with 6 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.25850858 -0.96594964  0.01070528]
True reward weights: [ 0.55877093 -0.03731799  0.82848199]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.246012

Running experiment 6/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 0), (0, 2), (0, 0), (0, 2), (3, 1), (4, 2), (3, 0), (0, 1), (3, 1)]), ([(0, 0), (0, 1), (0, 2), (0, 1), (1, 1), (2, 1), (5, 0)], [(0, 0), (0, 1), (0, 2), (0, 1), (1, 3), (4, 3), (5, None)]), ([(0, 0), (0, 3), (1, 1), (4, 0), (1, 0), (1, 0), (1, 0), (1, 1), (2, 1), (5, 0)], [(0, 0), (0, 3), (1, 1), (4, 0), (1, 0), (1, 0), (1, 0), (1, 0), (0, 2), (0, 1)]), ([(0, 0), (0, 0), (1, 3), (2, 1), (1, 1)], [(0, 0), (0, 0), (1, 3), (2, 2), (5, None)]), ([(0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 0), (0, 0), (0, 3), (1, 0), (1, 0), (0, 2), (0, 3), (1, 0), (1, 2)]), ([(0, 3), (3, 1), (3, 2), (3, 3), (4, 3), (1, 2), (1, 1), (2, 1), (5, 0), (5, 0)], [(0, 3), (3, 1), (3, 2), (3, 3), (4, 3), (1, 2), (1, 2), (0, 1), (3, 2), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6486
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.91331896 -0.40012206 -0.07583407]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6642
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.09896806 -0.97104375 -0.21743817]
True reward weights: [-0.97551325  0.11514156  0.18739349]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147855

Running PBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6684
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.23108559 -0.96964283 -0.07995148]
True reward weights: [-0.94979424 -0.27710227 -0.14527639]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148104

Running PBIRL with 4 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.3274
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.95425904 -0.11842735  0.27452621]
True reward weights: [0.74820709 0.38176184 0.54262699]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.231936

Running PBIRL with 5 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.0718
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.50446404 -0.81450359  0.28653084]
True reward weights: [ 0.87509211 -0.41183613  0.25417473]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.309666

Running PBIRL with 6 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.0748
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.48816665 -0.82854444  0.27423972]
True reward weights: [ 0.3181578  -0.9405309   0.11906823]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.264890

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 1), (4, 2), (3, 0), (0, 0), (0, 1), (1, 0), (1, 1), (4, 3), (4, 0)]), ([(0, 1), (3, 0), (0, 3), (1, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 1), (3, 0), (0, 3), (1, 2), (0, 0), (0, 1), (3, 2), (3, 1), (3, 3), (4, 3)]), ([(0, 1), (3, 3), (4, 2), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 1), (3, 3), (4, 2), (3, 3), (4, 1), (4, 3), (5, None)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 3), (1, 3), (2, 3), (2, 0), (2, 1), (5, None)]), ([(0, 0), (1, 0), (2, 1), (5, 0)], [(0, 0), (1, 0), (2, 2), (5, None)]), ([(0, 0), (0, 2), (0, 0), (0, 0), (0, 0), (0, 0), (0, 1), (3, 3), (0, 1), (3, 3)], [(0, 0), (0, 2), (0, 0), (0, 0), (0, 0), (0, 0), (0, 2), (0, 3), (1, 0), (1, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6604
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.11399166 -0.71614595  0.68857888]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6136
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.75125201  0.12125018  0.64878256]
True reward weights: [-0.98906429 -0.14337938  0.03455698]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.132883

Running PBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6228
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.61751284  0.28678304  0.73241613]
True reward weights: [-0.68818232  0.00255396  0.7255333 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.129974

Running PBIRL with 4 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.5996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.09144408 -0.89557406  0.43541368]
True reward weights: [-0.74614959  0.11409649  0.65592894]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.131477

Running PBIRL with 5 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6054
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.47605973 -0.03783236  0.87859879]
True reward weights: [ 0.33818313 -0.28250917  0.89767518]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.153729

Running PBIRL with 6 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3320
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.41608687  0.45310548  0.78839529]
True reward weights: [ 0.7387403  -0.63429576  0.22788519]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.239574

Running experiment 8/50...
Shuffled Demos: [([(0, 1), (1, 1), (2, 1), (5, 0)], [(0, 3), (1, 2), (4, 3), (5, None)]), ([(0, 2), (3, 1), (3, 0), (0, 1), (3, 0), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 2), (3, 1), (3, 0), (0, 1), (3, 0), (0, 1), (3, 3), (4, 0), (5, None)]), ([(0, 3), (1, 1), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 0), (1, 0), (1, 2), (0, 3), (1, 1), (4, 3), (4, 0), (3, 2), (3, 0)]), ([(0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 2), (0, 3), (3, 0), (0, 2), (0, 3), (1, 3), (2, 0), (2, 3), (2, 2)]), ([(0, 1), (1, 1), (4, 3), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 3), (4, 0), (5, None)]), ([(0, 3), (0, 2), (0, 0), (0, 0), (0, 3), (1, 1), (2, 1), (5, 0), (5, 0), (5, 0)], [(0, 3), (0, 2), (0, 0), (0, 0), (0, 3), (1, 3), (2, 3), (2, 0), (2, 0), (2, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.8661913  -0.04290549  0.4978672 ]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.2874669   0.7565044  -0.58742137]
True reward weights: [ 0.21663676  0.44704551 -0.86788181]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running PBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6490
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.76452576  0.62900358  0.14090728]
True reward weights: [ 0.25791206  0.46783965 -0.84534456]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.254056

Running PBIRL with 4 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.5900
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.75407853 -0.65333087 -0.06726322]
True reward weights: [-0.49608136  0.33933303  0.79922236]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.252492

Running PBIRL with 5 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.5158
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.70635124 -0.68230016 -0.18850576]
True reward weights: [-0.38975527 -0.77020004  0.50485912]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.264518

Running PBIRL with 6 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.5074
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.41324361 -0.78127025  0.46781033]
True reward weights: [ 0.22610365 -0.96521976  0.13125526]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.251834

Saving results to files...
Results saved successfully.

Running experiment 9/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 0), (1, 3), (2, 2), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 3), (3, 3), (3, 3), (4, 3), (5, None)]), ([(0, 2), (0, 0), (1, 3), (2, 2), (1, 0), (0, 3), (1, 1), (4, 3), (5, 0), (5, 0)], [(0, 2), (0, 0), (1, 3), (2, 2), (1, 0), (0, 3), (1, 3), (2, 2), (1, 1), (4, 1)]), ([(0, 3), (0, 1), (3, 3), (4, 3), (1, 1)], [(0, 3), (0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 0), (0, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 2), (0, 0), (0, 3), (1, 3), (2, 0), (2, 3), (5, None)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (4, 3), (5, 0)], [(0, 3), (1, 3), (2, 2), (1, 3), (2, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6538
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.45673541  0.63005556  0.62803086]
True reward weights: [-0.0505111  -0.01531811  0.99860602]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5552
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.14350347 -0.70276843  0.69679501]
True reward weights: [-0.62431317  0.57864084  0.52479315]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.139469

Running PBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.5682
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.01662375 -0.96355738  0.26698471]
True reward weights: [-0.29908997  0.06276383  0.95215854]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.134141

Running PBIRL with 4 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.0206
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.4367781   0.63601274  0.63617032]
True reward weights: [-0.98254735  0.0286602   0.18379143]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.409495

Running PBIRL with 5 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.0164
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.22086352  0.68970215  0.68958701]
True reward weights: [-0.9748804  -0.15800174 -0.15698296]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.349131

Running PBIRL with 6 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.0184
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.40598814  0.64615167  0.64626747]
True reward weights: [-0.86232886  0.34642463  0.36929516]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.320567

Running experiment 10/50...
Shuffled Demos: [([(0, 0), (0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 0), (0, 2), (3, 0), (4, 1), (4, 3), (5, None)]), ([(0, 1), (3, 1), (4, 3), (5, 0), (5, 0)], [(0, 1), (3, 1), (4, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 1), (0, 2), (0, 2), (0, 0), (0, 2), (0, 1), (3, 1), (3, 1), (3, 3)]), ([(0, 0), (0, 1), (0, 1), (3, 3), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 0), (0, 3), (1, 2), (0, 0), (0, 3), (1, 3), (2, 2), (1, 3), (4, 1)]), ([(0, 2), (0, 2), (0, 3), (1, 1), (4, 0), (1, 2), (4, 0), (1, 0), (1, 1), (4, 3)], [(0, 2), (0, 2), (0, 3), (1, 1), (4, 0), (1, 2), (4, 0), (1, 0), (1, 2), (0, 1)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (1, 3), (4, 3), (1, 0), (1, 1), (4, 2), (1, 0), (1, 0), (1, 1), (0, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6528
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.48246275 -0.52639923  0.70009538]
True reward weights: [-0.76518493 -0.64328377  0.02603879]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6564
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.54383493 -0.42069142  0.72612829]
True reward weights: [ 0.94231039 -0.31667137 -0.10849138]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123897

Running PBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6386
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.2421843  -0.77194357  0.58774986]
True reward weights: [0.52479064 0.5614247  0.63984146]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127822

Running PBIRL with 4 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5590
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.60254403 -0.70906995  0.36627925]
True reward weights: [0.27280917 0.26597887 0.92457038]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.148459

Running PBIRL with 5 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5672
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.60894018 -0.5277915   0.59213848]
True reward weights: [ 0.71400658 -0.58033115  0.39167634]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.167136

Running PBIRL with 6 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.5060
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.39854963 -0.90457443  0.15133838]
True reward weights: [-0.22570725 -0.69520893  0.68245204]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.182066

Saving results to files...
Results saved successfully.

Running experiment 11/50...
Shuffled Demos: [([(0, 2), (0, 0), (0, 1), (1, 2), (1, 3), (2, 1), (5, 0), (5, 0)], [(0, 2), (0, 0), (0, 1), (1, 2), (1, 3), (2, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 3), (2, 3), (2, 1), (5, None)]), ([(0, 3), (0, 1), (3, 0), (0, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (0, 1), (3, 0), (0, 2), (0, 3), (1, 1), (4, 2), (3, 3), (4, 1), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 1), (3, 0), (0, 1), (3, 1), (3, 3), (4, 2), (3, 0), (3, 1)]), ([(0, 1), (3, 2), (3, 2), (3, 1), (3, 0), (0, 0), (0, 3), (1, 2), (0, 1), (3, 3)], [(0, 1), (3, 2), (3, 2), (3, 1), (3, 0), (0, 0), (0, 3), (1, 2), (0, 0), (1, 3)]), ([(0, 0), (0, 1), (3, 0), (0, 3), (1, 0), (1, 3), (2, 1), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 0), (0, 3), (1, 0), (1, 3), (2, 3), (2, 0), (2, 3), (2, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6580
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.21573421 -0.70986021  0.6704903 ]
True reward weights: [-0.87829137 -0.24439244  0.41094598]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123672

Running PBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6036
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.3929544  -0.19490842  0.89866432]
True reward weights: [-0.73729622 -0.1498619   0.65873796]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.134346

Running PBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5944
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.13395332 -0.83466676  0.5342171 ]
True reward weights: [-0.94593961 -0.29672562  0.13096623]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.130832

Running PBIRL with 4 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6026
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.43545272 -0.89687032  0.07748906]
True reward weights: [ 0.08472964 -0.23895919  0.9673259 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.129457

Running PBIRL with 5 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3394
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.72946934  0.23971322  0.6406341 ]
True reward weights: [0.71858408 0.26419179 0.64330368]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.209771

Running PBIRL with 6 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3282
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.3017362   0.3754377   0.87635712]
True reward weights: [0.00358884 0.3292046  0.94425179]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.191201

Running experiment 12/50...
Shuffled Demos: [([(0, 3), (1, 1), (4, 2), (3, 3), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 1), (4, 2), (3, 0), (0, 0), (0, 1), (3, 2), (3, 2), (3, 3), (0, 2)]), ([(0, 1), (3, 1), (3, 3), (4, 1), (3, 1), (3, 0), (0, 0), (0, 2), (0, 1), (1, 1)], [(0, 1), (3, 1), (3, 3), (4, 1), (3, 1), (3, 0), (0, 0), (0, 2), (0, 0), (0, 2)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 0), (0, 1), (3, 0), (3, 1), (3, 3), (4, 1), (5, None)]), ([(0, 1), (0, 1), (3, 1), (3, 3), (4, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (0, 1), (3, 1), (3, 3), (4, 3), (4, 0), (1, 3), (2, 0), (2, 1), (1, 0)]), ([(0, 3), (1, 1), (4, 3), (5, 0), (5, 0)], [(0, 3), (1, 1), (4, 1), (4, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 2), (3, 2), (0, 2), (0, 3), (1, 3), (2, 3), (2, 3), (2, 0), (2, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6472
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.81847091 -0.49366199 -0.29394422]
True reward weights: [-0.8497926  -0.50850291  0.13884283]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5336
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.44620833 -0.04893493  0.89359023]
True reward weights: [ 0.97362943 -0.07037148  0.21701056]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.145952

Running PBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.5268
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.84111394 -0.30581956  0.44609611]
True reward weights: [ 0.19474609 -0.8258476  -0.52919722]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.137729

Running PBIRL with 4 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4464
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.50002313 -0.8659934  -0.00568366]
True reward weights: [ 0.3335249  -0.90269895  0.27183772]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.158913

Running PBIRL with 5 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4440
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.52643963 -0.78700852  0.32168138]
True reward weights: [-0.59551936 -0.79849488  0.08810573]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.151420

Running PBIRL with 6 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4440
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.14402523 -0.89171518 -0.42906965]
True reward weights: [-0.48601528 -0.80893784  0.33076988]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.146250

Saving results to files...
Results saved successfully.

Running experiment 13/50...
Shuffled Demos: [([(0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 0), (0, 3), (1, 0), (1, 1), (2, 1), (2, 1), (5, None)]), ([(0, 0), (0, 0), (0, 3), (1, 2), (0, 1), (3, 3), (3, 3), (4, 3), (1, 1), (4, 3)], [(0, 0), (0, 0), (0, 3), (1, 2), (0, 2), (0, 1), (3, 3), (3, 1), (3, 2), (3, 3)]), ([(0, 0), (0, 1), (3, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 1), (4, 1), (4, 2), (3, 1), (3, 2), (3, 2), (3, 3), (4, 0)]), ([(0, 1), (3, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 0), (0, 0), (0, 3), (1, 2), (0, 1), (1, 3), (2, 0), (1, 1), (4, 2)]), ([(0, 0), (0, 1), (3, 2), (3, 2), (3, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 2), (3, 2), (3, 1), (3, 1), (3, 0), (0, 0), (0, 3), (1, 3)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 0), (0, 0), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6690
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.40128511 -0.84081117  0.36332746]
True reward weights: [-0.34477193 -0.32328333  0.88126058]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3188
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.0068586  -0.74583467  0.66609579]
True reward weights: [-0.62976795  0.77513906 -0.05051501]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173816

Running PBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3392
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.13874719 -0.9850675   0.10193743]
True reward weights: [-0.45403314 -0.86971274  0.1935295 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154447

Running PBIRL with 4 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3308
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.28985063 -0.77300528  0.56431325]
True reward weights: [-0.53873854 -0.73955682 -0.40350526]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.145901

Running PBIRL with 5 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3350
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.21698425 -0.79474325  0.56683419]
True reward weights: [-0.29884233 -0.94017002  0.16362639]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.141606

Running PBIRL with 6 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.3340
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.00330555 -0.93382448  0.35771625]
True reward weights: [0.59229788 0.51858    0.61665064]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.138369

Running experiment 14/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 1), (3, 1), (3, 0), (0, 1), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 2), (0, 1), (3, 1), (3, 1), (3, 0), (0, 3), (1, 3), (2, 2), (1, 3), (2, 2)]), ([(0, 0), (0, 2), (0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 2), (0, 0), (0, 2), (0, 1), (3, 0), (0, 2), (0, 1), (3, 0)]), ([(0, 2), (0, 3), (1, 1), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 3), (1, 1), (0, 0), (0, 1), (3, 1), (3, 2), (3, 2), (3, 2), (3, 0)]), ([(0, 1), (3, 2), (3, 3), (4, 2), (3, 3), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 2), (3, 3), (4, 2), (3, 3), (3, 2), (3, 1), (3, 1), (3, 3), (4, 2)]), ([(0, 0), (0, 0), (0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 0), (0, 0), (0, 1), (3, 2), (3, 3), (4, 1), (4, 0), (5, None)]), ([(0, 3), (0, 3), (1, 0), (1, 1), (4, 3), (5, 0), (5, 0)], [(0, 3), (0, 3), (1, 0), (1, 1), (4, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.6520
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.83423765 -0.14736636  0.53134801]
True reward weights: [-0.43088031 -0.17122291  0.88601629]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4480
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.15934949 0.54587017 0.82257735]
True reward weights: [-0.0974542  -0.59807778  0.79549082]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.155781

Running PBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4442
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.01552017  0.2928881   0.95602076]
True reward weights: [0.30721395 0.45037084 0.83832315]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.144606

Running PBIRL with 4 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4380
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.33540294 0.30403476 0.89166571]
True reward weights: [-0.21780065  0.12594139  0.96783348]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.138768

Running PBIRL with 5 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4472
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.59419682 -0.80394372  0.0245892 ]
True reward weights: [-0.65766957 -0.60187671  0.4530068 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.160327

Running PBIRL with 6 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4418
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.54100139 -0.82954622  0.13845781]
True reward weights: [-0.64492359  0.30885098  0.69905983]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.158254

Saving results to files...
Results saved successfully.

Running experiment 15/50...
Shuffled Demos: [([(0, 1), (0, 3), (3, 3), (4, 1), (4, 2), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 1), (0, 3), (3, 3), (4, 1), (4, 2), (3, 2), (3, 0), (3, 2), (3, 3), (4, 3)]), ([(0, 1), (3, 0), (4, 2), (3, 0), (0, 0), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 1), (3, 0), (4, 2), (3, 0), (0, 0), (0, 0), (0, 0), (1, 0), (1, 2), (0, 1)]), ([(0, 3), (1, 1), (0, 2), (0, 1), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 1), (0, 2), (0, 0), (0, 2), (0, 3), (1, 0), (1, 2), (0, 1), (3, 3)]), ([(0, 3), (1, 0), (0, 2), (0, 2), (0, 2), (0, 1), (1, 1), (2, 1), (5, 0), (5, 0)], [(0, 3), (1, 0), (0, 2), (0, 2), (0, 2), (0, 3), (1, 1), (2, 3), (2, 1), (5, None)]), ([(0, 0), (0, 2), (0, 1), (3, 2), (3, 0), (0, 1), (3, 3), (4, 3), (1, 1), (4, 3)], [(0, 0), (0, 2), (0, 1), (3, 2), (3, 0), (0, 0), (0, 3), (1, 0), (1, 2), (0, 1)]), ([(0, 3), (1, 1), (4, 0), (1, 1), (4, 2), (3, 1), (3, 3), (4, 0), (1, 1), (4, 3)], [(0, 3), (1, 1), (4, 0), (1, 1), (4, 2), (3, 1), (3, 3), (4, 0), (1, 3), (2, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.6500
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.78548497 -0.11961172  0.60721199]
True reward weights: [-0.75433776 -0.02805593  0.65588674]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.4448
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.10062983 -0.05986528  0.99312123]
True reward weights: [ 0.6582255  -0.02368298  0.75244821]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.159846

Running PBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.4358
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.81820385 -0.54170417  0.19261114]
True reward weights: [-0.90008216 -0.22750229  0.37161109]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.146294

Running PBIRL with 4 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.4374
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.81005902 -0.38920446  0.43854792]
True reward weights: [-0.57129763 -0.68595573  0.45064815]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.140699

Running PBIRL with 5 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3316
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.58929156  0.20565159  0.78130844]
True reward weights: [0.10363201 0.30103585 0.9479651 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.173444

Running PBIRL with 6 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3372
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.86553217 -0.49903127 -0.04268318]
True reward weights: [-0.89570528 -0.18597914  0.4038859 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.188840

Running experiment 16/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 2), (4, 3), (5, None)]), ([(0, 1), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 3), (2, 3), (2, 3), (2, 3), (5, None)]), ([(0, 0), (0, 3), (1, 3), (2, 0), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 3), (2, 0), (2, 2), (1, 2), (0, 0), (0, 2), (3, 2), (3, 0)]), ([(0, 2), (0, 1), (3, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 1), (3, 3), (4, 0), (3, 1), (3, 3), (4, 0), (1, 2), (0, 2)]), ([(0, 3), (1, 3), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 3), (2, 2), (1, 2), (0, 1), (3, 2), (3, 3), (4, 0), (1, 1), (4, 3)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 0), (0, 2), (0, 0), (0, 3), (0, 2), (0, 3), (1, 3), (2, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.6528
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.07242825 -0.883552    0.46269861]
True reward weights: [-0.92032338 -0.11683579  0.37330186]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5326
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.21292732 -0.11880817  0.96981781]
True reward weights: [-0.80654422 -0.51092815  0.2973867 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147898

Running PBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5186
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.04988779 0.01848132 0.99858382]
True reward weights: [0.60753732 0.33697857 0.71926619]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.138909

Running PBIRL with 4 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5310
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.91349243  0.17262278  0.36841954]
True reward weights: [-0.61495281 -0.71733011 -0.32752184]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.135049

Running PBIRL with 5 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5290
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.65588772 -0.56156591  0.50443535]
True reward weights: [-0.90654466 -0.20718993  0.36776232]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.132643

Running PBIRL with 6 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.5356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.17460359  0.13392549  0.97548836]
True reward weights: [0.4627254  0.57542724 0.6743654 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.131558

Saving results to files...
Results saved successfully.

Running experiment 17/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 0), (0, 1), (3, 0), (0, 2), (0, 0), (1, 2), (0, 2), (0, 1), (3, 3)]), ([(0, 3), (1, 0), (1, 2), (0, 1), (0, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 3), (1, 0), (1, 2), (0, 1), (0, 2), (0, 3), (1, 1), (2, 0), (2, 0), (2, 0)]), ([(0, 3), (1, 0), (1, 2), (0, 0), (0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 3), (1, 0), (1, 2), (0, 0), (0, 3), (1, 0), (1, 3), (2, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 2), (0, 2), (0, 1), (1, 3), (2, 2), (1, 1), (4, 2), (3, 0), (4, 1)]), ([(0, 2), (0, 2), (3, 0), (3, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 2), (3, 0), (3, 1), (3, 0), (0, 2), (0, 1), (3, 3), (4, 2), (3, 2)]), ([(0, 0), (0, 3), (1, 0), (1, 0), (1, 0), (1, 3), (2, 1), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 0), (1, 0), (1, 0), (1, 3), (2, 0), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6670
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.26966025 -0.59104451  0.76023005]
True reward weights: [-0.76208548 -0.28664819  0.58056743]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6044
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.33778052 -0.51936639  0.78496043]
True reward weights: [-0.13566255  0.65828114  0.7404469 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.131375

Running PBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.3574
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.19167425  0.04493086  0.9804296 ]
True reward weights: [-0.20872972 -0.90548441  0.36949951]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.186110

Running PBIRL with 4 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.3638
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.986526   -0.15038921  0.0644169 ]
True reward weights: [-0.58824213 -0.23241583  0.77456703]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.167131

Running PBIRL with 5 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.3250
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.45156003  0.49907051  0.73960947]
True reward weights: [-0.37327291  0.15393983  0.91486057]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.168760

Running PBIRL with 6 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.3286
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.20576557  0.37837819  0.90249126]
True reward weights: [-0.448048   -0.14373476  0.88237934]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.160806

Running experiment 18/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 1), (3, 0), (0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 0), (0, 1), (3, 0), (0, 3), (1, 2), (0, 3), (1, 3), (2, 0), (2, 1)]), ([(0, 2), (0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 2), (0, 0), (0, 2), (0, 1), (3, 0), (3, 2), (3, 2), (3, 3), (4, 1)]), ([(0, 3), (1, 1), (4, 2), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 3), (1, 1), (4, 2), (3, 0), (0, 3), (1, 1), (4, 3), (4, 3), (5, None)]), ([(0, 1), (1, 1), (4, 3), (5, 0), (5, 0)], [(0, 1), (1, 1), (4, 1), (4, 3), (5, None)]), ([(0, 3), (1, 3), (2, 2), (1, 0), (1, 1), (4, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 3), (2, 2), (1, 0), (1, 0), (1, 0), (2, 2), (1, 3), (2, 1), (2, 3)]), ([(0, 1), (3, 3), (0, 1), (1, 1)], [(0, 3), (3, 3), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6668
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.96946495 -0.15994674 -0.18588911]
True reward weights: [-0.39174849 -0.30341429  0.86860399]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6330
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.4729481  -0.54052781  0.69580873]
True reward weights: [ 0.36528429 -0.90272044 -0.2272954 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.129886

Running PBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.5294
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.40253688 -0.21070168  0.89082482]
True reward weights: [ 0.39416788 -0.41423468  0.82039095]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.149528

Running PBIRL with 4 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.5344
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.38236831 -0.80190175  0.45907304]
True reward weights: [0.2788215  0.54065671 0.7936932 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.142698

Running PBIRL with 5 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.5296
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.3448881  -0.75250919  0.56105447]
True reward weights: [ 0.56272559 -0.44652076  0.69567171]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.139225

Running PBIRL with 6 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.0204
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.65832585 0.36470308 0.65848215]
True reward weights: [-0.25591636 -0.66095681  0.7054381 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.456017

Saving results to files...
Results saved successfully.

Running experiment 19/50...
Shuffled Demos: [([(0, 1), (3, 1), (4, 0), (1, 1), (4, 3), (4, 3), (1, 1), (0, 1), (3, 3), (4, 3)], [(0, 1), (3, 1), (4, 0), (1, 3), (2, 2), (1, 2), (4, 2), (3, 0), (0, 3), (1, 3)]), ([(0, 0), (0, 1), (3, 0), (0, 1), (0, 0), (0, 2), (0, 0), (1, 1), (4, 3), (5, 0)], [(0, 0), (0, 1), (3, 0), (0, 1), (0, 0), (0, 2), (0, 0), (1, 0), (1, 1), (4, 3)]), ([(0, 2), (0, 1), (3, 1), (3, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 1), (3, 2), (0, 2), (0, 1), (1, 1), (2, 0), (2, 0), (2, 1)]), ([(0, 1), (3, 3), (4, 3), (4, 0), (1, 3), (1, 1), (4, 3), (1, 1)], [(0, 1), (3, 3), (4, 3), (4, 0), (1, 3), (1, 3), (2, 1), (5, None)]), ([(0, 2), (0, 2), (0, 1), (3, 1), (3, 2), (3, 3), (4, 3), (4, 3), (5, 0), (5, 0)], [(0, 2), (0, 2), (0, 1), (3, 1), (3, 2), (3, 0), (0, 0), (0, 0), (0, 0), (0, 3)]), ([(0, 3), (1, 3), (2, 0), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 3), (2, 0), (2, 2), (1, 2), (4, 2), (3, 3), (4, 0), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6700
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.61294415 -0.27669819  0.74009295]
True reward weights: [-0.92648321 -0.16748955  0.33701054]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5284
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.12827653  0.6547496   0.74488126]
True reward weights: [0.1021149  0.94225677 0.31894315]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.144889

Running PBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.3952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.6205968  -0.24147275  0.74602314]
True reward weights: [-0.88074554 -0.12252048 -0.45746696]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.168390

Running PBIRL with 4 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.0030
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.59494264 -0.57715547 -0.55940577]
True reward weights: [-0.14531834  0.20224439  0.96849357]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.674495

Running PBIRL with 5 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.0016
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.5673258  0.57679967 0.58774448]
True reward weights: [-0.56394148 -0.60377705 -0.56340329]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.568036

Running PBIRL with 6 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.0024
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.59086778 -0.58202595 -0.55867796]
True reward weights: [0.57442949 0.56096465 0.59611192]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.562776

Running experiment 20/50...
Shuffled Demos: [([(0, 1), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 3), (4, 2), (3, 3), (3, 3), (4, 0), (1, 0), (1, 2), (0, 3)]), ([(0, 3), (1, 0), (1, 1), (4, 0), (1, 3), (2, 0), (2, 3), (2, 1), (5, 0)], [(0, 3), (1, 0), (1, 1), (4, 0), (1, 3), (2, 0), (2, 3), (2, 3), (5, None)]), ([(0, 3), (3, 0), (0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (3, 0), (0, 0), (0, 2), (0, 0), (0, 1), (0, 0), (0, 3), (0, 0), (0, 2)]), ([(0, 2), (3, 0), (0, 1), (0, 1), (3, 3), (4, 3), (1, 1), (2, 1), (5, 0), (5, 0)], [(0, 2), (3, 0), (0, 3), (1, 0), (1, 1), (4, 1), (4, 1), (4, 2), (3, 0), (0, 0)]), ([(0, 2), (0, 3), (3, 3), (3, 3), (4, 3), (5, 0)], [(0, 2), (0, 3), (3, 2), (3, 3), (4, 3), (5, None)]), ([(0, 0), (0, 1), (3, 3), (4, 0), (1, 0), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 3), (4, 0), (1, 0), (1, 3), (2, 3), (2, 1), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6670
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.53549636 -0.30931491  0.78585491]
True reward weights: [-0.61012931 -0.3129562   0.72787406]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6600
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.53115469 -0.84133012  0.10019148]
True reward weights: [ 0.2344368  -0.92136559  0.31004005]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147657

Running PBIRL with 3 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.68810411 -0.70731899  0.161903  ]
True reward weights: [-0.83404231 -0.48082493  0.27051952]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.163310

Running PBIRL with 4 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5728
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.4296406  -0.8838487   0.18498764]
True reward weights: [ 0.87516467 -0.08726541  0.47589028]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.166179

Running PBIRL with 5 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5604
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.5945856  -0.72366414 -0.35039718]
True reward weights: [-0.87067124  0.11588907  0.47801812]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.217159

Running PBIRL with 6 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.5736
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.21957396 -0.94088623 -0.25791546]
True reward weights: [-0.59626124 -0.7689109   0.23075649]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.214654

Saving results to files...
Results saved successfully.

Running experiment 21/50...
Shuffled Demos: [([(0, 1), (3, 2), (3, 3), (3, 3), (4, 1), (4, 1), (4, 2), (3, 3), (3, 3), (4, 3)], [(0, 1), (3, 2), (3, 3), (3, 3), (4, 1), (4, 1), (4, 2), (3, 1), (3, 1), (3, 2)]), ([(0, 2), (0, 1), (3, 0), (0, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 0), (0, 3), (0, 2), (0, 1), (0, 1), (3, 3), (4, 0), (1, 3)]), ([(0, 1), (0, 0), (0, 0), (1, 0), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (0, 0), (0, 0), (1, 0), (1, 2), (0, 1), (3, 3), (4, 0), (1, 1), (2, 3)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 2), (1, 2), (1, 3), (2, 3), (5, None)]), ([(0, 0), (0, 2), (0, 3), (1, 1), (4, 0), (1, 3), (1, 0), (1, 2), (0, 1), (3, 3)], [(0, 0), (0, 2), (0, 3), (1, 1), (4, 0), (1, 3), (1, 0), (1, 2), (0, 2), (0, 1)]), ([(0, 2), (0, 1), (3, 1), (3, 0), (0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 1), (3, 0), (0, 3), (1, 1), (4, 2), (3, 0), (0, 1), (1, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 21
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.49362141  0.60781691 -0.62201006]
True reward weights: [-0.3738588  -0.3262014   0.86822937]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6636
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.30401296 -0.91375868 -0.26948319]
True reward weights: [-0.44183989  0.83838687 -0.31919425]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183287

Running PBIRL with 3 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6506
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.85312263 -0.51502032 -0.0832817 ]
True reward weights: [-0.29755512  0.26104101  0.91832377]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.171952

Running PBIRL with 4 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5130
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.54772105 -0.6937046   0.46773453]
True reward weights: [-0.08397714  0.1637404   0.98292264]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.200029

Running PBIRL with 5 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5206
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.75287231 -0.54624061  0.36715729]
True reward weights: [-0.45515579 -0.55293462  0.697923  ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.242542

Running PBIRL with 6 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5196
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.11715283 -0.82718377  0.54958369]
True reward weights: [0.1901613  0.17606455 0.9658364 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.234944

Running experiment 22/50...
Shuffled Demos: [([(0, 2), (0, 3), (3, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 2), (0, 3), (3, 0), (0, 3), (3, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 3), (0, 1), (3, 3), (0, 2), (0, 1), (3, 1), (3, 3), (4, 1)]), ([(0, 2), (0, 3), (1, 1), (0, 2), (0, 1), (3, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 2), (0, 3), (1, 1), (0, 2), (0, 1), (3, 1), (3, 2), (3, 0), (0, 0), (0, 3)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 2), (0, 3), (1, 0), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 3), (1, 0), (2, 0), (2, 1), (1, 2), (0, 2), (3, 0), (0, 1), (3, 2)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (1, 1)], [(0, 3), (1, 0), (1, 0), (1, 3), (2, 2), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6514
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.91987571 -0.02109266  0.39164241]
True reward weights: [-0.70965126 -0.6051058   0.36089066]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6404
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.06533076 -0.85879999  0.50812839]
True reward weights: [-0.49606499 -0.51833726  0.69659601]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126595

Running PBIRL with 3 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6532
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.95576029 -0.28391711  0.07689823]
True reward weights: [ 0.12763112 -0.746513   -0.65301504]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.125681

Running PBIRL with 4 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6376
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.72628333 -0.25914533  0.63667591]
True reward weights: [ 0.81760388 -0.06640769  0.57193873]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.125286

Running PBIRL with 5 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6254
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.28303679 -0.62761015  0.72525559]
True reward weights: [-0.45975614  0.1607904   0.87336747]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.129716

Running PBIRL with 6 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.2082
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.92842906 -0.0522802   0.3678128 ]
True reward weights: [ 0.30650632 -0.81949267 -0.48423718]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.262805

Saving results to files...
Results saved successfully.

Running experiment 23/50...
Shuffled Demos: [([(0, 1), (0, 2), (0, 2), (0, 0), (0, 3), (1, 1), (4, 3), (1, 1), (4, 3), (5, 0)], [(0, 1), (0, 2), (0, 2), (0, 0), (0, 3), (1, 3), (2, 2), (2, 2), (1, 0), (1, 1)]), ([(0, 2), (0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 3), (1, 0), (1, 3), (2, 0), (2, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, 0)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (0, 1), (3, 2), (0, 2), (0, 2), (0, 2), (0, 3), (1, 2), (0, 3), (1, 3)]), ([(0, 2), (0, 1), (3, 2), (3, 2), (3, 2), (3, 3), (0, 1), (3, 3), (0, 1), (3, 3)], [(0, 2), (0, 1), (3, 2), (3, 2), (3, 2), (3, 2), (0, 1), (3, 2), (3, 1), (4, 1)]), ([(0, 0), (0, 3), (1, 0), (1, 3), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 0), (1, 3), (2, 0), (2, 2), (1, 3), (4, 1), (4, 2), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6588
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.80512941 -0.46174818 -0.3722301 ]
True reward weights: [-0.74528522 -0.55533186  0.36899385]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123671

Running PBIRL with 2 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6072
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.82624016 -0.41959459 -0.37585579]
True reward weights: [-0.75325475 -0.6491971   0.10559544]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.133996

Running PBIRL with 3 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.4372
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.93348292  0.22926878  0.27576341]
True reward weights: [-0.70062312 -0.70765006 -0.09142556]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.164857

Running PBIRL with 4 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.4282
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.83665315 -0.1621511   0.52318115]
True reward weights: [-0.49944037  0.80978803  0.30790041]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.159318

Running PBIRL with 5 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.4234
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.04937022 0.80535571 0.5907324 ]
True reward weights: [-0.74734831 -0.28879651  0.59838706]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.176220

Running PBIRL with 6 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.3762
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.66971746  0.58186302  0.46142599]
True reward weights: [-0.54850002 -0.23313059  0.80299306]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.186560

Running experiment 24/50...
Shuffled Demos: [([], [(0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 0), (0, 2), (0, 1), (3, 3), (4, 3), (4, 3), (4, 3), (5, 0), (5, 0)], [(0, 1), (3, 0), (0, 2), (0, 0), (0, 1), (3, 1), (3, 1), (3, 2), (3, 3), (4, 0)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 3), (1, 1), (2, 1), (2, 3), (2, 1), (5, None)]), ([(0, 0), (0, 0), (0, 1), (3, 2), (3, 2), (3, 0), (4, 3), (1, 1), (0, 1)], [(0, 0), (0, 0), (0, 1), (3, 2), (3, 2), (3, 0), (4, 1), (4, 3), (5, None)]), ([(0, 1), (3, 0), (0, 0), (0, 0), (0, 2), (0, 1), (3, 2), (3, 3), (3, 3), (4, 3)], [(0, 1), (3, 0), (0, 0), (0, 0), (0, 2), (0, 1), (3, 2), (3, 3), (3, 1), (3, 0)]), ([(0, 3), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (3, 2), (3, 0), (0, 1), (3, 3), (4, 0), (1, 1), (4, 0), (1, 0), (2, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6560
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.30125502 -0.21299582 -0.92945048]
True reward weights: [-0.68144434 -0.29187893  0.6711485 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.5214
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.4524657  -0.5270595   0.71936296]
True reward weights: [ 0.32883376 -0.21376903 -0.91987562]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147109

Running PBIRL with 3 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.3770
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.29341598 -0.55522257  0.77822552]
True reward weights: [-0.07793824 -0.99643     0.0324482 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.177576

Running PBIRL with 4 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.0160
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.25804204 -0.93124548  0.25728615]
True reward weights: [-0.62279456 -0.65508416 -0.42777528]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.411245

Running PBIRL with 5 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.0156
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.15502448 -0.97561316 -0.15539106]
True reward weights: [-0.39340365 -0.83280225 -0.38945345]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.422156

Running PBIRL with 6 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.0160
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.66808322 -0.32718057  0.66829461]
True reward weights: [-0.07543628 -0.99424207 -0.07610564]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.395983

Saving results to files...
Results saved successfully.

Running experiment 25/50...
Shuffled Demos: [([(0, 1), (3, 3), (3, 1), (3, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 3), (3, 1), (3, 1), (3, 0), (0, 3), (0, 2), (0, 0), (0, 3), (1, 1)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 0), (4, 2), (3, 3), (4, 2), (3, 1), (3, 3), (0, 1), (3, 3), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 3), (4, 0), (1, 2), (0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (1, 1), (4, 2), (3, 3), (3, 1), (3, 3), (3, 0), (0, 1), (3, 3), (4, 3)], [(0, 1), (1, 1), (4, 2), (3, 3), (3, 1), (3, 3), (3, 0), (0, 0), (0, 0), (1, 1)]), ([(0, 1), (3, 1), (4, 2), (3, 0), (0, 0), (0, 1), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 1), (3, 1), (4, 2), (3, 0), (0, 0), (0, 2), (0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 2), (3, 0), (0, 1), (0, 1), (1, 1), (0, 1), (3, 3), (4, 3), (4, 3), (1, 1)], [(0, 2), (3, 0), (0, 0), (0, 1), (3, 2), (3, 0), (0, 3), (1, 1), (4, 1), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6608
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.93063994 -0.28873323  0.22481644]
True reward weights: [-0.44714936 -0.30119858  0.84222139]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6256
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.50397888 -0.42014435  0.75464164]
True reward weights: [ 0.33113114 -0.90732528 -0.25906178]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.128770

Running PBIRL with 3 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5922
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.45372406  0.04504037  0.89000328]
True reward weights: [ 0.67374239 -0.73895821 -0.00345773]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.135507

Running PBIRL with 4 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.3272
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.33326945  0.63588968  0.69611478]
True reward weights: [-0.63277577 -0.60948997 -0.47761574]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.200587

Running PBIRL with 5 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.3402
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.38786273  0.49911808  0.77488299]
True reward weights: [-0.76540336  0.12171403  0.63193623]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.205574

Running PBIRL with 6 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.0202
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.13230556 -0.13291903  0.98225647]
True reward weights: [-0.61147735  0.50580306  0.60848889]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.533040

Running experiment 26/50...
Shuffled Demos: [([(0, 3), (1, 0), (1, 3), (2, 1), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 0), (1, 3), (2, 2), (1, 1), (4, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 3), (4, 1), (4, 0), (1, 3), (2, 1), (5, None)]), ([(0, 0), (0, 2), (0, 2), (0, 0), (0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 2), (0, 0), (0, 3), (1, 3), (2, 2), (1, 1), (4, 3), (5, None)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 2), (0, 1), (3, 0), (0, 2), (0, 0), (0, 1), (3, 0), (0, 2), (0, 2)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 2), (0, 3), (1, 2), (0, 1), (3, 0), (0, 1), (3, 3), (4, 3)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 2), (0, 1), (3, 2), (3, 3), (4, 2), (3, 1), (3, 3), (4, 1), (4, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6592
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.16409549 -0.82537254  0.54021555]
True reward weights: [-0.7333898  -0.47830978  0.48307263]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6522
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.70151149 -0.62419183 -0.34389852]
True reward weights: [-0.90282399 -0.15750864  0.40012482]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124097

Running PBIRL with 3 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6732
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.54170586 -0.59221407  0.59652096]
True reward weights: [-0.14077518  0.8528633   0.50279871]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124049

Running PBIRL with 4 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.5824
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.41642746 -0.71470627  0.56194583]
True reward weights: [0.32607738 0.22022506 0.91933371]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.140188

Running PBIRL with 5 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6014
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.24467657 -0.72420259  0.64472008]
True reward weights: [ 0.3957371  -0.91142065  0.11271443]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.136900

Running PBIRL with 6 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.5982
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.36549593 -0.05196339  0.92936136]
True reward weights: [-0.08414371 -0.9096263   0.40681671]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.134544

Saving results to files...
Results saved successfully.

Running experiment 27/50...
Shuffled Demos: [([(0, 2), (3, 0), (0, 1), (3, 3), (4, 3), (4, 3), (5, 0), (5, 0)], [(0, 2), (3, 0), (0, 1), (3, 2), (3, 1), (3, 3), (4, 3), (5, None)]), ([(0, 0), (0, 1), (3, 0), (4, 0), (3, 3), (4, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 0), (4, 0), (3, 2), (3, 1), (3, 3), (4, 0), (1, 1), (0, 3)]), ([(0, 3), (0, 3), (0, 3), (1, 1), (4, 0), (1, 1), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 3), (0, 3), (0, 3), (1, 1), (4, 0), (1, 0), (2, 3), (2, 3), (2, 3), (5, None)]), ([(0, 1), (3, 3), (4, 2), (3, 2), (3, 0), (0, 1), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 1), (3, 3), (4, 2), (3, 2), (3, 0), (0, 1), (0, 1), (3, 1), (3, 1), (4, 2)]), ([(0, 1), (3, 0), (0, 0), (0, 2), (0, 0), (0, 3), (1, 0), (1, 1), (4, 3), (5, 0)], [(0, 1), (3, 0), (0, 0), (0, 2), (0, 0), (0, 3), (1, 0), (1, 3), (2, 2), (1, 3)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 0), (0, 3), (1, 3), (2, 0), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6590
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.72760137 0.22176829 0.6491649 ]
True reward weights: [-0.65557811 -0.01374154  0.75500233]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6238
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.10463434 -0.97025837  0.21828959]
True reward weights: [ 0.2100191  -0.22105719  0.95237897]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.130303

Running PBIRL with 3 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6230
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.39358549 -0.91928684  0.00147004]
True reward weights: [-0.96683136 -0.25114851  0.04649241]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152298

Running PBIRL with 4 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6392
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.08348491 -0.95485895 -0.2850871 ]
True reward weights: [ 0.74527753 -0.3603636   0.56098082]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.151266

Running PBIRL with 5 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5324
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.42375565 -0.85840849  0.28907788]
True reward weights: [-0.36589868 -0.87459079 -0.31813378]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.178391

Running PBIRL with 6 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5270
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.0625653  -0.9141146   0.40059965]
True reward weights: [0.09057925 0.33589764 0.93753302]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.172834

Running experiment 28/50...
Shuffled Demos: [([(0, 1), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 3), (1, 0), (1, 3), (2, 1), (5, None)]), ([(0, 3), (1, 3), (1, 0), (1, 1), (4, 0), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 3), (1, 0), (1, 1), (4, 0), (1, 3), (1, 3), (4, 2), (3, 3), (4, 0)]), ([(0, 1), (3, 2), (3, 0), (3, 3), (3, 3), (0, 1), (1, 1), (4, 3), (5, 0), (5, 0)], [(0, 1), (3, 2), (3, 0), (3, 3), (3, 2), (3, 2), (0, 1), (1, 1), (4, 3), (5, None)]), ([(0, 2), (0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 2), (0, 0), (0, 2), (0, 2), (0, 0), (0, 2), (0, 2), (0, 2), (0, 0)]), ([(0, 3), (0, 1), (1, 1), (4, 2), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 3), (0, 1), (1, 1), (4, 2), (3, 0), (3, 3), (4, 3), (5, None)]), ([(0, 3), (1, 3), (2, 1), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 3), (2, 0), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.64778684 -0.44887269  0.61553678]
True reward weights: [-0.866301   -0.34967581  0.35672034]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6562
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.22760575 -0.2808187   0.93238215]
True reward weights: [0.71458473 0.29350331 0.63499959]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123657

Running PBIRL with 3 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6148
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.14044478 -0.9606369  -0.23969149]
True reward weights: [-0.21353893  0.46173491  0.86093089]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.131519

Running PBIRL with 4 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6072
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.36876169 -0.85938036 -0.35423187]
True reward weights: [-0.64471164 -0.57514501 -0.50354257]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.129598

Running PBIRL with 5 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6244
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.40701996  0.23992505  0.88134597]
True reward weights: [ 0.19386041 -0.85169664  0.48685827]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.128607

Running PBIRL with 6 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.5962
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.59894389  0.29224392  0.74555999]
True reward weights: [-0.5015924   0.4633272   0.73057031]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.135097

Saving results to files...
Results saved successfully.

Running experiment 29/50...
Shuffled Demos: [([(0, 2), (0, 2), (0, 3), (1, 0), (1, 3), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 2), (0, 3), (1, 0), (1, 3), (2, 2), (1, 2), (0, 2), (3, 1), (3, 2)]), ([(0, 0), (0, 2), (0, 3), (3, 3), (4, 0), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 3), (3, 3), (4, 0), (1, 2), (0, 3), (1, 2), (0, 2), (3, 1)]), ([(0, 1), (0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 2), (0, 1), (3, 2), (3, 2), (3, 2), (0, 1), (0, 0), (0, 1), (3, 0)]), ([(0, 3), (1, 0), (1, 1), (4, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 0), (1, 1), (4, 1), (4, 0), (1, 1), (4, 1), (4, 1), (5, None)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (3, 3), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 3), (1, 0), (1, 2), (0, 2), (0, 0), (1, 0), (0, 3), (1, 1), (4, 1), (4, 3)]), ([(0, 1), (3, 0), (0, 1), (3, 1), (3, 3), (4, 2), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 1), (3, 0), (0, 1), (3, 1), (3, 3), (4, 2), (3, 3), (4, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6562
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.18569731 -0.96181673  0.20105992]
True reward weights: [-0.6535586  -0.23072892  0.72085041]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6556
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.68569605  0.59700613  0.41641879]
True reward weights: [-0.99209891 -0.07634869  0.09955214]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.125462

Running PBIRL with 3 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6170
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.02859685 0.4618961  0.8864729 ]
True reward weights: [0.25837139 0.00570766 0.9660288 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.132017

Running PBIRL with 4 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6302
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.95100705  0.00916594  0.30903328]
True reward weights: [-0.80966174 -0.58619363 -0.02872099]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.129760

Running PBIRL with 5 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.3826
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.35117177 0.58364287 0.73214711]
True reward weights: [ 0.70247131 -0.70398632 -0.10458164]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.196371

Running PBIRL with 6 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.3884
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.63146415 -0.45648585  0.62679637]
True reward weights: [-0.42735859 -0.12603514  0.89525403]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.180830

Running experiment 30/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 2), (3, 1), (3, 0), (0, 1), (3, 1), (3, 3), (4, 3), (5, 0)], [(0, 2), (0, 1), (3, 2), (3, 1), (3, 0), (0, 1), (3, 1), (3, 3), (4, 1), (5, None)]), ([(0, 3), (0, 1), (3, 2), (3, 1), (3, 3), (4, 3), (5, 0)], [(0, 3), (0, 1), (3, 2), (3, 1), (3, 3), (4, 0), (5, None)]), ([(0, 1), (3, 2), (3, 2), (3, 3), (4, 3), (5, 0)], [(0, 1), (3, 2), (3, 2), (3, 3), (4, 1), (5, None)]), ([(0, 3), (0, 1), (3, 0), (4, 2), (3, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 3), (0, 1), (3, 0), (4, 2), (3, 0), (0, 0), (0, 0), (0, 1), (3, 1), (3, 1)]), ([(0, 0), (0, 1), (3, 3), (3, 0), (0, 2), (0, 3), (0, 2), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 1), (3, 3), (3, 0), (0, 2), (0, 3), (0, 2), (0, 3), (1, 1), (0, 0)]), ([(0, 2), (0, 3), (1, 2), (0, 2), (0, 0), (0, 1), (3, 2), (3, 3), (0, 1), (3, 3)], [(0, 2), (0, 3), (1, 2), (0, 2), (0, 0), (0, 1), (3, 2), (3, 2), (3, 2), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 30
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.22995375  0.94670286  0.22555478]
True reward weights: [-0.81326386 -0.04441834  0.5801973 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 30
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.06432689 -0.97632503 -0.20652235]
True reward weights: [-0.91545492  0.3317211  -0.22782316]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.202351

Running PBIRL with 3 demonstrations for experiment 30
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.25744529 0.94821154 0.18605589]
True reward weights: [ 0.30403095 -0.10687723  0.946648  ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.272611

Running PBIRL with 4 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.6790
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.17105912 -0.96159845 -0.2146327 ]
True reward weights: [ 0.50341873 -0.3794153   0.77628192]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.332988

Running PBIRL with 5 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.3216
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.06382747  0.47274907  0.87888245]
True reward weights: [ 0.16943219 -0.64437038  0.74570742]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.400417

Running PBIRL with 6 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.3356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.11752755  0.61596389  0.77895812]
True reward weights: [-0.72179832  0.01420336  0.6919577 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.446603

Saving results to files...
Results saved successfully.

Running experiment 31/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 1), (3, 3), (4, 1), (4, 3), (5, None)]), ([(0, 2), (0, 1), (3, 1), (3, 1), (3, 0), (0, 0), (0, 1), (1, 1), (4, 3), (5, 0)], [(0, 2), (0, 1), (3, 1), (3, 1), (3, 0), (0, 0), (0, 1), (1, 2), (0, 1), (3, 3)]), ([(0, 1), (3, 2), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 2), (3, 0), (0, 3), (0, 0), (0, 3), (1, 2), (1, 1), (4, 2), (3, 3)]), ([(0, 0), (0, 1), (3, 0), (3, 1), (3, 3), (4, 3), (4, 3), (5, 0)], [(0, 0), (0, 1), (3, 0), (3, 1), (3, 2), (3, 3), (4, 0), (5, None)]), ([(0, 2), (0, 3), (1, 1), (4, 3), (5, 0)], [(0, 2), (0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 3), (1, 3), (2, 0), (2, 0), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6744
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.49130072 -0.49088938  0.71947983]
True reward weights: [-0.27534761 -0.19938474  0.94044108]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6644
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.12446422 0.45293707 0.8828118 ]
True reward weights: [ 0.99121739 -0.05406767  0.1206846 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124084

Running PBIRL with 3 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6154
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.23762979 -0.88038557 -0.41043066]
True reward weights: [ 0.40111494 -0.76657144  0.50147285]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.131828

Running PBIRL with 4 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6172
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.03839108 -0.99351355  0.10703713]
True reward weights: [-0.10155292 -0.90094825 -0.42187588]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.153918

Running PBIRL with 5 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6228
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.09207944 -0.91790588  0.38596655]
True reward weights: [-0.9797753  -0.15689407 -0.12419583]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.207618

Running PBIRL with 6 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6122
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.21747301 -0.89809606 -0.38226817]
True reward weights: [ 0.57461491 -0.81627711  0.05924006]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.206933

Running experiment 32/50...
Shuffled Demos: [([(0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 0), (1, 0), (1, 3), (4, 3), (5, None)]), ([(0, 3), (1, 3), (2, 2), (1, 1), (4, 2), (3, 1), (3, 2), (3, 0), (0, 1), (3, 3)], [(0, 3), (1, 3), (2, 2), (1, 1), (4, 2), (3, 1), (3, 2), (3, 0), (0, 3), (1, 2)]), ([(0, 3), (1, 1), (4, 0), (3, 3), (4, 3), (1, 1), (4, 3)], [(0, 3), (1, 1), (4, 0), (3, 0), (3, 3), (4, 3), (5, None)]), ([(0, 3), (1, 0), (1, 0), (1, 1), (4, 2), (3, 0), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 3), (1, 0), (1, 0), (1, 1), (4, 2), (3, 0), (0, 1), (3, 2), (3, 3), (4, 2)]), ([(0, 2), (0, 2), (0, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 2), (0, 2), (0, 2), (0, 1), (3, 2), (3, 2), (3, 1), (4, 3), (5, None)]), ([(0, 1), (3, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 1), (4, 0), (1, 0), (1, 3), (4, 0), (1, 1), (4, 0), (1, 1), (4, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6546
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.83854707 -0.54318524 -0.04229188]
True reward weights: [-0.90753571 -0.40234227  0.1204144 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5670
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.57799167  0.05253629  0.81434978]
True reward weights: [-0.90186766 -0.05991206  0.4278379 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.140544

Running PBIRL with 3 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.1256
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.2146479   0.81641948 -0.53608349]
True reward weights: [-0.76013678 -0.11260428 -0.63993152]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.233375

Running PBIRL with 4 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.0044
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.59003659 -0.55802151 -0.58349706]
True reward weights: [-0.28509728  0.66880968 -0.68659534]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.767273

Running PBIRL with 5 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.0032
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.57943748 0.57153387 0.58103463]
True reward weights: [-0.60537447 -0.56375356 -0.56187514]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.639549

Running PBIRL with 6 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.0014
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.58329925 -0.5918549  -0.55630007]
True reward weights: [-0.58202924 -0.59142554 -0.55808403]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.611489

Saving results to files...
Results saved successfully.

Running experiment 33/50...
Shuffled Demos: [([(0, 3), (1, 2), (0, 2), (0, 2), (0, 0), (0, 0), (0, 2), (0, 0), (0, 1), (3, 3)], [(0, 3), (1, 2), (0, 2), (0, 2), (0, 0), (0, 0), (0, 2), (0, 0), (0, 0), (0, 3)]), ([(0, 1), (3, 1), (3, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 1), (3, 1), (3, 2), (3, 0), (0, 0), (0, 1), (3, 0), (0, 1), (3, 2)]), ([(0, 1), (1, 2), (4, 1), (4, 3), (1, 1), (4, 3), (5, 0)], [(0, 1), (1, 2), (4, 1), (4, 0), (1, 3), (2, 1), (5, None)]), ([(0, 0), (0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 2), (4, 3), (1, 0), (2, 3), (2, 1), (5, None)]), ([(0, 2), (0, 3), (1, 1), (4, 3), (4, 3), (5, 0)], [(0, 2), (0, 3), (1, 0), (1, 3), (2, 1), (5, None)]), ([(0, 0), (0, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 1), (3, 1), (4, 0), (1, 2), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 33
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.1067994   0.8899892  -0.44329799]
True reward weights: [-0.32556312 -0.30396656  0.89532842]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6604
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.00843523 -0.73705738  0.67577752]
True reward weights: [-0.65506919 -0.11100396  0.74737038]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182901

Running PBIRL with 3 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6670
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.62706092 -0.64785136 -0.43253118]
True reward weights: [-0.63172403 -0.64057932 -0.436558  ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.224810

Running PBIRL with 4 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6218
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.08955529 -0.74254896  0.66377775]
True reward weights: [-0.6810306   0.02477417  0.73183575]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.229102

Running PBIRL with 5 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.3318
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.26357752 0.59718944 0.75755638]
True reward weights: [-0.9874907  -0.04876735  0.14994619]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.306800

Running PBIRL with 6 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.3256
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.0237898  0.63046804 0.77585056]
True reward weights: [-0.82107243 -0.35308329  0.4485223 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.282486

Running experiment 34/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 0), (1, 1), (0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, 0)], [(0, 1), (3, 3), (4, 0), (1, 2), (0, 3), (1, 1), (4, 2), (3, 0), (0, 1), (3, 0)]), ([(0, 3), (1, 1), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 1), (0, 2), (0, 2), (0, 0), (0, 2), (0, 1), (3, 1), (3, 0), (0, 3)]), ([(0, 3), (0, 3), (1, 0), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (0, 3), (1, 0), (1, 3), (4, 0), (1, 0), (1, 0), (1, 1), (4, 0), (1, 0)]), ([(0, 0), (0, 1), (3, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 1), (3, 1), (3, 1), (4, 2), (3, 3), (4, 2), (3, 3), (4, 1)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 2), (3, 0), (0, 3), (1, 2), (0, 2), (0, 2), (0, 1), (3, 0)]), ([(0, 1), (3, 0), (0, 0), (0, 1), (0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, 0)], [(0, 1), (3, 0), (0, 0), (0, 2), (0, 1), (3, 2), (3, 1), (3, 1), (3, 3), (4, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.6650
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.3822783   0.41675817  0.82472779]
True reward weights: [-0.839382   -0.393236    0.37523766]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.6540
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.89773808 -0.33393655  0.2873199 ]
True reward weights: [-0.582409    0.41610309  0.69832512]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123976

Running PBIRL with 3 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5516
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.06233707  0.4232009   0.90388887]
True reward weights: [ 0.19211518 -0.86452259  0.46442701]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.149286

Running PBIRL with 4 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5502
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.21140508 -0.60770615  0.76550711]
True reward weights: [-0.52594745  0.38488375  0.75844827]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.142104

Running PBIRL with 5 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.5472
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.83946575  0.06199088  0.53986516]
True reward weights: [-0.73951897 -0.13293217  0.65987933]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.138607

Running PBIRL with 6 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.4782
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.12924363 -0.05049758  0.99032625]
True reward weights: [ 0.5453472  -0.15520355  0.82371615]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.160146

Saving results to files...
Results saved successfully.

Running experiment 35/50...
Shuffled Demos: [([(0, 2), (0, 0), (0, 1), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 0), (0, 3), (1, 1), (4, 0), (1, 2), (0, 1), (3, 2), (3, 1), (3, 2)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (3, 1), (3, 1), (3, 2), (3, 3), (4, 1), (5, None)]), ([(0, 3), (1, 2), (0, 2), (0, 0), (0, 2), (0, 3), (1, 2), (0, 1), (3, 3), (4, 3)], [(0, 3), (1, 2), (0, 2), (0, 0), (0, 2), (0, 3), (1, 2), (0, 2), (3, 0), (0, 1)]), ([(0, 0), (0, 3), (1, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 2), (0, 1), (3, 0), (0, 3), (1, 0), (1, 1), (0, 3), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 0), (0, 2), (3, 1), (3, 0), (0, 2), (0, 2), (3, 2), (3, 2), (3, 3)]), ([(0, 1), (3, 1), (3, 1), (3, 0), (3, 3), (4, 3), (1, 3), (4, 1), (4, 3), (5, 0)], [(0, 1), (3, 1), (3, 1), (3, 0), (3, 3), (4, 3), (1, 3), (4, 1), (4, 2), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.41373741  0.18535363  0.89132788]
True reward weights: [-0.94121751 -0.05112011  0.33391067]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6456
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.83295677 -0.54905073 -0.06874821]
True reward weights: [-0.4493374  -0.37670149  0.81005672]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.127408

Running PBIRL with 3 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6576
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.12618161 -0.96889461  0.21288832]
True reward weights: [0.57017413 0.54824673 0.61182267]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.150167

Running PBIRL with 4 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5640
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.06634866 -0.78698492  0.61339432]
True reward weights: [-0.45493407  0.60936434  0.64939209]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.172066

Running PBIRL with 5 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5674
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.14936698 -0.97766119  0.14787865]
True reward weights: [-0.485581   -0.46271251  0.74169281]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.166942

Running PBIRL with 6 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.5612
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.21712056 -0.76931402  0.6008449 ]
True reward weights: [-0.63966538  0.46136324  0.61479441]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.163753

Running experiment 36/50...
Shuffled Demos: [([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 1), (4, 1), (4, 2), (3, 1), (4, 1), (4, 3), (5, None)]), ([(0, 1), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 3), (1, 1), (4, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3)], [(0, 3), (1, 1), (4, 1), (4, 3), (5, None)]), ([(0, 2), (0, 1), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 2), (0, 2), (0, 2), (3, 1), (4, 1), (5, None)]), ([(0, 1), (1, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 1), (1, 2), (0, 0), (1, 0), (1, 3), (2, 2), (5, None)]), ([(0, 3), (1, 1), (4, 1), (4, 2), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 1), (4, 1), (4, 2), (1, 3), (1, 0), (1, 1), (4, 0), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.6682
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.27791694  0.3951941   0.87554771]
True reward weights: [-0.05456508 -0.040557    0.99768621]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.63906828 -0.31520305  0.7015973 ]
True reward weights: [-0.02734103  0.85647324  0.51546683]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.133884

Running PBIRL with 3 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.1868
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.93081421  0.11778523  0.34599357]
True reward weights: [-0.04645337 -0.60193011  0.79719648]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.218817

Running PBIRL with 4 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.0732
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.73819046  0.10299165  0.66668401]
True reward weights: [-0.90621036 -0.19333812  0.3760361 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.240215

Running PBIRL with 5 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.0738
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.85249973 -0.02975281  0.52188023]
True reward weights: [-0.87050157 -0.11572095  0.47836771]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.208972

Running PBIRL with 6 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.0730
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.75120954  0.09160412  0.65367646]
True reward weights: [-0.78419926 -0.50627955 -0.35876529]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.191418

Saving results to files...
Results saved successfully.

Running experiment 37/50...
Shuffled Demos: [([(0, 0), (0, 3), (1, 3), (2, 1), (1, 1), (4, 3)], [(0, 0), (0, 3), (1, 3), (2, 3), (2, 1), (5, None)]), ([(0, 2), (3, 1), (3, 1), (4, 2), (3, 2), (3, 2), (3, 3), (4, 3), (1, 1), (4, 3)], [(0, 2), (3, 1), (3, 1), (4, 2), (3, 2), (3, 2), (3, 1), (3, 1), (3, 1), (4, 0)]), ([(0, 1), (0, 1), (3, 3), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 0), (0, 1), (3, 3), (0, 2), (0, 3), (3, 1), (3, 0), (0, 2)]), ([(0, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (0, 3), (1, 1), (4, 0), (1, 0), (1, 2), (1, 2), (0, 0), (0, 3), (3, 0)]), ([(0, 3), (3, 3), (4, 0), (1, 2), (4, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (3, 3), (4, 0), (1, 2), (4, 3), (4, 2), (3, 1), (4, 0), (1, 2), (0, 0)]), ([(0, 1), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 0), (1, 2), (1, 0), (1, 3), (2, 2), (2, 3), (2, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6570
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.1434452   0.95276935 -0.2676827 ]
True reward weights: [-0.75478167 -0.33513042  0.563908  ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.5376
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.78057382  0.28653123 -0.5555217 ]
True reward weights: [ 0.73403763 -0.67873899 -0.0224087 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.145837

Running PBIRL with 3 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.3334
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.86842507 0.05041344 0.49325084]
True reward weights: [ 0.90376988  0.41005416 -0.12270126]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.184526

Running PBIRL with 4 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.1224
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.24296231 -0.96568225 -0.09179931]
True reward weights: [ 0.6923236  -0.68682705 -0.22126144]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.236691

Running PBIRL with 5 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.1240
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.68986811 -0.62768624  0.36068265]
True reward weights: [-0.07350318 -0.93682916 -0.3419772 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.206338

Running PBIRL with 6 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.1246
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.34477999 -0.93863606 -0.0094395 ]
True reward weights: [-0.20925972 -0.93438292 -0.28833823]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.188448

Running experiment 38/50...
Shuffled Demos: [([(0, 0), (0, 0), (1, 1), (4, 2), (4, 0), (1, 1), (2, 1), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 0), (1, 1), (4, 2), (4, 0), (1, 3), (2, 2), (1, 1), (4, 3), (5, None)]), ([(0, 2), (0, 3), (1, 0), (1, 0), (1, 3), (2, 1), (5, 0)], [(0, 2), (0, 3), (1, 0), (1, 0), (1, 3), (2, 2), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 0), (0, 3), (0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 3), (1, 0), (1, 2), (0, 1), (3, 1), (3, 1), (3, 1), (3, 3), (4, 3), (5, 0)], [(0, 3), (1, 0), (1, 2), (0, 1), (3, 1), (3, 1), (3, 1), (3, 0), (4, 2), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 1), (3, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 1), (3, 0), (3, 0), (0, 3), (1, 0), (1, 1), (4, 3), (5, 0), (5, 0)], [(0, 1), (3, 1), (3, 0), (3, 0), (0, 3), (1, 0), (1, 0), (1, 3), (2, 0), (2, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6632
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.38885369 -0.41939728  0.82030405]
True reward weights: [-0.49248773 -0.22865786  0.83974486]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6648
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.52608201 0.0424255  0.84937495]
True reward weights: [-0.76142206  0.57782437  0.29386297]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147779

Running PBIRL with 3 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6532
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.74188731 -0.61735377  0.26168214]
True reward weights: [ 0.76598921 -0.31998183  0.55755911]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148084

Running PBIRL with 4 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.5946
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.39763741 0.15227169 0.90481922]
True reward weights: [-0.90163587  0.33216554 -0.2769816 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.164492

Running PBIRL with 5 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.5870
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.22548889  0.11498721  0.96743615]
True reward weights: [0.71620108 0.24754557 0.65251605]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.161289

Running PBIRL with 6 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6014
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.50603274  0.32995369  0.79690742]
True reward weights: [ 0.73782826 -0.50426216  0.44869714]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.158969

Saving results to files...
Results saved successfully.

Running experiment 39/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (1, 1)], [(0, 3), (1, 1), (2, 1), (5, None)]), ([(0, 3), (1, 1), (0, 3), (1, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 1), (0, 3), (1, 2), (0, 2), (0, 0), (0, 3), (1, 0), (1, 0), (0, 2)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (4, 3), (5, 0)], [(0, 2), (0, 2), (0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 0), (0, 2), (0, 2), (0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 2), (0, 3), (1, 0), (1, 0), (1, 3), (2, 3), (5, None)]), ([(0, 3), (1, 2), (0, 2), (0, 3), (3, 1), (3, 3), (4, 1), (4, 3), (5, 0), (5, 0)], [(0, 3), (1, 2), (0, 2), (0, 3), (3, 1), (3, 3), (4, 1), (4, 1), (4, 0), (1, 3)]), ([(0, 2), (3, 1), (3, 2), (3, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 2), (3, 1), (3, 2), (3, 1), (3, 3), (0, 2), (0, 2), (0, 1), (3, 1), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.6618
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.82808715 0.55728571 0.06086304]
True reward weights: [-0.56481614 -0.46100063  0.68444222]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2394
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.53367465  0.78978292  0.30238108]
True reward weights: [-0.63118274  0.70296844 -0.32778609]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.185171

Running PBIRL with 3 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2460
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.90768555  0.06235102 -0.41499312]
True reward weights: [-0.10153634  0.87540391  0.47260805]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.161159

Running PBIRL with 4 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2398
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.43044585  0.7885114   0.43927912]
True reward weights: [-0.89785433  0.31743409  0.30511179]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.149848

Running PBIRL with 5 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.1778
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.74867103  0.62937658  0.208271  ]
True reward weights: [-0.49909525  0.8463092   0.18618449]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.170785

Running PBIRL with 6 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.0160
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.2002514  0.69090823 0.69465473]
True reward weights: [-0.58220284  0.60486389  0.54330426]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.436138

Running experiment 40/50...
Shuffled Demos: [([(0, 0), (0, 3), (1, 3), (4, 2), (3, 2), (3, 1), (3, 2), (3, 3), (4, 3), (5, 0)], [(0, 0), (0, 3), (1, 3), (4, 2), (3, 2), (3, 1), (3, 2), (3, 1), (3, 2), (3, 0)]), ([(0, 1), (3, 2), (3, 0), (0, 0), (0, 3), (1, 2), (1, 1), (0, 2), (3, 3), (4, 3)], [(0, 1), (3, 2), (3, 0), (0, 0), (0, 3), (1, 2), (1, 1), (0, 2), (3, 2), (3, 3)]), ([(0, 1), (3, 3), (4, 2), (3, 3), (4, 3), (1, 2), (1, 2), (0, 2), (0, 1), (0, 1)], [(0, 1), (3, 3), (4, 2), (3, 3), (4, 3), (1, 2), (1, 2), (0, 2), (0, 0), (0, 2)]), ([(0, 3), (1, 0), (1, 0), (1, 1), (4, 2), (3, 3), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 3), (1, 0), (1, 0), (1, 1), (4, 2), (3, 3), (3, 3), (4, 2), (3, 0), (0, 2)]), ([(0, 1), (3, 2), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 2), (0, 3), (0, 0), (0, 2), (0, 0), (0, 1), (3, 3), (4, 1), (4, 0)]), ([(0, 0), (0, 1), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 2), (0, 0), (0, 3), (3, 0), (0, 2), (0, 0), (0, 3), (1, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6558
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.92547263 -0.08203893  0.36982433]
True reward weights: [-0.6667631  -0.57474284  0.47444455]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6612
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.28062601 -0.7695363   0.57364007]
True reward weights: [ 0.02500716 -0.9384325   0.34455635]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148076

Running PBIRL with 3 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6694
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.78936142 -0.59289161 -0.1593364 ]
True reward weights: [ 0.52910391 -0.8484235   0.01505367]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.203018

Running PBIRL with 4 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6584
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.74765106 -0.58235587  0.31918573]
True reward weights: [ 0.49667322 -0.83308888  0.24347205]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.203195

Running PBIRL with 5 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6536
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.28304827 -0.9385015   0.19773367]
True reward weights: [ 0.8583082  -0.43355244 -0.27448009]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.203184

Running PBIRL with 6 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6450
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.37562191 -0.92321342 -0.08114905]
True reward weights: [-0.46834798 -0.25113921  0.8471005 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.208075

Saving results to files...
Results saved successfully.

Running experiment 41/50...
Shuffled Demos: [([(0, 1), (3, 2), (3, 3), (4, 1), (4, 0), (1, 1), (4, 3), (1, 1), (4, 3), (5, 0)], [(0, 1), (3, 2), (3, 3), (4, 1), (4, 0), (1, 0), (1, 2), (0, 2), (0, 3), (1, 1)]), ([(0, 0), (0, 1), (3, 0), (0, 1), (3, 3), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 0), (0, 2), (0, 1), (3, 1), (3, 1), (3, 1), (4, 1), (4, 1)]), ([(0, 3), (1, 0), (1, 0), (1, 3), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 0), (1, 0), (1, 3), (2, 0), (2, 0), (2, 2), (1, 3), (4, 0), (3, 1)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 0), (1, 2), (0, 1), (0, 3), (3, 2), (3, 1), (3, 1), (3, 1), (3, 1)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 0), (0, 3), (3, 3), (4, 2), (1, 3), (2, 0), (2, 1), (2, 0)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, 0)], [(0, 0), (0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.6722
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.76163454  0.64406769 -0.07134165]
True reward weights: [-0.46247666 -0.0267168   0.88622884]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5366
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.66606075  0.34779668  0.65984889]
True reward weights: [0.67979632 0.14209935 0.71950312]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.143682

Running PBIRL with 3 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5338
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.94107961 -0.19935999  0.27317532]
True reward weights: [-0.81213504 -0.24993227  0.52722911]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.137048

Running PBIRL with 4 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5248
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.04443842  0.3390664   0.9397123 ]
True reward weights: [-0.30090342 -0.94836873  0.10026903]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.133464

Running PBIRL with 5 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5344
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.19694581 -0.92598971  0.32211086]
True reward weights: [-0.3846387  -0.72282106  0.57409301]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.131445

Running PBIRL with 6 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.3298
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.92318602  0.1654497   0.34692069]
True reward weights: [-0.20804842  0.66780562  0.71466881]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.194536

Running experiment 42/50...
Shuffled Demos: [([(0, 0), (0, 3), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (3, 2), (3, 1), (3, 3), (4, 3), (4, 3), (5, None)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 0), (0, 2), (0, 1), (1, 0), (1, 0), (1, 2), (0, 0), (0, 1), (1, 0)]), ([(0, 3), (1, 3), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 3), (2, 2), (2, 2), (1, 3), (4, 0), (1, 1), (4, 2), (3, 3), (3, 2)]), ([(0, 1), (3, 1), (3, 1), (3, 2), (3, 1), (3, 2), (0, 3), (1, 1), (2, 1), (1, 1)], [(0, 1), (3, 1), (3, 1), (3, 2), (3, 1), (3, 2), (0, 3), (1, 2), (0, 0), (0, 2)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 2), (3, 1), (3, 1), (3, 1), (3, 2), (3, 2), (3, 2), (3, 0)]), ([(0, 3), (1, 2), (0, 1), (3, 0), (3, 2), (0, 2), (0, 1), (1, 2), (0, 1), (0, 1)], [(0, 3), (1, 2), (0, 1), (3, 0), (3, 2), (0, 2), (0, 1), (1, 2), (0, 0), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.6630
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.72350604 -0.10991943  0.68151062]
True reward weights: [-0.71505276 -0.68463496  0.14133129]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5648
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.74810325 -0.03941239  0.66241089]
True reward weights: [ 0.94443183 -0.32552964 -0.0455958 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.138193

Running PBIRL with 3 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.5790
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.08225971 -0.99580359  0.04010684]
True reward weights: [ 0.70160261 -0.28661835  0.6523831 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.133342

Running PBIRL with 4 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.4104
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.69054349 -0.54267781  0.47817411]
True reward weights: [-0.65711775 -0.5615449  -0.50285543]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.177201

Running PBIRL with 5 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.4046
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.13257509 -0.98468291 -0.11324049]
True reward weights: [-0.25155623 -0.85769251 -0.4484228 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.164594

Running PBIRL with 6 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3960
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.12553867 -0.99198178 -0.01456687]
True reward weights: [-0.23386647 -0.45008884  0.86181582]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.181163

Saving results to files...
Results saved successfully.

Running experiment 43/50...
Shuffled Demos: [([(0, 1), (0, 3), (1, 2), (1, 1), (4, 2), (3, 2), (3, 2), (3, 3), (4, 3), (5, 0)], [(0, 1), (0, 3), (1, 2), (1, 1), (4, 2), (3, 2), (3, 2), (3, 1), (3, 3), (4, 1)]), ([(0, 2), (0, 3), (0, 3), (1, 0), (1, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 3), (0, 3), (1, 0), (1, 3), (1, 3), (2, 2), (1, 1), (4, 0), (5, None)]), ([(0, 3), (1, 3), (2, 2), (1, 1), (4, 3), (4, 3), (5, 0)], [(0, 3), (1, 3), (2, 2), (1, 0), (1, 3), (2, 1), (5, None)]), ([(0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 0), (2, 3), (2, 3), (2, 1), (2, 0), (2, 2), (1, 2), (0, 0), (0, 3)]), ([(0, 1), (1, 1), (2, 1), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (3, 1), (3, 3), (4, 2), (3, 1), (3, 1), (3, 3), (4, 0), (1, 1)]), ([(0, 1), (3, 3), (4, 3), (5, 0)], [(0, 3), (3, 3), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.6590
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.54628497 -0.50550787  0.66785816]
True reward weights: [-0.94655708 -0.29794542  0.12352418]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.5998
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.41371057 -0.90977272  0.03401713]
True reward weights: [ 0.7509581  -0.07074143  0.65654975]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.133112

Running PBIRL with 3 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.3294
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.62679675  0.1998391   0.75312029]
True reward weights: [ 0.67528059 -0.12930824  0.72613738]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.191027

Running PBIRL with 4 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.3292
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.55465267  0.34867125  0.75550564]
True reward weights: [-0.59716382 -0.574681   -0.55958656]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.170700

Running PBIRL with 5 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.3266
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.93366793 -0.35733531 -0.02399321]
True reward weights: [-0.79299713 -0.49155654 -0.3598996 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.160393

Running PBIRL with 6 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.3272
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.84574388 -0.24989005  0.47145758]
True reward weights: [-0.73322938 -0.67539904  0.07880874]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.178279

Running experiment 44/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (1, 1), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 2), (0, 0), (0, 3), (1, 0), (1, 3), (2, 2), (1, 0), (1, 3), (2, 0)]), ([(0, 1), (3, 3), (3, 0), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 3), (3, 0), (0, 0), (0, 3), (1, 3), (2, 3), (2, 3), (2, 1), (1, 2)]), ([(0, 1), (3, 1), (3, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 1), (3, 1), (3, 3), (4, 2), (3, 3), (3, 3), (3, 2), (3, 3), (3, 3)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 0), (0, 2), (0, 0), (0, 2), (0, 3), (1, 1), (2, 2), (1, 2), (0, 1)]), ([(0, 0), (0, 0), (0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 0), (0, 3), (1, 0), (1, 1), (4, 0), (1, 3), (2, 2), (1, 0), (1, 1)]), ([(0, 1), (3, 0), (0, 0), (0, 0), (0, 3), (1, 1), (4, 3), (4, 3), (1, 1), (4, 3)], [(0, 1), (3, 0), (0, 0), (0, 0), (0, 3), (1, 0), (2, 0), (2, 2), (1, 3), (2, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.6530
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.16045726 -0.85672867  0.49017288]
True reward weights: [-0.88315502 -0.32822573  0.33511952]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.6380
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.58620582 -0.7476161  -0.31214244]
True reward weights: [-0.83916664  0.54369908  0.01380816]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126798

Running PBIRL with 3 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5484
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.36264359 -0.43846043  0.82233939]
True reward weights: [-0.94029851  0.2527923   0.227892  ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.148133

Running PBIRL with 4 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5570
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.98011186  0.03163201  0.19590853]
True reward weights: [ 0.30288576 -0.95033358  0.07159815]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.141457

Running PBIRL with 5 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5538
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.43736703 -0.84478811 -0.30829065]
True reward weights: [-0.82859905 -0.48755408 -0.2751629 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.137707

Running PBIRL with 6 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5596
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.11914419 -0.78713182  0.60516787]
True reward weights: [-0.6622557  -0.23635746  0.71102218]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.159362

Saving results to files...
Results saved successfully.

Running experiment 45/50...
Shuffled Demos: [([(0, 0), (0, 0), (0, 1), (0, 2), (0, 2), (0, 1), (3, 3), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 0), (0, 1), (0, 2), (0, 2), (0, 1), (3, 2), (3, 2), (3, 2), (3, 2)]), ([(0, 1), (3, 0), (0, 1), (0, 2), (0, 2), (3, 0), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 1), (3, 0), (0, 1), (0, 2), (0, 2), (3, 0), (3, 3), (4, 1), (4, 0), (1, 0)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 3), (1, 2), (0, 1), (3, 3), (4, 1), (4, 2), (3, 1), (3, 2), (3, 2)]), ([(0, 3), (1, 3), (1, 1), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 3), (1, 1), (0, 0), (0, 3), (1, 1), (4, 0), (3, 1), (4, 2), (3, 2)]), ([(0, 3), (3, 1), (3, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (3, 1), (3, 1), (3, 2), (3, 3), (0, 1), (0, 0), (0, 0), (0, 1), (3, 0)]), ([(0, 0), (0, 3), (1, 0), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 3), (1, 0), (1, 1), (4, 1), (4, 2), (3, 0), (0, 3), (1, 1), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 45
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.80015407 -0.55199591 -0.23463585]
True reward weights: [-0.83381491 -0.37890594  0.40147601]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6476
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.48960174 -0.82455504  0.28354739]
True reward weights: [ 0.31516609 -0.65308826 -0.68858265]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183872

Running PBIRL with 3 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6346
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.44583941 -0.88797944 -0.11278183]
True reward weights: [ 0.54196396 -0.83251945 -0.11483219]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.179214

Running PBIRL with 4 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6254
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.56306177 -0.79553024  0.22381483]
True reward weights: [ 0.22851251 -0.73675969  0.63637033]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.170633

Running PBIRL with 5 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.5870
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.13005778 -0.9824437   0.1337511 ]
True reward weights: [-0.27221821 -0.09113333  0.95791021]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.172496

Running PBIRL with 6 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.5956
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.0805198  -0.99674101 -0.00488993]
True reward weights: [-0.98986461  0.07652205  0.11963459]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.168004

Running experiment 46/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 2), (0, 1), (3, 0), (4, 3), (5, None)]), ([(0, 1), (0, 1), (3, 3), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (0, 0), (1, 3), (2, 0), (2, 3), (2, 2), (1, 1), (4, 0), (1, 1), (4, 1)]), ([(0, 1), (3, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 1), (4, 2), (3, 3), (4, 0), (1, 3), (4, 3), (1, 2), (0, 2), (0, 3)]), ([(0, 3), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (3, 2), (3, 3), (4, 0), (3, 3), (3, 2), (3, 3), (4, 0), (1, 3), (2, 3)]), ([(0, 3), (1, 3), (2, 3), (2, 3), (2, 2), (1, 2), (0, 3), (1, 1), (4, 3), (5, 0)], [(0, 3), (1, 3), (2, 3), (2, 3), (2, 2), (1, 2), (0, 3), (1, 3), (1, 0), (2, 3)]), ([(0, 3), (1, 1), (4, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 3), (1, 1), (4, 1), (3, 2), (3, 3), (4, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 46
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.76636723  0.64221239 -0.01563699]
True reward weights: [-0.82098277 -0.18440172  0.54035479]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6486
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.73192024  0.53216337  0.42555248]
True reward weights: [-0.64481729 -0.26064743 -0.7185218 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.181784

Running PBIRL with 3 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6196
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.65218998  0.13242013  0.74640012]
True reward weights: [ 0.12550461 -0.03929391  0.99131457]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.177845

Running PBIRL with 4 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6094
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.34840552 0.43164873 0.83204145]
True reward weights: [-0.76778486  0.19867757  0.6091253 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.174144

Running PBIRL with 5 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5420
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.24255146  0.42120501  0.87393085]
True reward weights: [-0.80911906 -0.10208478  0.57870981]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.183406

Running PBIRL with 6 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5246
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.31020833  0.42729728  0.84922778]
True reward weights: [ 0.19992281 -0.93773447  0.28405094]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.182291

Saving results to files...
Results saved successfully.

Running experiment 47/50...
Shuffled Demos: [([(0, 1), (3, 3), (0, 3), (1, 2), (1, 1), (2, 3), (2, 3), (2, 2), (1, 1), (4, 3)], [(0, 1), (3, 3), (0, 3), (1, 2), (1, 1), (2, 3), (2, 3), (2, 2), (1, 3), (2, 2)]), ([(0, 3), (3, 1), (3, 2), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (3, 1), (3, 2), (3, 3), (4, 2), (3, 2), (3, 0), (0, 1), (3, 2), (3, 1)]), ([(0, 2), (0, 1), (3, 0), (0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 0), (0, 2), (0, 0), (0, 2), (0, 2), (0, 0), (0, 3), (1, 2)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 2), (0, 1), (3, 2), (3, 3), (4, 0), (1, 0), (1, 1), (4, 2), (3, 3), (4, 3)]), ([(0, 1), (0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 2), (0, 2), (3, 0), (0, 2), (0, 1), (3, 1), (3, 2), (3, 2), (3, 0)]), ([(0, 2), (0, 1), (1, 3), (2, 2), (1, 1), (4, 1), (4, 3), (1, 1), (4, 3), (5, 0)], [(0, 2), (0, 1), (1, 3), (2, 2), (1, 1), (4, 1), (4, 2), (3, 3), (4, 1), (4, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 47
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.27282123  0.96176893 -0.02385573]
True reward weights: [-0.73857451 -0.23755048  0.63093381]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6612
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.5662397  -0.58713144  0.57848879]
True reward weights: [ 0.58200961 -0.12410951  0.80365518]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182864

Running PBIRL with 3 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5948
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.87428255 -0.44403322  0.19612376]
True reward weights: [ 0.56997655 -0.16501447  0.80492047]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.182177

Running PBIRL with 4 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6004
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.46005976 -0.76339928 -0.45339449]
True reward weights: [0.11907816 0.51259888 0.85033098]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.172655

Running PBIRL with 5 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5922
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.35687145 -0.8298758  -0.42889268]
True reward weights: [-0.88753507  0.2328682   0.39755993]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.167034

Running PBIRL with 6 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.5296
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.44833247 -0.73785752  0.50454363]
True reward weights: [-0.32134576 -0.02608982  0.94660246]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.185382

Running experiment 48/50...
Shuffled Demos: [([(0, 1), (3, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 1), (3, 1), (3, 1), (3, 3), (4, 1), (4, 3), (5, None)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, 0)], [(0, 0), (0, 1), (3, 3), (3, 2), (3, 3), (4, 3), (5, None)]), ([(0, 3), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 0), (1, 1), (0, 3), (1, 1), (4, 1), (4, 0), (1, 1), (4, 1), (4, 2)]), ([(0, 1), (3, 2), (0, 3), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 2), (0, 3), (0, 0), (0, 2), (0, 3), (0, 1), (3, 0), (4, 1), (4, 2)]), ([(0, 1), (3, 3), (0, 1), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 3), (0, 1), (1, 1), (4, 2), (3, 1), (3, 2), (3, 0), (0, 0), (0, 0)]), ([(0, 1), (3, 1), (3, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 1), (3, 1), (4, 1), (4, 1), (4, 0), (1, 2), (4, 2), (3, 2), (3, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.6576
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.37920843 -0.68989899  0.61663632]
True reward weights: [-0.95541883 -0.18852986  0.22722531]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5164
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.2061826  -0.91839666 -0.33769262]
True reward weights: [ 0.0122947  -0.57379443  0.81890708]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.146373

Running PBIRL with 3 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.4630
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.28945013 -0.44025157  0.84993951]
True reward weights: [ 0.42355242 -0.8876252  -0.18090014]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.156097

Running PBIRL with 4 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.4676
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.1203604  -0.64782581  0.75222011]
True reward weights: [-0.61961045 -0.73447873  0.27681022]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.147125

Running PBIRL with 5 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.4526
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.71782258 0.35854998 0.59680202]
True reward weights: [ 0.03931041 -0.98224238 -0.18345192]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.142138

Running PBIRL with 6 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.4660
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.64178784 -0.70346357  0.30536431]
True reward weights: [-0.0552594  -0.99730162 -0.04833086]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.138706

Saving results to files...
Results saved successfully.

Running experiment 49/50...
Shuffled Demos: [([(0, 0), (0, 1), (1, 3), (2, 0), (2, 2), (1, 1), (2, 1), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (1, 3), (2, 0), (2, 2), (1, 2), (0, 3), (1, 1), (2, 2), (1, 1)]), ([(0, 3), (1, 2), (0, 1), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 2), (0, 0), (0, 0), (0, 3), (1, 1), (0, 1), (0, 1), (0, 0), (0, 0)]), ([(0, 2), (0, 0), (0, 0), (0, 3), (1, 1), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 2), (0, 0), (0, 0), (0, 3), (1, 2), (0, 0), (0, 1), (3, 2), (3, 2), (3, 1)]), ([(0, 0), (0, 1), (0, 1), (1, 1), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (4, 2), (3, 3), (4, 2), (3, 3)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 3), (4, 0), (1, 3), (2, 0), (2, 0), (2, 1), (5, None)]), ([(0, 1), (1, 1), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 2), (0, 0), (0, 0), (0, 3), (1, 0), (1, 2), (0, 3), (1, 1), (4, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.6426
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.60848711 -0.35413946  0.71016102]
True reward weights: [-0.98484463 -0.1489005   0.08893646]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.6086
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.39016765  0.18988862  0.90095034]
True reward weights: [-0.96593637 -0.25813647  0.01823462]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.132793

Running PBIRL with 3 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5702
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.00915385 -0.04836415  0.99878782]
True reward weights: [-0.30708512 -0.92384626  0.22848811]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.136772

Running PBIRL with 4 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5922
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.76838383 -0.34378048  0.53981595]
True reward weights: [-0.23663501 -0.89485934  0.37845823]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.133823

Running PBIRL with 5 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5788
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.63532363 -0.72158183 -0.27510644]
True reward weights: [-0.33060862  0.32035769  0.88773244]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.131507

Running PBIRL with 6 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.5742
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.8405556   0.11920422  0.52844739]
True reward weights: [-0.4544413  -0.75437767 -0.47370607]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.130172

Running experiment 50/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (4, 3), (5, 0)], [(0, 2), (0, 3), (1, 3), (4, 3), (5, None)]), ([(0, 1), (1, 0), (1, 1), (4, 3), (1, 1), (4, 3), (5, 0), (5, 0)], [(0, 1), (1, 0), (1, 1), (4, 2), (3, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 1), (3, 0), (0, 2), (0, 0), (0, 1), (3, 1), (3, 3), (4, 0), (1, 1), (4, 2)]), ([(0, 0), (0, 1), (3, 1), (3, 0), (0, 0), (0, 1), (3, 3), (4, 3), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 1), (3, 0), (0, 0), (0, 1), (3, 1), (3, 3), (4, 2), (4, 3)]), ([(0, 3), (1, 3), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 3), (1, 3), (2, 0), (2, 1), (1, 0), (1, 2), (0, 0), (0, 3), (3, 3), (4, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 0), (1, 1), (2, 1), (5, 0), (5, 0), (5, 0), (5, 0)], [(0, 0), (0, 1), (3, 3), (4, 0), (1, 2), (0, 3), (3, 0), (0, 2), (0, 0), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.6688
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.38864041  0.29868049  0.87163559]
True reward weights: [-0.47840873 -0.41855607  0.77196885]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.1874
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.66276899 -0.25490763  0.70410181]
True reward weights: [-0.91473594 -0.21950426 -0.3392286 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.190755

Running PBIRL with 3 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.1822
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.44846551 -0.00505236  0.89378586]
True reward weights: [-0.33821409 -0.35210306  0.87271683]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.165280

Running PBIRL with 4 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.1870
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.57498528 -0.14302406  0.80556567]
True reward weights: [0.1068643  0.42911763 0.89690472]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.154687

Running PBIRL with 5 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.1940
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.7733443  -0.4232564   0.47200912]
True reward weights: [-0.75420197 -0.29979197  0.58421244]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.146819

Running PBIRL with 6 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.1896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.82888014 -0.47501238  0.29550117]
True reward weights: [-0.81528438 -0.53398801 -0.22398256]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.142705

Saving results to files...
Results saved successfully.
Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [([(0, 0), (0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 0), (0, 3), (1, 3), (2, 3), (2, 0), (2, 1), (5, None)]), ([(0, 3), (1, 1), (2, 1), (5, None)], [(0, 3), (1, 0), (1, 2), (0, 2), (3, 3), (4, 2), (4, 3), (5, None)]), ([(0, 3), (1, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 2), (1, 0), (1, 0), (1, 1), (4, 3), (5, None)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (1, 1), (2, 1), (5, None)], [(0, 2), (0, 3), (1, 2), (0, 2), (0, 1), (1, 3), (2, 0), (2, 0), (2, 0), (2, 0)]), ([(0, 1), (3, 1), (3, 2), (3, 3), (0, 1), (3, 2), (3, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 3), (0, 1), (3, 2), (3, 2), (3, 1), (3, 0), (0, 0)]), ([(0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 2), (0, 1), (3, 3), (3, 1), (3, 3), (4, 2), (3, 3), (4, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6630
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.45935461 -0.8833812  -0.09290317]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6390
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.56031704 -0.42497358 -0.71094463]
True reward weights: [ 0.42300377 -0.58478509  0.69216632]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126292

Running PBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4390
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.39797195 -0.70163168  0.59104257]
True reward weights: [-0.28568177 -0.31633124 -0.90461067]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.172272

Running PBIRL with 4 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4088
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.96832309 -0.23469614 -0.08525321]
True reward weights: [-0.30867524 -0.49566019 -0.81181314]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.166372

Running PBIRL with 5 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.2996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.87857111 -0.10338189  0.46628853]
True reward weights: [-0.90713315 -0.16442602 -0.38739326]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.203818

Running PBIRL with 6 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.2738
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.4789844  -0.10354544  0.87169506]
True reward weights: [-0.87012693 -0.01816326  0.49249286]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.196532

Running experiment 2/50...
Shuffled Demos: [([(0, 0), (0, 2), (3, 3), (4, 1), (3, 3), (3, 0), (0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (3, 3), (4, 1), (3, 3), (3, 0), (0, 2), (0, 0), (0, 1), (3, 1)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 3), (4, 0), (1, 3), (2, 3), (2, 0), (2, 0), (2, 2), (1, 1)]), ([(0, 1), (3, 1), (3, 1), (3, 3), (3, 2), (3, 3), (4, 0), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 3), (3, 2), (3, 3), (4, 0), (3, 3), (4, 1), (4, 3)]), ([(0, 3), (1, 2), (1, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (1, 3), (4, 2), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 2), (3, 1), (3, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 1), (3, 2), (0, 3), (1, 1), (4, 0), (1, 0), (1, 0), (1, 0)]), ([(0, 2), (0, 3), (1, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 2), (0, 1), (3, 0), (0, 1), (3, 3), (4, 2), (4, 1), (4, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6524
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.17746266 -0.87292649  0.45442969]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4812
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.64058138 -0.22452998  0.73433084]
True reward weights: [0.69689878 0.67165245 0.2514261 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.152427

Running PBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4782
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.76787331 -0.48288569  0.42094179]
True reward weights: [-0.88577522  0.38907998  0.25301983]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.144513

Running PBIRL with 4 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.4272
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.2696626  -0.84398614  0.46364802]
True reward weights: [-0.76293191  0.36273905  0.53512175]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.155389

Running PBIRL with 5 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.3046
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.73893502 -0.59648762  0.31333298]
True reward weights: [-0.36043145 -0.34423058  0.86694549]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.202704

Running PBIRL with 6 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.6772348  -0.57782201  0.45549396]
True reward weights: [-0.11930618 -0.66653464  0.73586521]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.184712

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 0), (1, 1), (2, 1), (5, None)], [(0, 1), (3, 3), (4, 0), (1, 1), (2, 3), (2, 0), (2, 3), (5, None)]), ([(0, 2), (0, 3), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 2), (3, 0), (4, 3), (1, 1), (4, 2), (3, 1), (3, 0)]), ([(0, 0), (0, 2), (0, 1), (3, 0), (4, 2), (3, 2), (3, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 1), (3, 0), (4, 2), (3, 2), (3, 1), (3, 3), (3, 2), (3, 1)]), ([(0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 3), (1, 0), (1, 3), (2, 2), (5, None)]), ([(0, 3), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 0), (0, 3), (3, 0), (0, 2), (0, 0), (0, 3), (1, 0), (1, 3), (2, 2)]), ([(0, 3), (1, 0), (1, 3), (2, 2), (1, 0), (1, 2), (4, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 3), (2, 2), (1, 0), (1, 2), (4, 1), (4, 1), (4, 2), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6618
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.13812773 -0.6337172  -0.76113287]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6354
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.2581765  -0.96565346  0.02929647]
True reward weights: [-0.39783121 -0.72926035  0.55669531]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.130547

Running PBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.4398
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.32175939 -0.78110068  0.53511926]
True reward weights: [ 0.03429776 -0.99835811  0.04587757]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.171447

Running PBIRL with 4 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2408
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.80148157 -0.24558887  0.54526453]
True reward weights: [-0.3310696  -0.53376314  0.77813227]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.224492

Running PBIRL with 5 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2304
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.65131813 -0.38512498  0.65380688]
True reward weights: [-0.52831886 -0.52255256  0.66919205]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.195766

Running PBIRL with 6 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2322
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.66718993 -0.21419529  0.71342692]
True reward weights: [-0.57876863 -0.10096561  0.80921741]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.181445

Running experiment 4/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 0), (0, 1), (3, 1), (3, 2), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 0), (0, 1), (3, 1), (3, 2), (3, 0), (0, 2), (0, 0), (0, 3)]), ([(0, 0), (0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 1), (3, 3), (4, 1), (4, 3), (5, None)]), ([(0, 3), (1, 1), (0, 3), (1, 0), (0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (0, 3), (1, 0), (0, 2), (0, 0), (0, 2), (0, 0), (0, 0), (0, 2)]), ([(0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 0), (1, 3), (2, 0), (2, 0), (2, 3), (2, 2), (5, None)]), ([(0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (2, 0), (1, 0), (1, 1), (4, 3), (5, None)]), ([(0, 3), (1, 3), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 3), (1, 3), (2, 0), (2, 2), (1, 3), (2, 2), (1, 1), (4, 3), (4, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6652
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.86495417 0.24064599 0.4403905 ]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123672

Running PBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4404
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.61761023 -0.71322967  0.33145291]
True reward weights: [-0.10711229 -0.6526612   0.75004021]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.159080

Running PBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4442
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.80464423 -0.43932076  0.39943077]
True reward weights: [-0.35066833 -0.7384516   0.57595221]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.146900

Running PBIRL with 4 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4116
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.30068515 -0.70196281  0.64562888]
True reward weights: [-0.89765381 -0.27920697  0.34097082]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.154471

Running PBIRL with 5 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.1128
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.8998368  -0.29249787  0.32363364]
True reward weights: [-0.2347542  -0.08383731  0.96843264]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.271480

Running PBIRL with 6 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.1122
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.90046807 -0.29674995  0.31795711]
True reward weights: [-0.87375976 -0.07527426  0.48049732]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.229386

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 0), (0, 2), (0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 3), (1, 1), (4, 2), (3, 2), (0, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 2), (3, 2), (0, 3), (0, 1), (3, 0), (0, 3), (1, 3), (1, 3)]), ([(0, 1), (3, 2), (3, 2), (3, 0), (4, 1), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 2), (3, 0), (4, 1), (4, 1), (4, 0), (1, 1), (4, 1), (3, 3)]), ([(0, 2), (0, 3), (1, 3), (2, 3), (2, 2), (1, 0), (2, 0), (2, 2), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 3), (2, 3), (2, 2), (1, 0), (2, 0), (2, 2), (1, 3), (2, 2)]), ([(0, 2), (0, 1), (3, 2), (3, 2), (3, 3), (4, 1), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 2), (3, 2), (3, 3), (4, 1), (4, 0), (1, 3), (2, 3), (2, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6580
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.15415626  0.53056553 -0.83350829]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6266
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.38232143  0.22477814 -0.8962729 ]
True reward weights: [0.02409491 0.72742886 0.68575993]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.130129

Running PBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.5190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.83460422  0.38431026  0.39464088]
True reward weights: [-0.54884404  0.8358654  -0.00996284]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154898

Running PBIRL with 4 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2804
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.65901234  0.03593452  0.75127321]
True reward weights: [-0.82286379  0.28920608 -0.48913702]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.213950

Running PBIRL with 5 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2262
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.62603374  0.29333125  0.72252234]
True reward weights: [-0.879693   -0.33161622  0.34083853]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.217950

Running PBIRL with 6 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2260
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.65681093 -0.0452084   0.75269888]
True reward weights: [-0.35732439  0.11456951  0.9269267 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.198256

Running experiment 6/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (3, 2), (3, 0), (3, 1), (3, 2), (0, 0), (0, 2), (0, 3), (1, 1)]), ([], [(0, 1), (1, 1), (4, 3), (5, None)]), ([], [(0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 1), (3, 1), (3, 3), (4, 1), (4, 1), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 3), (4, 1), (4, 1), (4, 0), (1, 2), (0, 3), (1, 2)]), ([(0, 1), (0, 3), (1, 1), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 1), (0, 3), (1, 0), (1, 1), (4, 0), (5, None)]), ([(0, 0), (0, 1), (0, 1), (3, 0), (0, 0), (0, 0), (0, 3), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (0, 1), (3, 0), (0, 0), (0, 0), (0, 3), (3, 1), (3, 3), (4, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6628
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.6492265  -0.60606513 -0.45955413]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.27287676 -0.71922901 -0.6389428 ]
True reward weights: [ 0.43449951 -0.58993413 -0.68057909]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.137153

Running PBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.5616
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.78260159 -0.41837537 -0.46097375]
True reward weights: [-0.78314732 -0.33531615  0.5236825 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.136664

Running PBIRL with 4 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4618
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.29035097 -0.90511555  0.31058357]
True reward weights: [ 0.16108567 -0.55008941 -0.81942238]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.162058

Running PBIRL with 5 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.0212
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.93880568  0.08043944 -0.33492295]
True reward weights: [-0.94796202 -0.28207789  0.14764846]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.323617

Running PBIRL with 6 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.0028
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.9998176  -0.01877941  0.00347846]
True reward weights: [-0.82020171  0.04017061 -0.57066232]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.550520

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Shuffled Demos: [([(0, 3), (1, 1), (0, 1), (3, 3)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 2), (0, 3), (1, 1), (0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 3), (2, 2), (1, 1), (2, 0), (1, 0), (0, 3), (3, 1), (3, 2)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (4, 0), (5, None)]), ([(0, 1), (3, 0), (0, 1), (3, 2), (3, 2), (3, 2), (3, 2), (3, 0), (0, 1), (0, 1)], [(0, 1), (3, 0), (0, 1), (3, 2), (3, 2), (3, 2), (3, 2), (3, 0), (0, 3), (1, 1)]), ([(0, 1), (1, 2), (0, 2), (0, 0), (0, 2), (0, 3), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (1, 2), (0, 2), (0, 0), (0, 2), (0, 3), (3, 1), (3, 2), (3, 2), (3, 0)]), ([(0, 1), (3, 0), (0, 0), (0, 2), (3, 3), (4, 1), (4, 1), (4, 2), (3, 3), (0, 1)], [(0, 1), (3, 0), (0, 0), (0, 2), (3, 3), (4, 1), (4, 1), (4, 2), (3, 1), (3, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6484
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.06117815  0.95363481 -0.29468268]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3016
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.84479229  0.38433848 -0.37230353]
True reward weights: [0.72938235 0.55015876 0.40660388]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.177240

Running PBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.2864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.96734678  0.17966797 -0.17877256]
True reward weights: [-0.7252174   0.01340365 -0.68838947]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.167228

Running PBIRL with 4 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.2964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.64879532  0.74477017  0.15614745]
True reward weights: [-0.27652104  0.75015018  0.6006753 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.154559

Running PBIRL with 5 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.0148
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.19467823  0.69355676  0.69359888]
True reward weights: [-0.61640904  0.75805145  0.21306779]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.434732

Running PBIRL with 6 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.0190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.99889601  0.03333443  0.03309945]
True reward weights: [-0.86308072  0.36453034  0.34958447]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.459215

Running experiment 8/50...
Shuffled Demos: [([(0, 2), (0, 3), (1, 1), (4, 0), (1, 2), (0, 3), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 0), (1, 2), (0, 3), (1, 3), (4, 3), (5, None)]), ([(0, 1), (3, 1), (3, 2), (3, 0), (0, 3), (1, 3), (2, 3), (2, 1), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 0), (0, 3), (1, 3), (2, 3), (2, 3), (2, 2), (1, 2)]), ([(0, 2), (0, 2), (0, 1), (1, 2), (0, 1), (3, 3), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 1), (1, 2), (0, 1), (3, 3), (3, 0), (0, 2), (0, 1), (3, 0)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (2, 3), (2, 1), (2, 2), (2, 1), (5, None)]), ([(0, 0), (0, 1), (3, 2), (3, 3), (4, 0), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 2), (3, 3), (4, 0), (3, 2), (3, 2), (0, 2), (0, 1), (3, 1)]), ([(0, 1), (3, 1), (3, 3), (3, 3), (0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 3), (3, 3), (0, 3), (1, 0), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6524
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.73354718  0.48708506  0.47397962]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123677

Running PBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.3048
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.59454795  0.38482472  0.70599056]
True reward weights: [ 0.70283594  0.61863565 -0.35115749]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.177164

Running PBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.2900
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.6064375   0.67660396  0.41766092]
True reward weights: [-0.05429432  0.34038097  0.93871877]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.171519

Running PBIRL with 4 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.0134
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.67343588 -0.00473313  0.73923049]
True reward weights: [-0.17803546  0.57575658  0.79800234]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.387419

Running PBIRL with 5 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.0170
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.78264226 -0.0044808   0.62245564]
True reward weights: [ 0.34111715 -0.01135168  0.93995225]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.338637

Running PBIRL with 6 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.0142
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.96328522 -0.00458317  0.26844101]
True reward weights: [ 0.48764422 -0.01562738  0.87290257]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.338753

Saving results to files...
Results saved successfully.

Running experiment 9/50...
Shuffled Demos: [([(0, 0), (0, 1), (0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 2), (1, 2), (0, 0), (0, 3), (1, 1), (4, 0), (1, 3), (4, 3)]), ([(0, 1), (3, 2), (3, 0), (0, 0), (0, 2), (0, 1), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 0), (0, 0), (0, 2), (0, 1), (0, 0), (0, 0), (0, 1), (3, 2)]), ([(0, 1), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 2), (0, 1), (3, 2), (3, 3), (4, 0), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 2), (3, 0), (0, 1), (3, 3), (4, 2), (3, 3), (4, 0)]), ([(0, 3), (1, 2), (0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 2), (0, 2), (0, 1), (3, 3), (4, 2), (3, 3), (4, 3), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 2), (3, 2), (3, 3), (4, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6486
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.93811439 -0.33066301  0.10297261]
True reward weights: [-0.0505111  -0.01531811  0.99860602]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4820
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.53704207 -0.07827381  0.83991608]
True reward weights: [-0.99303963 -0.03551459 -0.11229876]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.150084

Running PBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3038
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.68701635 -0.20664753  0.6966386 ]
True reward weights: [-0.67716249  0.67213205  0.29948199]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.191962

Running PBIRL with 4 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.3050
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.01403138 -0.49555739  0.86846186]
True reward weights: [-0.74346131  0.00438934  0.66876454]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.171498

Running PBIRL with 5 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.2466
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.45525911 -0.39639754  0.79725036]
True reward weights: [-0.03977491 -0.62711647  0.77790931]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.181071

Running PBIRL with 6 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.2466
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.65152463 -0.65267125  0.38669872]
True reward weights: [-0.30368118 -0.941413    0.14669456]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.169543

Running experiment 10/50...
Shuffled Demos: [([(0, 2), (0, 1), (0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 1), (0, 0), (0, 3), (1, 2), (0, 2), (0, 1), (3, 1), (3, 0), (0, 0)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3)], [(0, 1), (3, 1), (3, 3), (4, 3), (5, None)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 3), (4, 1), (4, 2), (3, 2), (3, 2), (3, 1), (3, 2), (0, 1)]), ([(0, 0), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 2), (3, 0), (0, 3), (1, 1), (4, 2), (3, 1), (3, 2), (3, 1), (3, 1)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 2), (0, 1), (3, 1), (3, 3), (0, 3), (1, 1), (2, 0), (2, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6494
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.96555686 -0.25865577  0.02823379]
True reward weights: [-0.76518493 -0.64328377  0.02603879]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.1674
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.25317708 -0.423357   -0.86986793]
True reward weights: [ 0.62865261 -0.44035289  0.6410033 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.194013

Running PBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.1512
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.63031547 -0.354145   -0.69085724]
True reward weights: [ 0.51912527 -0.28800136 -0.80471372]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.191519

Running PBIRL with 4 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.1662
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.06275226 -0.51695909 -0.85370689]
True reward weights: [ 0.81760342 -0.31616741 -0.48120974]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.179505

Running PBIRL with 5 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.1670
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.64393665 -0.33398449 -0.68833128]
True reward weights: [-0.6550378  -0.48226426 -0.58167574]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.173733

Running PBIRL with 6 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.1386
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.08315449 -0.41191703 -0.90741925]
True reward weights: [-0.67230759 -0.43190398 -0.60121665]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.197242

Saving results to files...
Results saved successfully.

Running experiment 11/50...
Shuffled Demos: [([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 2), (4, 1), (4, 3), (1, 0), (1, 0), (1, 3), (2, 1), (5, None)]), ([(0, 3), (1, 2), (0, 0), (0, 1), (1, 0), (1, 1), (4, 3), (4, 0), (1, 1), (4, 3)], [(0, 3), (1, 2), (0, 0), (0, 1), (1, 0), (1, 1), (4, 3), (4, 0), (1, 0), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (3, 0), (0, 1), (3, 1), (3, 2), (3, 0), (0, 1), (3, 2)]), ([(0, 2), (0, 3), (1, 0), (0, 3), (1, 2), (0, 0), (0, 1), (3, 3), (4, 3), (1, 1)], [(0, 2), (0, 3), (1, 0), (0, 3), (1, 2), (0, 0), (0, 2), (0, 3), (1, 0), (1, 0)]), ([(0, 3), (1, 1), (0, 1), (3, 1), (3, 3), (3, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (0, 1), (3, 1), (3, 3), (3, 3), (0, 2), (0, 1), (0, 1), (3, 2)]), ([(0, 2), (0, 2), (0, 0), (0, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 0), (0, 3), (0, 0), (0, 1), (3, 2), (3, 2), (0, 0), (1, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6684
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.98195062  0.16146308 -0.098502  ]
True reward weights: [-0.87829137 -0.24439244  0.41094598]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.5362
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.36809429  0.83227702  0.41451364]
True reward weights: [-0.23869768 -0.33391167 -0.9118807 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.146078

Running PBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.2132
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.68722322 -0.34647045 -0.63850018]
True reward weights: [-0.9714895  -0.1062736  -0.21192942]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.207576

Running PBIRL with 4 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.2160
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.73956542 -0.36888667 -0.562997  ]
True reward weights: [-0.4101435   0.07158805  0.90920705]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.181906

Running PBIRL with 5 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.2140
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.85829288 -0.50332084  0.10000726]
True reward weights: [-0.66418678 -0.65576016 -0.35893529]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.183074

Running PBIRL with 6 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.2012
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.62975597 -0.08804524  0.77178718]
True reward weights: [-0.72189525 -0.65973461 -0.208848  ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.171849

Running experiment 12/50...
Shuffled Demos: [([(0, 3), (1, 0), (1, 3), (2, 3), (2, 2), (1, 2), (0, 1), (3, 3), (0, 1), (0, 1)], [(0, 3), (1, 0), (1, 3), (2, 3), (2, 2), (1, 2), (0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 2), (3, 1), (3, 2), (3, 3), (4, 2), (3, 2), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 1), (3, 2), (3, 3), (4, 2), (3, 2), (3, 1), (3, 2), (3, 3)]), ([(0, 3), (1, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (1, 2), (0, 3), (1, 3), (2, 3), (2, 0), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 3), (0, 0), (0, 2), (0, 0), (0, 0), (0, 2), (0, 1), (3, 2)]), ([(0, 2), (0, 0), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 0), (0, 2), (0, 0), (0, 0), (0, 1), (0, 1), (3, 3), (4, 2)]), ([(0, 3), (1, 0), (1, 1), (4, 2), (1, 1), (4, 1), (4, 0), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 0), (1, 1), (4, 2), (1, 1), (4, 1), (4, 0), (1, 3), (2, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.39462676 -0.91053214  0.12329208]
True reward weights: [-0.8497926  -0.50850291  0.13884283]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6616
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.74769112 0.58504653 0.31413143]
True reward weights: [0.07235026 0.49319222 0.8669065 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.183567

Running PBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4464
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.54195021 -0.68753966  0.4833003 ]
True reward weights: [-0.53284063  0.7854463   0.31488882]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.214850

Running PBIRL with 4 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4396
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.29059273 -0.72595969  0.62332848]
True reward weights: [-0.38214418 -0.16504364  0.90924497]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.199676

Running PBIRL with 5 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4314
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.63401146 -0.5122191   0.57936263]
True reward weights: [-0.40760392 -0.87159477  0.27236301]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.187342

Running PBIRL with 6 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4432
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.32037754 -0.87245212  0.36903325]
True reward weights: [ 0.12367565 -0.21784997  0.96811452]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.235256

Saving results to files...
Results saved successfully.

Running experiment 13/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 2), (0, 3), (1, 0), (1, 2), (1, 1), (0, 2), (0, 0), (0, 3)]), ([(0, 1), (3, 3), (4, 0), (1, 3), (2, 1), (5, None)], [(0, 1), (3, 3), (4, 0), (1, 3), (2, 0), (2, 0), (2, 2), (1, 1), (4, 0), (1, 1)]), ([(0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 0), (0, 1), (1, 1), (4, 0), (1, 2), (0, 1), (3, 2), (3, 0)]), ([(0, 2), (0, 0), (0, 2), (0, 0), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 2), (0, 0), (0, 0), (0, 0), (0, 1), (3, 0), (4, 3), (1, 3)]), ([(0, 0), (0, 0), (0, 3), (1, 3), (1, 3), (1, 2), (1, 2), (0, 1), (3, 3), (4, 3)], [(0, 0), (0, 0), (0, 3), (1, 3), (1, 3), (1, 2), (1, 2), (0, 2), (0, 0), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6550
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.12794893 -0.80583394 -0.57815287]
True reward weights: [-0.34477193 -0.32328333  0.88126058]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5818
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.34674802 -0.61384846  0.70919383]
True reward weights: [-0.97922065 -0.19819655 -0.0429541 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.137062

Running PBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5782
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.43965265 -0.88184509  0.17045462]
True reward weights: [-0.7311345  -0.48704296 -0.47773581]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.133538

Running PBIRL with 4 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5682
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.11963354 -0.92965736  0.34846091]
True reward weights: [-0.7290727  -0.61726115  0.29570538]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.131456

Running PBIRL with 5 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5348
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.3308794  -0.87713585 -0.34806828]
True reward weights: [ 0.66522259 -0.67735735 -0.31411133]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.142478

Running PBIRL with 6 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.5298
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.40760201 -0.90846556 -0.09247122]
True reward weights: [ 0.42974355 -0.68969726  0.58278484]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.163944

Running experiment 14/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 3), (1, 1), (2, 1), (5, None)], [(0, 1), (3, 0), (0, 3), (1, 2), (0, 2), (0, 3), (0, 0), (0, 0), (0, 0), (0, 0)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 2), (1, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 3), (4, 1), (4, 1), (4, 1), (4, 2), (3, 0), (0, 2), (0, 3)]), ([(0, 1), (1, 1), (4, 3), (1, 1), (4, 3)], [(0, 2), (0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 2), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 1), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 3), (1, 1), (4, 1), (4, 2), (4, 1), (4, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 1), (4, 2), (4, 1), (4, 1), (4, 2), (3, 3), (4, 0), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.6644
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.90002555 -0.08010501  0.42841241]
True reward weights: [-0.43088031 -0.17122291  0.88601629]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5286
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.06465802 -0.48052841  0.87459236]
True reward weights: [-0.64447247 -0.15827919  0.74806613]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148243

Running PBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.5042
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.93795114 -0.33530074  0.08843688]
True reward weights: [-0.37958008 -0.79716186  0.46952309]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.139900

Running PBIRL with 4 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.3032
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.1931619  -0.97698041 -0.09054149]
True reward weights: [-0.28650709 -0.67537687 -0.6795438 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.197319

Running PBIRL with 5 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.3108
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.1119265  -0.92467538  0.36393392]
True reward weights: [ 0.1618334  -0.92049047  0.35567857]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.178693

Running PBIRL with 6 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.2954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.13188973 -0.98984619 -0.05300579]
True reward weights: [-0.45333632 -0.88436557 -0.11128215]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.171988

Saving results to files...
Results saved successfully.

Running experiment 15/50...
Shuffled Demos: [([(0, 3), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 3), (2, 2), (1, 1), (4, 2), (3, 2), (3, 0), (0, 0), (1, 3), (1, 1)]), ([(0, 2), (0, 2), (0, 3), (1, 0), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 3), (1, 0), (1, 2), (0, 0), (1, 0), (1, 2), (0, 1), (1, 3)]), ([(0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 0), (0, 3), (0, 1), (3, 2), (3, 0), (0, 0), (0, 3), (1, 3)]), ([(0, 2), (0, 1), (1, 1), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 2), (0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 0), (0, 2), (0, 3), (1, 3), (2, 2), (1, 0), (1, 0), (1, 2), (0, 1), (3, 3)], [(0, 0), (0, 2), (0, 3), (1, 3), (2, 2), (1, 0), (1, 0), (1, 2), (0, 3), (1, 0)]), ([(0, 0), (0, 2), (0, 3), (1, 3), (2, 0), (2, 1), (5, None)], [(0, 0), (0, 2), (0, 3), (1, 3), (2, 0), (2, 3), (2, 3), (2, 2), (1, 0), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.6606
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.1711544  -0.5585588   0.81161459]
True reward weights: [-0.75433776 -0.02805593  0.65588674]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5854
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.71523522 -0.1580783  -0.6807715 ]
True reward weights: [-0.02334657 -0.9913508  -0.12914534]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.135828

Running PBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.5472
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.9991021  0.0233848 -0.0353291]
True reward weights: [ 0.04220066 -0.9855575   0.16399854]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.141516

Running PBIRL with 4 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.3552
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.07843051 -0.50075679  0.86202743]
True reward weights: [-0.54681828 -0.07893213  0.83352234]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.191628

Running PBIRL with 5 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.0186
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.6869841  -0.68661237 -0.23794181]
True reward weights: [0.26488866 0.00165428 0.96427759]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.425178

Running PBIRL with 6 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.0188
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.6542955  -0.65446949 -0.37890247]
True reward weights: [-0.54185411 -0.53520072 -0.64803882]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.370408

Running experiment 16/50...
Shuffled Demos: [([(0, 0), (0, 1), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 0), (0, 0), (0, 2), (0, 3), (1, 1), (4, 2), (3, 2), (3, 3)]), ([(0, 0), (0, 2), (0, 2), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 2), (0, 3), (1, 3), (2, 0), (2, 1), (5, None)]), ([(0, 2), (0, 1), (3, 0), (3, 1), (3, 0), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 0), (3, 1), (3, 0), (0, 3), (1, 0), (2, 2), (1, 3), (1, 0)]), ([(0, 0), (0, 0), (0, 2), (0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 2), (0, 2), (0, 2), (0, 3), (3, 2), (3, 1), (3, 2), (3, 1)]), ([(0, 1), (3, 2), (3, 2), (3, 2), (3, 0), (4, 2), (3, 3), (4, 0), (1, 1), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 2), (3, 2), (3, 0), (4, 2), (3, 3), (4, 0), (1, 2), (0, 3)]), ([(0, 2), (0, 3), (1, 3), (2, 0), (2, 1), (5, None)], [(0, 2), (0, 3), (1, 3), (2, 0), (2, 0), (2, 0), (2, 3), (2, 3), (2, 2), (1, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.6630
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.31857317 -0.86089429  0.3967016 ]
True reward weights: [-0.92032338 -0.11683579  0.37330186]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.6160
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.40899192 -0.68048446  0.60800207]
True reward weights: [-0.48665056 -0.37452311  0.78924247]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.130209

Running PBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4354
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.08631933 -0.92682747  0.365431  ]
True reward weights: [ 0.84942413 -0.46413687 -0.25110879]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.170152

Running PBIRL with 4 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.4354
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.67654679 -0.72420808 -0.13344326]
True reward weights: [-0.34164463 -0.4834648   0.8059409 ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.160425

Running PBIRL with 5 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.3528
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.59133823 -0.53405877  0.60423532]
True reward weights: [-0.18878255 -0.98162071  0.02796309]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.185182

Running PBIRL with 6 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.3534
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.61861982 -0.70296862  0.35091969]
True reward weights: [ 0.08306756 -0.67804205  0.73031415]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.172840

Saving results to files...
Results saved successfully.

Running experiment 17/50...
Shuffled Demos: [([], [(0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 0), (0, 2), (0, 2), (0, 2), (0, 0), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 2), (0, 2), (0, 0), (1, 1), (4, 1), (3, 2), (3, 1), (3, 0)]), ([(0, 1), (3, 1), (3, 1), (4, 0), (1, 1), (4, 2), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (4, 0), (1, 1), (4, 2), (3, 0), (0, 1), (3, 1), (3, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 2), (0, 3), (1, 0), (1, 3), (2, 0), (1, 3), (1, 1), (4, 2), (3, 0)]), ([(0, 2), (0, 3), (0, 3), (3, 2), (3, 1), (4, 1), (4, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (0, 3), (3, 2), (3, 1), (4, 1), (4, 3), (4, 1), (4, 3), (5, None)]), ([(0, 2), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6706
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.11514938 -0.99119693 -0.06533963]
True reward weights: [-0.76208548 -0.28664819  0.58056743]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.5764
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.20198841 -0.51092009  0.8355605 ]
True reward weights: [-0.7580819 -0.0785217 -0.647415 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.137987

Running PBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.3718
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.39238424 -0.59748902  0.69931501]
True reward weights: [ 0.39029927 -0.65067018  0.65137915]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.182790

Running PBIRL with 4 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.3496
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.10270865 -0.86342432  0.49391231]
True reward weights: [ 0.38041847 -0.92330285  0.05285489]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.182844

Running PBIRL with 5 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.3416
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.01187965 -0.92952043  0.36857923]
True reward weights: [-0.35808886 -0.46919092  0.80723742]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.169125

Running PBIRL with 6 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.3428
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.36407302 -0.75705953  0.54250502]
True reward weights: [ 0.22179318 -0.91252675  0.34366076]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.159956

Running experiment 18/50...
Shuffled Demos: [([(0, 2), (3, 1), (3, 3), (4, 0), (1, 2), (4, 0), (1, 2), (0, 1), (1, 1), (4, 3)], [(0, 2), (3, 1), (3, 3), (4, 0), (1, 2), (4, 0), (1, 2), (0, 3), (1, 0), (1, 0)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 3), (4, 3), (4, 1), (5, None)]), ([(0, 2), (0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 0), (0, 3), (0, 3), (1, 1), (2, 0), (2, 2), (5, None)]), ([(0, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 2), (0, 0), (1, 2), (0, 3), (1, 2), (0, 0), (0, 2), (0, 1)]), ([(0, 2), (3, 2), (3, 0), (0, 1), (3, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 2), (3, 2), (3, 0), (0, 1), (3, 1), (3, 2), (3, 2), (3, 1), (3, 3), (4, 0)]), ([(0, 1), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 1), (0, 3), (1, 0), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6596
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.16813928  0.34185541  0.92458859]
True reward weights: [-0.39174849 -0.30341429  0.86860399]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123672

Running PBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.1626
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.86411091 -0.4906082  -0.1123207 ]
True reward weights: [-0.30732752  0.92821379 -0.20968775]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.191321

Running PBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.0040
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.02390548 -0.01635531 -0.99958043]
True reward weights: [-0.92942288 -0.36181428  0.07255021]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.612586

Running PBIRL with 4 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.0026
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.03442447 -0.00570124  0.99939104]
True reward weights: [-0.02390548 -0.01635531 -0.99958043]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.579524

Running PBIRL with 5 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.0034
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.0122781  0.00189389 0.99992283]
True reward weights: [-0.05667216 -0.03179285  0.99788651]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.510342

Running PBIRL with 6 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.0028
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.04096325 -0.01246739  0.99908287]
True reward weights: [0.0122781  0.00189389 0.99992283]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.585166

Saving results to files...
Results saved successfully.

Running experiment 19/50...
Shuffled Demos: [([(0, 0), (0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 2), (3, 3), (4, 2), (4, 1), (3, 1), (3, 2), (3, 1), (3, 1)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 0), (0, 2), (0, 1), (1, 2), (0, 3), (1, 2), (0, 3), (1, 0)]), ([(0, 2), (0, 0), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 3), (1, 0), (1, 0), (1, 2), (1, 1), (4, 0), (1, 1), (4, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 0), (0, 2), (0, 0), (0, 1), (3, 2), (3, 1), (3, 1), (3, 3), (4, 3)]), ([(0, 2), (0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 0), (0, 0), (0, 3), (1, 1), (4, 0), (1, 2), (0, 0), (0, 2)]), ([(0, 3), (3, 3), (4, 3), (5, None)], [(0, 3), (3, 0), (0, 1), (1, 1), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6510
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.28946815 -0.00417686  0.95717853]
True reward weights: [-0.92648321 -0.16748955  0.33701054]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.5726
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.42559374 -0.82059798  0.3814301 ]
True reward weights: [ 0.62633308 -0.38896444  0.67558385]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.138387

Running PBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4980
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.55906153 -0.44403645 -0.70020128]
True reward weights: [ 0.22228216 -0.85344765 -0.47139978]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151764

Running PBIRL with 4 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.03618698 -0.61669034  0.78637366]
True reward weights: [-0.30457107 -0.48027048 -0.82254284]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.147218

Running PBIRL with 5 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4876
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.7899143  -0.53752313 -0.29513435]
True reward weights: [-0.05357185 -0.69641162 -0.71564021]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.142062

Running PBIRL with 6 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.4752
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.5645406  -0.80792863 -0.16895336]
True reward weights: [-0.3598434  -0.92961512 -0.07955158]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.140514

Running experiment 20/50...
Shuffled Demos: [([(0, 3), (1, 0), (1, 0), (1, 3), (4, 1), (4, 0), (1, 2), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 0), (1, 0), (1, 3), (4, 1), (4, 0), (1, 2), (1, 3), (2, 0), (2, 1)]), ([(0, 1), (0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 1), (4, 1), (4, 1), (4, 3), (5, None)]), ([(0, 1), (3, 0), (4, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (4, 2), (3, 0), (0, 1), (0, 1), (3, 0), (3, 3), (3, 3), (4, 0)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 1), (3, 1), (3, 3), (4, 1), (4, 1), (5, None)]), ([(0, 0), (0, 1), (3, 3), (3, 0), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 3), (3, 0), (3, 3), (4, 1), (4, 2), (3, 3), (4, 1), (5, None)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (0, 0), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6512
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.10121209  0.45713941  0.88361738]
True reward weights: [-0.61012931 -0.3129562   0.72787406]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.3240
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.87760847 -0.37487533  0.29878397]
True reward weights: [-0.5670355  -0.53867315  0.62313882]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174645

Running PBIRL with 3 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.2332
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.89230902 -0.39694257  0.21499118]
True reward weights: [-0.76875982 -0.07033653  0.63565801]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.184135

Running PBIRL with 4 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.2254
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.78496724 -0.28546724  0.54984988]
True reward weights: [-0.6665247  -0.61947767 -0.41471948]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.180789

Running PBIRL with 5 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.2204
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.84932847 -0.51839925 -0.09951567]
True reward weights: [-0.8625207  -0.25566118  0.43668684]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.168650

Running PBIRL with 6 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.2092
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.92870027 -0.32343886  0.18139213]
True reward weights: [-0.73597892 -0.19546757  0.6481724 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.159240

Saving results to files...
Results saved successfully.

Running experiment 21/50...
Shuffled Demos: [([(0, 1), (3, 1), (3, 0), (4, 2), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (4, 2), (3, 2), (3, 1), (3, 1), (3, 1), (3, 1), (3, 1)]), ([(0, 2), (0, 1), (3, 3), (3, 3), (0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 1), (3, 1), (3, 3), (3, 3), (4, 2), (3, 0), (0, 3)]), ([(0, 1), (0, 2), (0, 2), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (0, 2), (0, 2), (3, 0), (0, 1), (3, 1), (3, 3), (4, 2), (4, 1), (3, 3)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 1), (3, 0), (0, 1), (1, 2), (1, 1), (4, 1), (4, 3)]), ([(0, 1), (3, 2), (3, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 1), (3, 2), (3, 1), (3, 0), (3, 3), (4, 0), (5, None)]), ([(0, 1), (3, 1), (4, 0), (1, 1), (4, 2), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (4, 0), (1, 1), (4, 2), (3, 0), (0, 0), (0, 2), (0, 0), (0, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6546
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.52629241 0.01982063 0.85007261]
True reward weights: [-0.3738588  -0.3262014   0.86822937]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5702
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.86949008 -0.38450693  0.3100668 ]
True reward weights: [-0.72295704 -0.41601196  0.55160417]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.138875

Running PBIRL with 3 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5700
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.13644482 0.09194389 0.9863717 ]
True reward weights: [-0.16472025 -0.98586617 -0.03058003]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.134062

Running PBIRL with 4 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5592
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.31784122 -0.66280048  0.6779915 ]
True reward weights: [ 0.09766555 -0.74967129  0.65456428]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.135180

Running PBIRL with 5 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5362
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.48911367 -0.25067454  0.8354221 ]
True reward weights: [-0.2281064  -0.09982683  0.96850507]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.139975

Running PBIRL with 6 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.5026
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.59422674 -0.64261125  0.48367899]
True reward weights: [-0.25475879 -0.00655366  0.96698243]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.145439

Running experiment 22/50...
Shuffled Demos: [([(0, 0), (1, 3), (2, 0), (2, 1), (5, None)], [(0, 0), (1, 3), (2, 0), (2, 2), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 1), (4, 0), (1, 0), (1, 0), (1, 3), (2, 2), (1, 2), (0, 2)]), ([(0, 2), (0, 1), (3, 0), (0, 2), (0, 1), (3, 1), (3, 3), (4, 2), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 0), (0, 2), (0, 1), (3, 1), (3, 3), (4, 2), (3, 2), (3, 0)]), ([(0, 3), (1, 2), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 3), (1, 3), (2, 1), (1, 2), (1, 2), (1, 1), (4, 0), (3, 0)]), ([(0, 0), (0, 0), (0, 2), (0, 3), (1, 0), (1, 1), (4, 1), (4, 3), (1, 1), (4, 3)], [(0, 0), (0, 0), (0, 2), (0, 3), (1, 0), (1, 1), (4, 1), (4, 0), (1, 2), (0, 2)]), ([(0, 1), (3, 1), (3, 3), (0, 1), (3, 0), (0, 1), (3, 2), (3, 1), (3, 3), (4, 3)], [(0, 1), (3, 1), (3, 3), (0, 1), (3, 0), (0, 1), (3, 2), (3, 1), (3, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 22
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.75715046  0.64105844 -0.12556776]
True reward weights: [-0.70965126 -0.6051058   0.36089066]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6494
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.62551529  0.5909986  -0.50936361]
True reward weights: [-0.13091814  0.37093829  0.91938307]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.182443

Running PBIRL with 3 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.4842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.35771973 -0.24127882  0.90212035]
True reward weights: [-0.34717943 -0.66564099  0.66059709]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.208164

Running PBIRL with 4 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.4754
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.22825652 -0.03690548  0.9729013 ]
True reward weights: [-0.09652845 -0.99474842  0.03402697]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.193612

Running PBIRL with 5 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.4674
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.49505305 -0.74765938  0.44263747]
True reward weights: [-0.46531593  0.42201761  0.77806312]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.238173

Running PBIRL with 6 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.4626
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.73102923 -0.5954357   0.33324555]
True reward weights: [0.12656832 0.01129359 0.9918936 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.302052

Saving results to files...
Results saved successfully.

Running experiment 23/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 2), (0, 2), (0, 3), (1, 0), (1, 3), (1, 0), (1, 0), (1, 1)]), ([(0, 1), (3, 0), (3, 1), (3, 0), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 1), (3, 0), (3, 1), (3, 0), (0, 3), (1, 0), (1, 3), (1, 0), (1, 3), (2, 1)]), ([(0, 2), (0, 0), (1, 0), (1, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (1, 0), (1, 1), (0, 0), (0, 1), (3, 3), (4, 0), (1, 2), (0, 3)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 3), (4, 1), (4, 3), (5, None)]), ([(0, 0), (0, 3), (3, 1), (3, 0), (0, 3), (1, 2), (0, 0), (0, 1), (0, 1), (3, 3)], [(0, 0), (0, 3), (3, 1), (3, 0), (0, 3), (1, 2), (0, 0), (0, 3), (1, 2), (0, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6574
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.41178849  0.80255536  0.43166554]
True reward weights: [-0.74528522 -0.55533186  0.36899385]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6398
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.08599654 -0.92805947  0.36236751]
True reward weights: [-0.91369212  0.33273068  0.23336024]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.127489

Running PBIRL with 3 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.5176
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.61865408  0.17453544  0.76603166]
True reward weights: [-0.90565195 -0.12193153  0.40611235]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151653

Running PBIRL with 4 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.5300
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.26096343 -0.03196112  0.96481945]
True reward weights: [-0.70258207  0.11656965  0.70199   ]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.168035

Running PBIRL with 5 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.4262
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.49427674 -0.29554089  0.81752436]
True reward weights: [-0.57605696 -0.80051443 -0.16533309]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.192610

Running PBIRL with 6 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.2464
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.59534975 -0.18213473  0.78255071]
True reward weights: [-0.60950585 -0.58335741  0.5368396 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.247857

Running experiment 24/50...
Shuffled Demos: [([(0, 0), (0, 3), (0, 0), (0, 0), (1, 0), (1, 0), (1, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (0, 0), (0, 0), (1, 0), (1, 0), (1, 2), (0, 2), (0, 2), (0, 2)]), ([(0, 2), (0, 3), (1, 0), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 0), (1, 0), (1, 0), (1, 1), (4, 0), (1, 3), (4, 2), (3, 1)]), ([(0, 2), (0, 1), (3, 0), (0, 1), (3, 0), (0, 2), (0, 1), (3, 3), (4, 3), (1, 1)], [(0, 2), (0, 1), (3, 0), (0, 1), (3, 0), (0, 2), (0, 3), (1, 0), (1, 3), (2, 3)]), ([(0, 0), (0, 0), (0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 0), (0, 2), (0, 2), (0, 3), (1, 2), (0, 0), (0, 0), (0, 0)]), ([(0, 0), (1, 0), (1, 1), (4, 0), (1, 1), (4, 3), (5, None)], [(0, 0), (1, 0), (1, 1), (4, 0), (1, 1), (4, 0), (1, 0), (1, 1), (0, 3), (1, 2)]), ([(0, 1), (0, 1), (3, 3), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 0), (0, 1), (1, 0), (1, 2), (0, 0), (0, 1), (3, 2), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6438
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.82456815  0.34205961  0.45064686]
True reward weights: [-0.68144434 -0.29187893  0.6711485 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123672

Running PBIRL with 2 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.4896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.6032499  -0.01307542  0.79744504]
True reward weights: [-0.91767264 -0.33287027  0.21696613]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.154662

Running PBIRL with 3 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.3530
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.80701884 -0.04111034  0.58909297]
True reward weights: [-0.09090366 -0.03424585  0.99527069]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.182694

Running PBIRL with 4 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.2848
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.85351712  0.26752538  0.44714505]
True reward weights: [-0.13248821  0.4884057   0.86250029]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.192694

Running PBIRL with 5 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.2766
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.5845365  -0.17272448  0.79276941]
True reward weights: [-0.51579111 -0.21699     0.82877915]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.175397

Running PBIRL with 6 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.2678
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.86608602 -0.26753803  0.42227764]
True reward weights: [-0.84575289 -0.48856847  0.21448285]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.165276

Saving results to files...
Results saved successfully.

Running experiment 25/50...
Shuffled Demos: [([(0, 0), (0, 1), (3, 1), (3, 0), (0, 3), (1, 1), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 1), (3, 0), (0, 3), (1, 2), (0, 2), (0, 2), (0, 0), (1, 2)]), ([(0, 3), (1, 3), (2, 1), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 2), (1, 1), (2, 3), (2, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 3), (3, 1), (4, 3), (1, 1), (4, 1), (4, 2), (3, 3), (4, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 2), (0, 1), (3, 1), (3, 1), (3, 3), (4, 2), (1, 2), (0, 2)]), ([(0, 1), (3, 3), (4, 2), (3, 1), (3, 1), (3, 2), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 2), (3, 1), (3, 1), (3, 2), (3, 1), (3, 1), (4, 0), (1, 0)]), ([(0, 2), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 2), (0, 3), (1, 1), (4, 0), (1, 0), (1, 1), (4, 2), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6508
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.56393276 -0.26295109  0.78283879]
True reward weights: [-0.44714936 -0.30119858  0.84222139]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123672

Running PBIRL with 2 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5784
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.87218408 -0.35307587  0.33857401]
True reward weights: [-0.25076305 -0.32814514  0.91073523]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.139952

Running PBIRL with 3 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.64688073 -0.50092355  0.57499646]
True reward weights: [ 0.31781335 -0.92510923  0.20776809]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.136475

Running PBIRL with 4 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.5544
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.2426106  -0.56465382  0.78886384]
True reward weights: [ 0.59937313 -0.12020886  0.79139224]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.133095

Running PBIRL with 5 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.4242
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.24096879 -0.34868029  0.90573511]
True reward weights: [ 0.81524605 -0.42975346  0.38818275]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.173417

Running PBIRL with 6 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.4172
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.70927067 -0.37253338  0.59845969]
True reward weights: [-0.9418873  -0.27383213  0.19458744]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.166730

Running experiment 26/50...
Shuffled Demos: [([(0, 3), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (4, 0), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (4, 0), (1, 2), (4, 3), (5, None)]), ([(0, 1), (1, 1), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 1), (1, 0), (1, 3), (2, 1), (5, None)]), ([(0, 1), (0, 0), (1, 1), (4, 3), (5, None)], [(0, 1), (0, 0), (1, 3), (2, 0), (2, 3), (2, 0), (2, 0), (2, 0), (2, 3), (2, 3)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 0), (0, 3), (1, 0), (1, 2), (0, 2), (3, 0), (0, 1), (1, 1)]), ([(0, 2), (0, 0), (0, 3), (1, 3), (4, 0), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 3), (1, 3), (4, 0), (3, 0), (0, 2), (3, 3), (4, 2), (4, 3)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 1), (4, 0), (1, 1), (4, 0), (1, 0), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 26
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.80460548 0.47833394 0.35186171]
True reward weights: [-0.7333898  -0.47830978  0.48307263]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6666
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.88774063  0.41999761 -0.18846373]
True reward weights: [-0.93512776 -0.07282247  0.34674625]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.184976

Running PBIRL with 3 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.0628
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.1907135  0.13498697 0.97232036]
True reward weights: [0.25764932 0.15244043 0.95413769]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.280455

Running PBIRL with 4 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.0482
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.05671061  0.13944557  0.98860449]
True reward weights: [0.38627583 0.02123621 0.92213882]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.258506

Running PBIRL with 5 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.0470
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.00178373  0.13927016  0.99025282]
True reward weights: [-0.41963778  0.13593894  0.89745459]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.229304

Running PBIRL with 6 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.0452
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.09179489  0.13397148  0.98672455]
True reward weights: [-0.57711264  0.06522436  0.81405576]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.232584

Saving results to files...
Results saved successfully.

Running experiment 27/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (1, 3), (4, 3), (4, 0), (5, None)]), ([(0, 2), (0, 0), (0, 1), (3, 1), (3, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (3, 1), (3, 1), (3, 2), (3, 1), (3, 1), (3, 3), (4, 2)]), ([(0, 0), (0, 3), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 0), (1, 0), (0, 2), (0, 3), (3, 1), (3, 0), (0, 0), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (3, 1), (3, 2), (3, 1), (3, 2), (0, 2), (0, 2), (0, 3)]), ([(0, 3), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (3, 0), (0, 1), (3, 2), (3, 2), (3, 3), (4, 2), (1, 1), (4, 1), (5, None)]), ([(0, 3), (1, 2), (1, 0), (1, 0), (1, 1), (4, 1), (4, 2), (3, 3), (3, 3), (4, 3)], [(0, 3), (1, 2), (1, 0), (1, 0), (1, 1), (4, 1), (4, 2), (3, 1), (3, 1), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6550
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.29806603 -0.62218909 -0.72390426]
True reward weights: [-0.65557811 -0.01374154  0.75500233]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5426
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.08952754 -0.79659477 -0.59784746]
True reward weights: [-0.77752384 -0.07753361 -0.62405547]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.142032

Running PBIRL with 3 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5506
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.59395635 -0.63074099  0.49938128]
True reward weights: [ 0.17706915 -0.54253133 -0.82116154]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.135373

Running PBIRL with 4 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5436
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.65109532 -0.15539018  0.74291909]
True reward weights: [-0.68457591 -0.48484988  0.54431279]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.137810

Running PBIRL with 5 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5356
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.33037008 -0.80303015 -0.49598204]
True reward weights: [-0.25702816 -0.84934338 -0.46103399]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.135191

Running PBIRL with 6 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.5374
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.36074632 -0.87526839 -0.32212938]
True reward weights: [ 0.23124827 -0.89429247 -0.3830995 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.157240

Running experiment 28/50...
Shuffled Demos: [([(0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 3), (4, 0), (5, None)]), ([(0, 3), (1, 0), (1, 2), (0, 1), (3, 3), (4, 2), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 2), (0, 1), (3, 3), (4, 2), (3, 0), (0, 2), (0, 1), (3, 1)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 1), (3, 1), (3, 3), (4, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (0, 1)], [(0, 2), (0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 2), (0, 3), (1, 3), (2, 1), (5, None)], [(0, 2), (0, 3), (1, 3), (2, 2), (5, None)]), ([(0, 1), (3, 3), (4, 3), (4, 3), (4, 3), (1, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 2), (0, 2), (0, 0), (0, 0), (1, 2), (0, 2), (0, 0), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 28
MCMC completed with acceptance ratio: 1.0000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.77927558 0.57831058 0.24142585]
True reward weights: [-0.866301   -0.34967581  0.35672034]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.147550

Running PBIRL with 2 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6558
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.55348254  0.11371926  0.82506061]
True reward weights: [-0.96019638  0.05524773 -0.27380759]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.184272

Running PBIRL with 3 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6140
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.87915043 -0.16750506  0.44613515]
True reward weights: [-0.41081444 -0.5408371  -0.73398006]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.181712

Running PBIRL with 4 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.1248
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.94094165 -0.15480315  0.30110597]
True reward weights: [-0.70450316 -0.6688573  -0.23728719]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.292719

Running PBIRL with 5 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.1266
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.97540467 -0.10680682  0.19281607]
True reward weights: [-0.93975175 -0.30656468 -0.15127702]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.303468

Running PBIRL with 6 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.1202
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.96849518 -0.1251256   0.21531528]
True reward weights: [-0.93934505 -0.34289671 -0.00726087]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.299140

Saving results to files...
Results saved successfully.

Running experiment 29/50...
Shuffled Demos: [([(0, 3), (1, 0), (1, 1), (0, 2), (0, 1), (3, 2), (3, 3), (4, 1), (4, 3), (4, 3)], [(0, 3), (1, 0), (1, 1), (0, 2), (0, 1), (3, 2), (3, 3), (4, 1), (4, 0), (1, 2)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (3, 3), (4, 0), (1, 2), (0, 1), (3, 1), (3, 1), (3, 2), (3, 2)]), ([(0, 1), (0, 2), (0, 2), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 1), (0, 2), (0, 2), (0, 3), (1, 3), (2, 2), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 2), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 0), (0, 1), (3, 0), (3, 2), (3, 2), (3, 2), (0, 3), (3, 2)]), ([(0, 1), (3, 1), (3, 3), (4, 3), (4, 3), (1, 1)], [(0, 1), (3, 1), (3, 2), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 1), (3, 3), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6540
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.11125146  0.74978895  0.65225735]
True reward weights: [-0.6535586  -0.23072892  0.72085041]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123673

Running PBIRL with 2 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.3474
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.76344478 -0.1846156  -0.6189258 ]
True reward weights: [-0.51388381  0.22100381 -0.82890334]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.169074

Running PBIRL with 3 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.3442
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.91198246  0.03515119 -0.40872043]
True reward weights: [-0.80406685 -0.19545986 -0.56149083]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154553

Running PBIRL with 4 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.2682
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.99196121 -0.07524006  0.10174428]
True reward weights: [-0.83647944  0.065524    0.54406686]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.170860

Running PBIRL with 5 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.0388
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.53906552 -0.26605253 -0.7991398 ]
True reward weights: [-0.99485159 -0.08421605 -0.05637343]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.301098

Running PBIRL with 6 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.0396
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.55343599 -0.27694023 -0.78550157]
True reward weights: [-0.36913384 -0.26692261 -0.89022049]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.245759

Running experiment 30/50...
Shuffled Demos: [([(0, 0), (0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 1), (1, 1), (4, 0), (1, 1), (4, 2), (3, 2), (3, 2), (3, 1)]), ([(0, 0), (0, 1), (0, 3), (1, 1), (2, 3), (2, 1), (5, None)], [(0, 0), (0, 1), (0, 3), (1, 1), (2, 3), (2, 2), (1, 0), (1, 0), (1, 0), (1, 2)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (0, 2), (3, 2), (3, 3), (4, 3), (5, None)]), ([(0, 0), (0, 3), (1, 1), (4, 2), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 1), (4, 2), (3, 0), (0, 1), (3, 1), (3, 2), (3, 2), (3, 2)]), ([(0, 0), (1, 1), (4, 3), (5, None)], [(0, 0), (1, 0), (2, 0), (2, 0), (2, 2), (1, 2), (1, 0), (1, 1), (0, 3), (3, 3)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 1), (4, 2), (3, 3), (4, 0), (3, 3), (4, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.6568
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.19409794 -0.92742959 -0.31968789]
True reward weights: [-0.81326386 -0.04441834  0.5801973 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.5410
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.63864528 -0.1662778   0.75132143]
True reward weights: [-0.23137561  0.40440048  0.88483082]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.143387

Running PBIRL with 3 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.4394
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.28227827 -0.76866228 -0.57400112]
True reward weights: [-0.29548886 -0.01370492  0.95524788]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.161248

Running PBIRL with 4 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.4366
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.77554481 -0.38313143 -0.50173754]
True reward weights: [ 0.03390231 -0.79758013  0.60225955]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.155410

Running PBIRL with 5 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.4310
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.48124105 -0.87627034 -0.02360814]
True reward weights: [-0.72127423 -0.6678964  -0.18351536]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.147982

Running PBIRL with 6 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.4346
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.11858467 -0.57375583  0.81039615]
True reward weights: [ 0.14210007 -0.57755128  0.80389184]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.144489

Saving results to files...
Results saved successfully.

Running experiment 31/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 3), (1, 1), (4, 2), (4, 0), (3, 3), (3, 2), (3, 2), (3, 0)]), ([(0, 1), (3, 2), (3, 2), (3, 2), (3, 2), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 2), (3, 2), (3, 2), (3, 1), (3, 2), (3, 3), (4, 2), (4, 0)]), ([(0, 0), (0, 2), (0, 0), (0, 3), (1, 1), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 0), (0, 3), (1, 0), (1, 1), (4, 2), (1, 1), (4, 1), (4, 3)]), ([(0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 3), (1, 2), (0, 2), (0, 0), (0, 3), (1, 0), (1, 2), (1, 0)]), ([(0, 2), (3, 2), (0, 1), (0, 0), (0, 3), (1, 3), (2, 3), (2, 1), (2, 1), (5, None)], [(0, 2), (3, 2), (0, 1), (0, 0), (0, 3), (1, 3), (2, 3), (2, 3), (2, 3), (2, 0)]), ([(0, 1), (3, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 2), (3, 2), (3, 2), (3, 0), (0, 0), (0, 1), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6640
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.74814909 -0.64590737 -0.15190988]
True reward weights: [-0.27534761 -0.19938474  0.94044108]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6226
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.67027599 -0.39853721  0.62601772]
True reward weights: [-0.39038869 -0.91838487 -0.06454384]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.130210

Running PBIRL with 3 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.5590
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.19173066 -0.98040629  0.04519812]
True reward weights: [ 0.37464875 -0.47426214  0.79668923]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.143294

Running PBIRL with 4 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.5020
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.13975575  0.12740292  0.98195561]
True reward weights: [ 0.4362167  -0.87604732 -0.20556284]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.152202

Running PBIRL with 5 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.4934
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.31006189 -0.43251551  0.84663567]
True reward weights: [-0.96945289 -0.13095718 -0.2073917 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.151335

Running PBIRL with 6 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.4784
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.65447387 -0.29933642  0.69430661]
True reward weights: [-0.00461181  0.13332309  0.9910619 ]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.150083

Running experiment 32/50...
Shuffled Demos: [([(0, 1), (3, 3), (0, 2), (0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (0, 2), (0, 1), (3, 1), (4, 2), (3, 0), (0, 1), (3, 2), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 2), (0, 1), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 3), (1, 2), (0, 3), (1, 2), (0, 3), (3, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 3), (1, 2), (0, 3), (3, 1), (3, 2), (3, 1), (3, 1), (3, 1)]), ([(0, 0), (0, 0), (0, 3), (3, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 3), (3, 1), (3, 1), (3, 2), (3, 3), (4, 3), (5, None)]), ([(0, 0), (0, 0), (1, 3), (1, 1), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 0), (1, 3), (1, 0), (1, 1), (4, 3), (5, None)]), ([(0, 2), (3, 0), (0, 0), (0, 3), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (3, 0), (0, 0), (0, 3), (0, 0), (0, 1), (3, 3), (4, 1), (4, 0), (1, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6550
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.52248371 -0.6515664  -0.54997455]
True reward weights: [-0.90753571 -0.40234227  0.1204144 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6234
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.43482525 -0.87130636  0.22749117]
True reward weights: [-0.83937175 -0.01263284  0.54341097]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.129793

Running PBIRL with 3 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5662
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.82880076 -0.10801164  0.54901983]
True reward weights: [ 0.90151405 -0.42877502  0.05851847]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.142680

Running PBIRL with 4 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.5708
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.04473824 -0.99548413  0.08372478]
True reward weights: [ 0.87929477 -0.46704866 -0.09330731]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.137667

Running PBIRL with 5 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.0174
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.84599429 -0.00568314  0.53316166]
True reward weights: [-0.01427127 -0.45660477  0.88955518]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.458316

Running PBIRL with 6 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.0196
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.53290056 -0.00600647  0.84615655]
True reward weights: [-0.98649014 -0.00660983  0.16368725]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.396719

Saving results to files...
Results saved successfully.

Running experiment 33/50...
Shuffled Demos: [([(0, 1), (3, 0), (0, 1), (3, 2), (3, 0), (0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 2), (3, 0), (0, 2), (0, 0), (1, 2), (0, 1), (3, 3)]), ([(0, 3), (1, 3), (1, 3), (2, 0), (2, 0), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (1, 3), (2, 0), (2, 0), (1, 3), (1, 3), (2, 2), (5, None)]), ([(0, 2), (0, 3), (0, 0), (0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (0, 0), (0, 1), (3, 3), (3, 0), (0, 2), (0, 3), (3, 0), (0, 2)]), ([(0, 2), (3, 2), (3, 3), (4, 3), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (3, 2), (3, 2), (3, 0), (0, 1), (3, 1), (3, 2), (0, 0), (0, 0), (0, 2)]), ([(0, 2), (0, 0), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (1, 2), (1, 1), (2, 3), (2, 0), (2, 0), (2, 1), (5, None)]), ([(0, 0), (0, 3), (1, 0), (1, 2), (4, 2), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 0), (1, 2), (4, 2), (3, 1), (3, 1), (3, 3), (4, 2), (1, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6568
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.22680927 -0.67822358  0.69897806]
True reward weights: [-0.32556312 -0.30396656  0.89532842]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.5626
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.61166148  0.28837659  0.73668797]
True reward weights: [-0.91849668  0.05133039 -0.39208294]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.137177

Running PBIRL with 3 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.4122
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.63368204 -0.68528633  0.35892857]
True reward weights: [-0.65442268  0.6852891   0.31954626]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.175597

Running PBIRL with 4 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.3586
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.40074586 -0.48755761  0.775687  ]
True reward weights: [-0.90761033 -0.22550915  0.35410325]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.176387

Running PBIRL with 5 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.3544
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.42403763 -0.74804206  0.51051461]
True reward weights: [-0.96402729 -0.26490053 -0.02188822]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.166225

Running PBIRL with 6 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.3534
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.34951011 -0.33086332  0.87656839]
True reward weights: [-0.5694783  -0.4235952   0.70445836]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.157457

Running experiment 34/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 2), (0, 0), (0, 2), (0, 1), (1, 3), (2, 2), (1, 0), (2, 3)]), ([(0, 0), (0, 3), (1, 2), (4, 2), (4, 0), (1, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 2), (4, 2), (4, 0), (1, 0), (1, 1), (4, 3), (5, None)]), ([(0, 3), (1, 1), (4, 1), (3, 3), (0, 1), (1, 1), (4, 1), (4, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 1), (3, 3), (0, 1), (1, 1), (4, 1), (4, 1), (4, 2), (3, 3)]), ([(0, 1), (3, 3), (3, 3), (3, 2), (3, 2), (0, 0), (0, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (3, 3), (3, 2), (3, 2), (0, 0), (0, 3), (0, 1), (3, 2), (0, 1)]), ([(0, 1), (1, 1), (4, 3), (1, 1)], [(0, 1), (1, 0), (2, 1), (5, None)]), ([(0, 0), (0, 0), (0, 1), (3, 3), (3, 0), (0, 1), (3, 3), (4, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 1), (3, 3), (3, 0), (0, 1), (3, 3), (4, 1), (3, 0), (0, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.6614
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.59699638 -0.51619713  0.61411387]
True reward weights: [-0.839382   -0.393236    0.37523766]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.2778
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.91597846 -0.1742585  -0.36141034]
True reward weights: [ 0.38369048 -0.74599443 -0.54431051]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.181166

Running PBIRL with 3 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.2096
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.50284525 -0.01722616  0.86420479]
True reward weights: [-0.45570973 -0.16953646 -0.8738341 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.189711

Running PBIRL with 4 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.2074
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.61700015 -0.07965124  0.78292177]
True reward weights: [-0.93590186 -0.34021331  0.09133787]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.178285

Running PBIRL with 5 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.0020
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [0.21311667 0.1100414  0.97081006]
True reward weights: [-0.3126122   0.07911809  0.94658013]
MAP Policy for current environment:
Information gain 5 demonstrations: 9.778828

Running PBIRL with 6 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.0034
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [0.23006371 0.08765289 0.96922013]
True reward weights: [0.28282185 0.07438932 0.95628345]
MAP Policy for current environment:
Information gain 6 demonstrations: 9.785984

Saving results to files...
Results saved successfully.

Running experiment 35/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (4, 3), (4, 3), (5, None)], [(0, 3), (3, 2), (3, 3), (4, 3), (5, None)]), ([(0, 3), (1, 2), (0, 2), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 2), (3, 1), (3, 2), (3, 3), (3, 1), (3, 0), (3, 2), (3, 3)]), ([(0, 2), (0, 3), (1, 1), (4, 2), (3, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 1), (4, 2), (3, 3), (0, 1), (3, 1), (3, 0), (0, 1), (3, 1)]), ([(0, 0), (0, 2), (0, 3), (0, 0), (0, 2), (3, 0), (0, 1), (3, 3), (4, 3), (1, 1)], [(0, 0), (0, 2), (0, 3), (0, 0), (0, 2), (3, 0), (0, 1), (3, 0), (0, 3), (1, 1)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 1), (0, 0), (0, 1), (3, 3), (3, 3), (4, 1), (4, 2), (3, 3)]), ([(0, 0), (0, 3), (1, 2), (0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (1, 1), (4, 3)], [(0, 0), (0, 3), (1, 2), (0, 0), (0, 3), (1, 3), (2, 3), (2, 0), (2, 2), (1, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6604
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.61529116 0.77771886 0.12872514]
True reward weights: [-0.94121751 -0.05112011  0.33391067]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.0920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.00933257  0.1944085   0.98087626]
True reward weights: [-0.53471897  0.11124169 -0.8376759 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.200823

Running PBIRL with 3 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.0888
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.02709758 0.19285309 0.98085341]
True reward weights: [0.12129093 0.14809422 0.98150732]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.169568

Running PBIRL with 4 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.0960
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.04264236  0.19203696  0.98046083]
True reward weights: [ 0.89536316 -0.0222619   0.44477997]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.187414

Running PBIRL with 5 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.0768
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.19236827  0.18609507  0.96351599]
True reward weights: [-0.78277854  0.06818743  0.61855333]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.225144

Running PBIRL with 6 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.0708
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.19857166  0.18881128  0.9617274 ]
True reward weights: [-0.82985928  0.02250628  0.55751865]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.268314

Running experiment 36/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 1), (3, 3), (4, 1), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 1), (3, 3), (4, 1), (4, 2), (3, 2), (3, 1), (3, 1), (3, 3)]), ([], [(0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 3), (1, 1), (4, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 1), (5, None)]), ([(0, 0), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 1), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.6634
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.66743947 -0.43215856  0.6064351 ]
True reward weights: [-0.05456508 -0.040557    0.99768621]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.10381774 -0.83410616 -0.54174605]
True reward weights: [ 0.32443152 -0.94008554 -0.10480152]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.136237

Running PBIRL with 3 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.5896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.89012547 -0.44360261 -0.10437132]
True reward weights: [-0.44561382 -0.44321379 -0.77781094]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.156051

Running PBIRL with 4 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.0026
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.99998636 -0.00498352 -0.00156274]
True reward weights: [-0.14351152 -0.9894909   0.01766921]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.497991

Running PBIRL with 5 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.0040
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.99980498 -0.01702556  0.01000645]
True reward weights: [ 0.99979717 -0.01156273  0.01649008]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.550018

Running PBIRL with 6 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.0034
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.99995113 -0.00263519 -0.00952871]
True reward weights: [-0.99493364 -0.02573982  0.09718287]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.577084

Saving results to files...
Results saved successfully.

Running experiment 37/50...
Shuffled Demos: [([(0, 0), (0, 3), (1, 0), (0, 3), (1, 0), (1, 1), (0, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 0), (0, 3), (1, 0), (1, 1), (0, 3), (0, 0), (1, 1), (4, 2)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 3), (4, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 2), (0, 0), (0, 1), (0, 3), (3, 0), (0, 0), (0, 1), (3, 1)]), ([(0, 2), (0, 1), (3, 2), (3, 3), (0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 2), (3, 3), (0, 2), (0, 0), (0, 1), (3, 0), (3, 2), (3, 2)]), ([(0, 3), (1, 0), (1, 0), (2, 2), (1, 0), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 0), (2, 2), (1, 0), (1, 0), (0, 3), (1, 0), (1, 1), (4, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6570
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.72815218 -0.46782518  0.50093312]
True reward weights: [-0.75478167 -0.33513042  0.563908  ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6570
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.59548256  0.6144191  -0.51758061]
True reward weights: [0.48816767 0.40437408 0.77341704]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147807

Running PBIRL with 3 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.3006
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.59046244 -0.46102641  0.66242641]
True reward weights: [-0.13541055 -0.30759201  0.94183392]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.230012

Running PBIRL with 4 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.3030
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.90263211 -0.37934512  0.20335328]
True reward weights: [-0.81901797 -0.20887205  0.53439876]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.205973

Running PBIRL with 5 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.2972
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.89747088 -0.44051165  0.02225985]
True reward weights: [ 0.06687196 -0.30544751  0.94985786]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.196345

Running PBIRL with 6 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.3036
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.8305113  -0.54561863  0.11203258]
True reward weights: [-0.80752575 -0.42529492  0.40868863]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.189082

Running experiment 38/50...
Shuffled Demos: [([(0, 0), (0, 3), (1, 2), (0, 1), (3, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 2), (0, 1), (3, 1), (3, 1), (3, 3), (0, 1), (3, 0), (0, 2)]), ([(0, 0), (0, 1), (3, 2), (3, 2), (3, 1), (3, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 2), (3, 2), (3, 1), (3, 1), (3, 1), (3, 2), (0, 0), (0, 2)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 1), (1, 3), (2, 1), (5, None)]), ([(0, 0), (0, 3), (3, 3), (4, 2), (3, 2), (3, 1), (3, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (3, 3), (4, 2), (3, 2), (3, 1), (3, 0), (0, 0), (0, 2), (3, 0)]), ([(0, 3), (1, 2), (4, 3), (4, 3)], [(0, 3), (1, 2), (4, 0), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 2), (3, 3), (0, 3), (1, 3), (1, 3), (4, 0), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6688
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.76142587 -0.51920121 -0.38815042]
True reward weights: [-0.49248773 -0.22865786  0.83974486]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6240
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.04648576 -0.62971873  0.77543111]
True reward weights: [ 0.80548078 -0.4328154   0.4048105 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.129837

Running PBIRL with 3 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6290
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.88422763 -0.03649924  0.46562786]
True reward weights: [ 0.4744384  -0.73651602 -0.48213314]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.152004

Running PBIRL with 4 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.5204
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [0.10036174 0.09236327 0.99065461]
True reward weights: [ 0.05522871 -0.99805192  0.02902007]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.179292

Running PBIRL with 5 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.0028
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.99967411  0.0012754   0.02549586]
True reward weights: [-0.41747644 -0.30842627  0.85474362]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.665823

Running PBIRL with 6 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.0038
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-9.99983072e-01  4.61807408e-04  5.80022901e-03]
True reward weights: [-0.99922383 -0.03399289 -0.01990516]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.632065

Saving results to files...
Results saved successfully.

Running experiment 39/50...
Shuffled Demos: [([(0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 3), (4, 1), (4, 3), (5, None)]), ([(0, 2), (0, 0), (0, 0), (0, 0), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 0), (0, 0), (1, 3), (4, 0), (1, 1), (4, 2), (1, 0), (1, 0)]), ([(0, 1), (3, 3), (0, 1), (3, 3), (0, 1), (3, 3), (3, 3), (4, 3), (1, 1), (4, 3)], [(0, 3), (1, 0), (2, 2), (1, 0), (1, 3), (2, 2), (1, 2), (0, 3), (1, 2), (0, 2)]), ([(0, 1), (3, 1), (3, 2), (3, 3), (4, 2), (3, 0), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 2), (3, 3), (4, 2), (3, 0), (4, 0), (1, 3), (2, 0), (2, 3)]), ([(0, 1), (3, 1), (3, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 2), (0, 1), (3, 2), (3, 2), (0, 1), (3, 1), (3, 3)]), ([(0, 1), (1, 2), (0, 2), (0, 0), (0, 0), (0, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (1, 2), (0, 2), (0, 0), (0, 0), (0, 2), (0, 0), (0, 2), (3, 2), (3, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.6630
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.05085325 -0.50745291 -0.8601776 ]
True reward weights: [-0.56481614 -0.46100063  0.68444222]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.5002
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.1698809  -0.46950921  0.86643037]
True reward weights: [-0.01844039 -0.8402513   0.54188349]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.150957

Running PBIRL with 3 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2546
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.84150144 -0.3818201   0.38221557]
True reward weights: [-0.19398572 -0.9089226  -0.36909248]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.195626

Running PBIRL with 4 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2548
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.55404176 -0.45589249 -0.69656282]
True reward weights: [-0.78370259 -0.53166099  0.32116483]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.178178

Running PBIRL with 5 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2382
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.68734236 -0.46003114  0.56207813]
True reward weights: [-0.99939947 -0.02072385  0.02777091]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.170480

Running PBIRL with 6 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.1786
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.72173773 -0.56069552  0.40585117]
True reward weights: [-0.9192679  -0.28398191  0.27258173]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.209960

Running experiment 40/50...
Shuffled Demos: [([(0, 3), (1, 0), (2, 2), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (2, 2), (1, 2), (0, 3), (1, 0), (1, 3), (4, 0), (1, 3), (2, 0)]), ([(0, 3), (0, 3), (1, 0), (1, 3), (2, 1), (5, None)], [(0, 3), (0, 3), (1, 0), (1, 3), (2, 2), (5, None)]), ([(0, 1), (0, 0), (0, 2), (0, 2), (3, 3), (4, 1), (4, 0), (1, 3), (2, 1), (5, None)], [(0, 1), (0, 0), (0, 2), (0, 2), (3, 3), (4, 1), (4, 0), (1, 3), (2, 2), (1, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (4, 3), (1, 2), (1, 3), (2, 3), (2, 2), (2, 1), (5, None)]), ([(0, 0), (1, 1), (4, 3), (1, 2), (0, 1), (3, 1), (3, 1), (3, 2), (3, 3), (4, 3), (5, None)], [(0, 0), (1, 1), (4, 3), (1, 2), (0, 1), (3, 1), (3, 1), (3, 2), (3, 2), (3, 0)]), ([(0, 0), (0, 2), (0, 0), (0, 3), (3, 2), (3, 0), (0, 1), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 0), (0, 3), (3, 2), (3, 0), (0, 3), (1, 1), (0, 1), (3, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6438
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.01057961 -0.14085815  0.98997326]
True reward weights: [-0.6667631  -0.57474284  0.47444455]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6566
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.58215398  0.65208299  0.48567944]
True reward weights: [-0.16802071 -0.74412082 -0.64657037]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.147753

Running PBIRL with 3 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.5842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.59164252  0.67031895  0.44791923]
True reward weights: [-0.69284435 -0.71173053  0.11578585]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.167010

Running PBIRL with 4 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.5274
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.36830635  0.39102126  0.84347662]
True reward weights: [-0.26686567  0.5535598   0.78889433]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.171905

Running PBIRL with 5 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.4494
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.26732775  0.06918866  0.96111852]
True reward weights: [-0.65358625 -0.75037091  0.09883575]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.199011

Running PBIRL with 6 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.4402
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.59468962  0.67625881  0.43476233]
True reward weights: [ 0.03927845 -0.52643361  0.84930846]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.189591

Saving results to files...
Results saved successfully.

Running experiment 41/50...
Shuffled Demos: [([(0, 2), (0, 1), (3, 3), (0, 1), (0, 1), (3, 3), (4, 3), (4, 3), (1, 1), (0, 1)], [(0, 2), (0, 0), (0, 2), (3, 0), (0, 1), (3, 1), (3, 1), (3, 1), (3, 3), (4, 0)]), ([(0, 2), (0, 1), (3, 2), (3, 2), (3, 3), (4, 0), (1, 2), (0, 2), (0, 1), (3, 3)], [(0, 2), (0, 1), (3, 2), (3, 2), (3, 3), (4, 0), (1, 2), (0, 2), (0, 2), (0, 1)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 3), (2, 1), (5, None)]), ([(0, 1), (3, 2), (3, 0), (0, 2), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 1), (3, 2), (3, 0), (0, 2), (0, 3), (1, 2), (0, 0), (0, 1), (3, 2), (3, 0)]), ([(0, 3), (1, 2), (0, 1), (3, 3), (4, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 0), (0, 0), (0, 1), (0, 2), (0, 1), (3, 3), (3, 3), (4, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 0), (0, 0), (0, 0), (1, 1), (4, 0), (3, 0), (0, 3), (1, 2)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.6624
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.01697651 -0.77037676 -0.63736289]
True reward weights: [-0.46247666 -0.0267168   0.88622884]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.6584
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.51846703 -0.7981312   0.3068852 ]
True reward weights: [-0.4706032  -0.85831283  0.20452804]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.148002

Running PBIRL with 3 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5644
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.07824521 -0.99360129  0.0814504 ]
True reward weights: [ 0.05867593 -0.22340089  0.97295898]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.169078

Running PBIRL with 4 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5526
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.28441964 -0.71280043 -0.64110921]
True reward weights: [ 0.78641755 -0.56806519  0.24258892]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.170296

Running PBIRL with 5 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5346
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.39759465 -0.90413042  0.15641826]
True reward weights: [-2.29026577e-05 -5.62342624e-01  8.26904331e-01]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.165369

Running PBIRL with 6 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.5040
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.58209733 -0.81210566  0.04058428]
True reward weights: [-0.25909108 -0.46728267  0.84529209]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.173582

Running experiment 42/50...
Shuffled Demos: [([(0, 2), (3, 3), (4, 3), (1, 3), (2, 1), (2, 1), (5, None)], [(0, 2), (3, 3), (4, 3), (1, 3), (2, 0), (2, 2), (1, 3), (2, 1), (5, None)]), ([(0, 1), (0, 2), (0, 1), (3, 3), (4, 3), (1, 1)], [(0, 1), (0, 2), (0, 1), (3, 3), (4, 0), (5, None)]), ([(0, 2), (3, 1), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (3, 1), (4, 0), (1, 2), (0, 3), (0, 0), (0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (0, 3), (1, 0), (1, 1), (4, 2), (3, 3), (4, 3), (5, None)], [(0, 1), (0, 3), (1, 0), (1, 1), (4, 2), (3, 2), (3, 2), (3, 1), (3, 0), (0, 0)]), ([(0, 3), (1, 2), (0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 2), (0, 0), (0, 3), (1, 2), (0, 2), (0, 2), (0, 3), (1, 0), (1, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 2), (0, 2), (0, 1), (3, 2), (3, 3), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.6604
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.0620974  -0.9181522   0.39133164]
True reward weights: [-0.71505276 -0.68463496  0.14133129]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3222
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.07369444 -0.74003869 -0.66851468]
True reward weights: [-0.64478377  0.33375077  0.6876513 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.176708

Running PBIRL with 3 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3150
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.19201602 -0.67246563 -0.71478655]
True reward weights: [ 0.48961646 -0.8600037   0.14376842]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.161116

Running PBIRL with 4 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.2904
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.2826443  -0.95672679 -0.06918127]
True reward weights: [ 0.53828333 -0.84173382 -0.04165615]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.167464

Running PBIRL with 5 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.1588
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.06578948 -0.77850262 -0.6241838 ]
True reward weights: [ 0.61777099 -0.71238105  0.33297483]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.221370

Running PBIRL with 6 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.1634
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.24488055 -0.50212901 -0.82939735]
True reward weights: [-0.44682726 -0.32199914 -0.83466278]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.198871

Saving results to files...
Results saved successfully.

Running experiment 43/50...
Shuffled Demos: [([(0, 1), (1, 1), (2, 1), (2, 1), (5, None)], [(0, 0), (0, 2), (3, 1), (4, 2), (3, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 2), (5, None)]), ([(0, 1), (3, 3), (4, 2), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 3), (4, 2), (3, 1), (3, 3), (4, 1), (4, 3), (5, None)]), ([(0, 0), (0, 1), (3, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 1), (3, 3), (3, 0), (0, 3), (1, 2), (0, 2), (3, 0), (0, 3)]), ([(0, 0), (0, 1), (3, 1), (3, 2), (3, 3), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 1), (3, 2), (3, 1), (3, 1), (3, 1), (3, 1), (3, 3), (3, 1)]), ([(0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 3), (1, 0), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.6576
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.08096434 -0.79894589 -0.59592805]
True reward weights: [-0.94655708 -0.29794542  0.12352418]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.1866
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90505075 -0.42528933  0.00348066]
True reward weights: [ 0.18484849 -0.96214568 -0.20026663]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.189299

Running PBIRL with 3 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.1942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.81817965 -0.37572857  0.43521271]
True reward weights: [-0.32070899 -0.10312576  0.94154703]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.163843

Running PBIRL with 4 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.1876
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.85183784 -0.46030112 -0.24999035]
True reward weights: [-0.15395254 -0.12088228  0.98065595]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.156683

Running PBIRL with 5 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.1642
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.84268972 -0.49374827  0.2146781 ]
True reward weights: [-0.84671267 -0.34833216 -0.40217206]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.174893

Running PBIRL with 6 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.1526
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.81240837 -0.39887223  0.42531587]
True reward weights: [-0.69541936 -0.39516568  0.60019664]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.166000

Running experiment 44/50...
Shuffled Demos: [([(0, 3), (1, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 1), (4, 0), (1, 2), (0, 0), (0, 3), (1, 3), (1, 1), (0, 2)]), ([(0, 1), (1, 2), (0, 3), (1, 1), (2, 0), (2, 1), (2, 2), (1, 3), (2, 1), (5, None)], [(0, 1), (1, 2), (0, 3), (1, 1), (2, 0), (2, 1), (2, 2), (1, 3), (2, 3), (2, 3)]), ([(0, 1), (3, 1), (3, 0), (0, 1), (3, 3), (3, 3), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 0), (0, 0), (0, 1), (3, 0), (0, 2), (0, 0), (0, 1), (1, 2)]), ([(0, 1), (3, 2), (3, 0), (3, 1), (3, 1), (3, 2), (3, 1), (3, 3), (0, 1), (3, 3)], [(0, 1), (3, 2), (3, 0), (3, 1), (3, 1), (3, 2), (3, 1), (3, 0), (4, 1), (3, 1)]), ([(0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 3), (4, 1), (4, 0), (1, 3), (1, 1), (0, 1), (3, 3), (0, 1)]), ([(0, 0), (0, 1), (3, 3), (3, 1), (3, 1), (3, 3), (4, 0), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 3), (3, 1), (3, 1), (3, 3), (4, 0), (3, 1), (3, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.6600
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-9.05504606e-01 -2.80319427e-04  4.24336340e-01]
True reward weights: [-0.88315502 -0.32822573  0.33511952]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.5056
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.48430718  0.32915779  0.8106181 ]
True reward weights: [-0.86128073  0.32778036 -0.38827252]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.150963

Running PBIRL with 3 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.4718
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.15594245 -0.74713982  0.64611457]
True reward weights: [ 0.01568094 -0.10949369  0.99386379]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.149279

Running PBIRL with 4 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.4680
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.00761246 -0.79558904  0.60578885]
True reward weights: [-0.83399487  0.12709605  0.53693496]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.167482

Running PBIRL with 5 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.4456
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.26291116 -0.71759292  0.64493265]
True reward weights: [ 0.25097237 -0.80619943  0.53577547]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.173879

Running PBIRL with 6 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.4084
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.5170357  -0.6621864   0.54238662]
True reward weights: [-0.1728702  -0.29575111  0.93949304]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.188049

Saving results to files...
Results saved successfully.

Running experiment 45/50...
Shuffled Demos: [([(0, 0), (0, 1), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 2), (0, 3), (1, 2), (0, 1), (3, 3), (4, 2), (4, 2), (3, 1)]), ([(0, 1), (3, 3), (4, 3), (1, 1), (2, 1), (2, 1), (5, None)], [(0, 3), (3, 2), (3, 1), (3, 3), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 3), (1, 0), (1, 0), (1, 3), (1, 3), (2, 2), (5, None)]), ([(0, 2), (0, 0), (0, 1), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 1), (3, 1), (3, 1), (3, 2), (3, 1), (3, 3), (4, 1), (4, 2)]), ([(0, 0), (0, 1), (3, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 1), (3, 3), (4, 2), (4, 2), (4, 3), (5, None)]), ([(0, 2), (3, 2), (3, 0), (0, 0), (0, 3), (1, 0), (1, 3), (2, 2), (2, 1), (5, None)], [(0, 2), (3, 2), (3, 0), (0, 0), (0, 3), (1, 0), (1, 3), (2, 2), (2, 2), (1, 1)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6524
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.8406777  -0.48691551 -0.23701116]
True reward weights: [-0.83381491 -0.37890594  0.40147601]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.4378
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.81806875 -0.55344941  0.15638823]
True reward weights: [-0.14674092 -0.64402628 -0.75079774]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.159205

Running PBIRL with 3 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.0136
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.0042532  -0.89187955 -0.45225301]
True reward weights: [ 0.09737765 -0.21534031  0.97167183]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.348714

Running PBIRL with 4 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.0162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.00444032 -0.84372035  0.53676462]
True reward weights: [-0.02320194 -0.75141662 -0.65941999]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.296789

Running PBIRL with 5 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.0162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.00426759 -0.74227935 -0.67007698]
True reward weights: [-0.00615853 -0.17267242  0.98496005]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.276264

Running PBIRL with 6 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.0114
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.00410764 -0.17149998  0.98517556]
True reward weights: [-0.01223631 -0.84608487 -0.53290774]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.310224

Running experiment 46/50...
Shuffled Demos: [([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (0, 3), (1, 3), (4, 2), (1, 0), (1, 1), (4, 3), (5, None)]), ([], [(0, 1), (3, 3), (4, 3), (5, None)]), ([(0, 2), (0, 2), (0, 0), (0, 3), (3, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 2), (0, 0), (0, 3), (3, 1), (3, 3), (4, 2), (1, 3), (4, 0), (1, 0)]), ([(0, 2), (3, 0), (3, 0), (0, 2), (0, 3), (1, 1), (2, 0), (2, 2), (1, 1), (4, 3), (5, None)], [(0, 2), (3, 0), (3, 0), (0, 2), (0, 3), (1, 1), (2, 0), (2, 2), (1, 2), (0, 1)]), ([(0, 1), (3, 0), (0, 1), (3, 0), (0, 1), (3, 3), (0, 1), (3, 3), (4, 3), (1, 1)], [(0, 1), (3, 0), (0, 1), (3, 0), (0, 1), (3, 0), (3, 3), (4, 1), (4, 2), (3, 2)]), ([(0, 0), (0, 1), (3, 3), (4, 1), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 3), (4, 1), (4, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6520
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.5666294  -0.67868072  0.46725111]
True reward weights: [-0.82098277 -0.18440172  0.54035479]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.5366
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.77322053 -0.04363093 -0.63263445]
True reward weights: [-0.1701439  -0.1984015   0.96523981]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.144534

Running PBIRL with 3 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.4710
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.18105433 -0.91214753  0.36770397]
True reward weights: [ 0.36709066 -0.69142307  0.62223677]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151756

Running PBIRL with 4 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.3080
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.7417505  -0.51919368  0.42455166]
True reward weights: [-0.70586648 -0.51585022  0.48543904]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.197726

Running PBIRL with 5 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.2786
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.20215953 -0.66697706  0.71712838]
True reward weights: [-0.05563481 -0.38244377  0.9223023 ]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.208413

Running PBIRL with 6 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.2628
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.03040928 -0.81119412  0.58398576]
True reward weights: [ 0.36951748 -0.65765442  0.65646591]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.213950

Saving results to files...
Results saved successfully.

Running experiment 47/50...
Shuffled Demos: [([(0, 3), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 3), (2, 0), (2, 1), (5, None)]), ([(0, 1), (3, 2), (3, 3), (4, 3), (1, 1), (2, 1), (5, None)], [(0, 1), (3, 2), (3, 3), (4, 3), (1, 2), (0, 1), (3, 2), (3, 0), (0, 1), (3, 2)]), ([(0, 0), (1, 1), (4, 3), (5, None)], [(0, 0), (1, 2), (4, 3), (5, None)]), ([(0, 3), (1, 3), (2, 1), (5, None)], [(0, 3), (1, 3), (2, 3), (2, 1), (5, None)]), ([(0, 0), (0, 0), (0, 1), (3, 2), (3, 0), (0, 0), (1, 3), (4, 0), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 0), (0, 1), (3, 2), (3, 0), (0, 0), (1, 3), (4, 0), (3, 0), (4, 3)]), ([(0, 0), (1, 0), (1, 0), (0, 1), (3, 3), (0, 0), (0, 3), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 0), (1, 0), (1, 0), (0, 1), (3, 3), (0, 0), (0, 3), (3, 2), (0, 0), (0, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6516
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.18239066 -0.55119084 -0.81420041]
True reward weights: [-0.73857451 -0.23755048  0.63093381]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6308
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.50374484 -0.61966024  0.60188232]
True reward weights: [-0.44256569 -0.63405972  0.63411661]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.128428

Running PBIRL with 3 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6184
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.80722636 -0.58251095  0.09521864]
True reward weights: [ 0.23032042 -0.39201965 -0.8906588 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.151241

Running PBIRL with 4 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6398
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.84958598 -0.50775983 -0.1427712 ]
True reward weights: [ 0.25441146 -0.42715225 -0.86764956]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.150583

Running PBIRL with 5 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.4422
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.51836187 -0.50916075  0.68706354]
True reward weights: [-0.35658559 -0.67399683  0.64697371]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.207707

Running PBIRL with 6 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.4366
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [ 0.63623521 -0.3445215   0.69029681]
True reward weights: [ 0.86658162 -0.49836364  0.02588395]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.196677

Running experiment 48/50...
Shuffled Demos: [([(0, 3), (0, 1), (0, 1), (1, 1), (0, 1), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (0, 0), (0, 1), (0, 3), (1, 0), (1, 1), (4, 2), (3, 3), (4, 1), (4, 2)]), ([(0, 0), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 1), (3, 0), (0, 2), (0, 3), (1, 1), (0, 1), (1, 2), (0, 1), (3, 1)]), ([(0, 3), (1, 0), (1, 1), (4, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 1), (4, 1), (4, 2), (3, 0), (0, 3), (1, 1), (4, 3), (4, 2)]), ([(0, 0), (0, 0), (0, 0), (0, 3), (1, 0), (2, 1), (1, 1)], [(0, 0), (0, 0), (0, 0), (0, 3), (1, 0), (2, 2), (5, None)]), ([(0, 1), (3, 0), (0, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 2), (0, 1), (3, 1), (4, 3), (4, 2), (4, 3), (5, None)]), ([(0, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 0), (1, 0), (1, 3), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.6786
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.81135608 -0.24828871  0.52920132]
True reward weights: [-0.95541883 -0.18852986  0.22722531]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5982
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.39190178 -0.54540774  0.74090714]
True reward weights: [-0.80381827 -0.59282169 -0.04938252]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.133405

Running PBIRL with 3 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.5876
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.60362736 -0.6930936   0.39402446]
True reward weights: [-0.58893718 -0.58184461  0.56090093]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.135201

Running PBIRL with 4 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.2134
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [ 0.57501617 -0.79101666  0.20892353]
True reward weights: [-0.59978385  0.29156732  0.74514953]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.234563

Running PBIRL with 5 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.2108
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [ 0.51499432 -0.83837464  0.17863036]
True reward weights: [-0.08051221 -0.98328316 -0.16331566]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.203566

Running PBIRL with 6 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.1098
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.23681954 -0.76788047 -0.59521096]
True reward weights: [-0.34084365 -0.86990045 -0.35650921]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.265061

Saving results to files...
Results saved successfully.

Running experiment 49/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 0), (1, 1), (2, 1), (5, None)]), ([(0, 1), (3, 1), (3, 1), (3, 2), (3, 1), (3, 3), (4, 3), (1, 1), (4, 3), (5, None)], [(0, 1), (3, 1), (3, 1), (3, 2), (3, 1), (3, 2), (3, 0), (0, 1), (3, 0), (4, 3)]), ([(0, 1), (1, 1), (4, 3), (5, None)], [(0, 2), (0, 1), (3, 3), (4, 0), (1, 2), (0, 3), (1, 1), (2, 0), (2, 2), (1, 2)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (1, 1), (4, 0), (1, 0), (2, 0), (2, 1), (5, None)]), ([(0, 0), (0, 3), (1, 1), (4, 3), (5, None)], [(0, 0), (0, 3), (1, 0), (1, 0), (1, 3), (4, 1), (5, None)]), ([(0, 0), (0, 2), (0, 3), (1, 2), (0, 1), (3, 3), (4, 3), (5, None)], [(0, 0), (0, 2), (0, 3), (1, 2), (0, 2), (0, 0), (1, 3), (1, 3), (2, 1), (2, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.6596
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.87512153  0.31340668 -0.36869846]
True reward weights: [-0.98484463 -0.1489005   0.08893646]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.1904
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.73713703 -0.35059652  0.57767731]
True reward weights: [-0.71517897 -0.32883322  0.61675583]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.188762

Running PBIRL with 3 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.1700
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.34046138  0.0872682   0.93619993]
True reward weights: [0.07195914 0.18108194 0.9808319 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.176652

Running PBIRL with 4 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.1710
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.84398729 -0.4302723   0.32023618]
True reward weights: [-0.6113519  -0.37971159  0.69431115]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.163793

Running PBIRL with 5 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.1768
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.81578504 -0.33551544  0.47108828]
True reward weights: [-0.06658597  0.14028403  0.98786978]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.155358

Running PBIRL with 6 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.1706
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.77030742 -0.27603711  0.5748304 ]
True reward weights: [-0.72485231 -0.14980288  0.67241968]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.149239

Running experiment 50/50...
Shuffled Demos: [([(0, 1), (3, 1), (3, 3), (4, 2), (3, 0), (0, 3), (0, 3), (1, 2), (0, 1), (3, 3)], [(0, 1), (3, 1), (3, 3), (4, 2), (3, 0), (0, 3), (0, 3), (1, 2), (0, 3), (1, 1)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 2), (3, 3), (3, 3), (4, 1), (3, 1), (3, 1), (3, 0)]), ([(0, 0), (1, 2), (4, 2), (3, 3), (4, 3), (5, None)], [(0, 0), (1, 2), (4, 2), (3, 2), (3, 0), (0, 1), (3, 2), (3, 0), (0, 0), (0, 2)]), ([(0, 3), (0, 2), (0, 0), (0, 2), (0, 2), (0, 3), (3, 3), (4, 2), (3, 3), (0, 1)], [(0, 3), (0, 2), (0, 0), (0, 2), (0, 2), (0, 3), (3, 3), (4, 2), (3, 2), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 1), (1, 1), (4, 3), (5, None)]), ([(0, 2), (0, 0), (0, 2), (3, 0), (0, 0), (0, 1), (3, 3), (3, 3), (4, 3), (5, None)], [(0, 2), (0, 0), (0, 2), (3, 0), (0, 0), (0, 1), (3, 3), (3, 2), (3, 0), (0, 3)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.6500
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.85390957 -0.41618707 -0.31245283]
True reward weights: [-0.47840873 -0.41855607  0.77196885]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.2626
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.72151222 -0.2898244   0.62882583]
True reward weights: [-0.88036051  0.44859994 -0.15402423]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.178272

Running PBIRL with 3 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.2590
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.98001566 -0.19804497 -0.01864098]
True reward weights: [-0.70226458 -0.6374478  -0.31699333]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.160194

Running PBIRL with 4 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.2634
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 4
MAP Solution: [-0.84977676 -0.5171218   0.10229617]
True reward weights: [-0.69078944 -0.18137203 -0.69993867]
MAP Policy for current environment:
Information gain 4 demonstrations: 0.174955

Running PBIRL with 5 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.2698
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 5
MAP Solution: [-0.85444652 -0.51714495 -0.0498222 ]
True reward weights: [-0.70203357 -0.68213501 -0.20454998]
MAP Policy for current environment:
Information gain 5 demonstrations: 0.169121

Running PBIRL with 6 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.2338
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 6
MAP Solution: [-0.8432653  -0.53221645 -0.07516171]
True reward weights: [-0.23139548 -0.19491966  0.95313297]
MAP Policy for current environment:
Information gain 6 demonstrations: 0.192152

Saving results to files...
Results saved successfully.
