Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.6160
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.98079996  0.06435225 -0.18409299]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.2802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.93112458 -0.11438063  0.34630058]
True reward weights: [-0.26438861  0.93093684  0.25190327]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174616

Running PBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.2758
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.44916147  0.2099478   0.86843301]
True reward weights: [-0.88270677 -0.32957831  0.33497298]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155673

Running experiment 2/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6284
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.7269668  -0.21886869  0.65085771]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.6268
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.63234891  0.24719585 -0.73418599]
True reward weights: [ 0.60523103  0.71304869 -0.3539166 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123851

Running PBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2980
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.75111475 -0.20798192  0.62655419]
True reward weights: [-0.41044408  0.58365032 -0.70063397]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.197889

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.6126
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.9841415   0.05454898 -0.16878958]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.80102892 -0.19111683  0.56729801]
True reward weights: [6.04670942e-04 9.52335539e-01 3.05051891e-01]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174586

Running PBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.67666545 -0.03163222  0.73561081]
True reward weights: [-0.80217806 -0.43261671 -0.41152539]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154968

Running experiment 4/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.6248
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.3504199  -0.29531192  0.88881762]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123672

Running PBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.2748
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.84537968 -0.16753172  0.50721427]
True reward weights: [-0.74148389 -0.56188741 -0.36672085]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173916

Running PBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.2772
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.46238291 -0.05823561  0.88476588]
True reward weights: [-0.31498702  0.60710226  0.72952726]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155090

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.6130
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.9269716  -0.11801439  0.35608462]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2858
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.69895742 -0.22572181  0.67860754]
True reward weights: [0.14312893 0.55437408 0.81986797]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.175393

Running PBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2834
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.92309824 -0.35758657  0.14149731]
True reward weights: [-0.21756991 -0.11034666  0.96978706]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155567

Running experiment 6/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6272
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.9353021   0.11206086 -0.33563722]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.6118
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.69139895  0.22822526 -0.68547846]
True reward weights: [ 0.18405598  0.83502232 -0.5185182 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123738

Running PBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.2894
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.86061608 -0.44282274  0.25149152]
True reward weights: [-0.75202523  0.27262568  0.60011106]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.197805

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.6318
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.52226074 -0.26966157  0.80902803]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.2828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.64718315 -0.24083901  0.72329146]
True reward weights: [0.274361   0.30743124 0.91115974]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.172688

Running PBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.2764
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.67577247 -0.23287818  0.69935636]
True reward weights: [-0.55995975  0.52215379  0.64327327]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154407

Running experiment 8/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6222
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.99162149 -0.04291411  0.12184084]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.6184
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.99863318  0.01575781 -0.04983435]
True reward weights: [-0.81503216  0.56471664  0.12968303]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123947

Running PBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.2780
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.96695433 -0.17739276  0.18311508]
True reward weights: [-0.3304381   0.88756909 -0.32098562]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.195345

Saving results to files...
Results saved successfully.

Running experiment 9/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.6224
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.12310727 -0.3133671   0.94161864]
True reward weights: [-0.0505111  -0.01531811  0.99860602]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.2878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.48920478  0.39936255  0.7753633 ]
True reward weights: [-0.61763066 -0.78569933  0.03476971]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174347

Running PBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.2958
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.79512388 -0.19187088  0.57529435]
True reward weights: [0.02255168 0.46058011 0.88733161]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155222

Running experiment 10/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.20571122  0.30886488 -0.92859323]
True reward weights: [-0.76518493 -0.64328377  0.02603879]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.6266
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.72497439  0.28012091 -0.62924114]
True reward weights: [0.32878674 0.66034333 0.67516366]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124027

Running PBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.2816
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.80542524 -0.18453482  0.56323803]
True reward weights: [-0.6093068   0.09817031  0.78683405]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.195191

Saving results to files...
Results saved successfully.

Running experiment 11/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6132
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.92907907  0.11859715 -0.3503524 ]
True reward weights: [-0.87829137 -0.24439244  0.41094598]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.6134
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.98972973  0.04410276 -0.13597799]
True reward weights: [-0.20611458 -0.1248878   0.97052554]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123714

Running PBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.2828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.81529523 -0.18388919  0.54907053]
True reward weights: [-0.70517516  0.6511735  -0.28053711]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.198040

Running experiment 12/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6104
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.6504647  -0.23896768  0.72096472]
True reward weights: [-0.8497926  -0.50850291  0.13884283]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.6338
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.56408305  0.26360397 -0.78251087]
True reward weights: [0.53152528 0.58210971 0.61532849]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124019

Running PBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.2746
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.93341977 -0.10423899  0.34331002]
True reward weights: [ 0.38375168  0.78867425 -0.48034111]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.198577

Saving results to files...
Results saved successfully.

Running experiment 13/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.6272
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.82013603  0.18310761 -0.54207794]
True reward weights: [-0.34477193 -0.32328333  0.88126058]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.2798
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.77109038 -0.19906973  0.60480647]
True reward weights: [ 0.18974534  0.1758577  -0.96595589]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.172675

Running PBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.2800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.92130585 -0.12154327  0.36935453]
True reward weights: [-0.88886066 -0.45212656  0.07421798]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154221

Running experiment 14/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.6194
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.03633994 -0.31348428  0.94889779]
True reward weights: [-0.43088031 -0.17122291  0.88601629]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.2814
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.14439096  0.25966286  0.95484368]
True reward weights: [ 0.34426211 -0.66882664  0.65890403]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173552

Running PBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.2816
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.83363927 -0.04651502  0.55034709]
True reward weights: [-0.17503543 -0.13699246  0.97498496]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155247

Saving results to files...
Results saved successfully.

Running experiment 15/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.6202
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.87530973  0.15307056 -0.45869629]
True reward weights: [-0.75433776 -0.02805593  0.65588674]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.6344
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.99015733 -0.04384263  0.13291459]
True reward weights: [-0.92217504 -0.37598874  0.09069546]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123817

Running PBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.2894
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.90545816 -0.00395272  0.42441713]
True reward weights: [ 0.05702722  0.50273708 -0.86255627]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.194738

Running experiment 16/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.6162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.98143311  0.0612014  -0.18177854]
True reward weights: [-0.92032338 -0.11683579  0.37330186]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.6280
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.69817452  0.71050718  0.08793108]
True reward weights: [0.54116759 0.77659869 0.32254011]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123835

Running PBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.2810
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.941286   -0.10943955  0.31938011]
True reward weights: [-0.80552464 -0.06934697  0.58849049]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.193181

Saving results to files...
Results saved successfully.

Running experiment 17/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.6246
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.62262405  0.24465834 -0.74329105]
True reward weights: [-0.76208548 -0.28664819  0.58056743]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.2916
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.59715222  0.0873003   0.79736308]
True reward weights: [-0.69843216  0.23545087  0.67583682]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174805

Running PBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.2852
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.8318406  -0.33820362  0.44006763]
True reward weights: [-0.17478801  0.44185249  0.87989518]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.156117

Running experiment 18/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.6292
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.04711858 -0.31764614  0.94703789]
True reward weights: [-0.39174849 -0.30341429  0.86860399]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.2850
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.92441639 -0.32911108  0.19271802]
True reward weights: [-0.23767163 -0.7770732   0.58281166]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.171870

Running PBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.2738
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.90593678 -0.13328829  0.40188652]
True reward weights: [-0.41034986  0.53218542  0.74053472]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155113

Saving results to files...
Results saved successfully.

Running experiment 19/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.93153535 -0.11658095  0.34445722]
True reward weights: [-0.92648321 -0.16748955  0.33701054]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.6126
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.71277827  0.21911477 -0.66628512]
True reward weights: [-0.51397764  0.35087295 -0.78276124]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123753

Running PBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.2842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.68144945 -0.23099028  0.69445672]
True reward weights: [-0.4111129   0.29542704  0.86238567]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.194329

Running experiment 20/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6376
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.83855381 -0.1741453   0.51623727]
True reward weights: [-0.61012931 -0.3129562   0.72787406]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.6162
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.47347591  0.28125609 -0.8346949 ]
True reward weights: [-0.88612299  0.46247099 -0.0301102 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124091

Running PBIRL with 3 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.2786
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.72979744 -0.2155948   0.6487793 ]
True reward weights: [-0.81232479 -0.24521936 -0.52914639]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.197204

Saving results to files...
Results saved successfully.

Running experiment 21/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.6198
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.39113733  0.28917519 -0.87372152]
True reward weights: [-0.3738588  -0.3262014   0.86822937]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.2828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.67502128 -0.23235025  0.70025683]
True reward weights: [-0.71701233  0.09607972 -0.69040713]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.172974

Running PBIRL with 3 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.2906
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.84382872 -0.49509316  0.20696825]
True reward weights: [-0.99472713 -0.05819878  0.08444434]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155032

Running experiment 22/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.6264
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.6017304   0.66054403 -0.44900122]
True reward weights: [-0.70965126 -0.6051058   0.36089066]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.2744
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.78266879  0.09246898  0.61553152]
True reward weights: [-0.08459146  0.6021098  -0.79391944]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174816

Running PBIRL with 3 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.2782
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.65657689 -0.23953886  0.7152118 ]
True reward weights: [-0.30666384 -0.29122834  0.9061696 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155331

Saving results to files...
Results saved successfully.

Running experiment 23/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.6276
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.79667573  0.05892684  0.60152756]
True reward weights: [-0.74528522 -0.55533186  0.36899385]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.2816
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.84647099 -0.16578337  0.50596713]
True reward weights: [-0.80925356 -0.57353381  0.12715203]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.172340

Running PBIRL with 3 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.2850
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.93436307 -0.11003478  0.33890708]
True reward weights: [-0.53165846  0.58251613  0.61482863]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154065

Running experiment 24/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.6290
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.93432606 -0.10978148  0.33909119]
True reward weights: [-0.68144434 -0.29187893  0.6711485 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.2922
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.79048917 -0.1940804   0.58091279]
True reward weights: [-0.85831916 -0.30054707  0.4158842 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173396

Running PBIRL with 3 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.2724
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.87629101 -0.15266145  0.45695575]
True reward weights: [-0.485801    0.51045642  0.70952916]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154255

Saving results to files...
Results saved successfully.

Running experiment 25/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6248
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.87667997  0.15073704 -0.45684852]
True reward weights: [-0.44714936 -0.30119858  0.84222139]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.6218
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.3135059   0.3009012  -0.90065116]
True reward weights: [0.34891392 0.85326285 0.38755848]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123852

Running PBIRL with 3 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.2838
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.88458529  0.02134859  0.46588958]
True reward weights: [ 0.34529488  0.70850682 -0.6154588 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.197801

Running experiment 26/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.6270
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.82697136 -0.17483433  0.53437002]
True reward weights: [-0.7333898  -0.47830978  0.48307263]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.2882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.78709607 -0.18814692  0.58743554]
True reward weights: [ 0.78764615 -0.50181149  0.35748954]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173311

Running PBIRL with 3 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.2794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.6901956  -0.22886455  0.68647728]
True reward weights: [-0.13874028  0.31988687  0.93724251]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.156498

Saving results to files...
Results saved successfully.

Running experiment 27/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6246
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.33415977  0.29882726 -0.8938901 ]
True reward weights: [-0.65557811 -0.01374154  0.75500233]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.6056
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.13809885  0.31590546 -0.93868655]
True reward weights: [ 0.66721041  0.70031356 -0.25375418]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123879

Running PBIRL with 3 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.2836
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.88825421 -0.28330609  0.36158279]
True reward weights: [-0.67890397 -0.24824106 -0.69098899]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.196735

Running experiment 28/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.6282
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.80451373 -0.18859126  0.56319712]
True reward weights: [-0.866301   -0.34967581  0.35672034]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123672

Running PBIRL with 2 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.2808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.03818117  0.45284077  0.89077351]
True reward weights: [-0.81269615  0.5371791   0.22575117]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.172605

Running PBIRL with 3 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.2822
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.86378028 -0.15892496  0.47814902]
True reward weights: [-0.70415711 -0.13681082  0.69673924]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154615

Saving results to files...
Results saved successfully.

Running experiment 29/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6200
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.22645555  0.30885489 -0.92375675]
True reward weights: [-0.6535586  -0.23072892  0.72085041]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.6256
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.2826125   0.30177881 -0.91052717]
True reward weights: [-0.76614919  0.06939194 -0.63890545]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124123

Running PBIRL with 3 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.2772
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.63758564 -0.24449079  0.73055376]
True reward weights: [-0.40798899  0.48629007  0.77270108]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.199164

Running experiment 30/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.6126
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.96556188  0.13478045  0.22254099]
True reward weights: [-0.81326386 -0.04441834  0.5801973 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.6314
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.9914802   0.04359216 -0.12274662]
True reward weights: [-0.93775985 -0.2787525   0.20713163]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123976

Running PBIRL with 3 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.2806
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.94547264 -0.10343745  0.30884005]
True reward weights: [-0.22000327 -0.07902364  0.97229307]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.197851

Saving results to files...
Results saved successfully.

Running experiment 31/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.6214
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.99697626 -0.02635571  0.07310067]
True reward weights: [-0.27534761 -0.19938474  0.94044108]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.2696
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.64809031 -0.07408861  0.75795107]
True reward weights: [-0.33828757  0.90193151 -0.26847918]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174792

Running PBIRL with 3 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.2840
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.78344714 -0.05381376  0.6191241 ]
True reward weights: [0.1083722  0.67069162 0.73377668]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155427

Running experiment 32/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6212
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.61511704  0.24985742 -0.7477983 ]
True reward weights: [-0.90753571 -0.40234227  0.1204144 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.6316
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.80713197  0.18786646 -0.55968222]
True reward weights: [-0.95620075  0.27595285  0.09762244]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124197

Running PBIRL with 3 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.2710
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.56252404  0.10670316  0.81986654]
True reward weights: [-0.81103594  0.49739256  0.30793076]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.194317

Saving results to files...
Results saved successfully.

Running experiment 33/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.6238
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.93107686 -0.11289926  0.34691446]
True reward weights: [-0.32556312 -0.30396656  0.89532842]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.2764
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.74738487 -0.21207649  0.62963436]
True reward weights: [-0.65798242 -0.71322101  0.24160903]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.175939

Running PBIRL with 3 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.2822
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.19698143  0.17251329  0.96511009]
True reward weights: [-0.71609951 -0.65208418 -0.24897333]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.156123

Running experiment 34/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.6210
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.71875436  0.22210539 -0.65883334]
True reward weights: [-0.839382   -0.393236    0.37523766]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123670

Running PBIRL with 2 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.2692
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.36728807 -0.02396708  0.92979839]
True reward weights: [-0.74894976 -0.60376285 -0.27302871]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174382

Running PBIRL with 3 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.2704
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.88217319 -0.16208488  0.44215263]
True reward weights: [-0.89401277 -0.12212423  0.43107637]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154933

Saving results to files...
Results saved successfully.

Running experiment 35/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6224
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.72920219 -0.21601571  0.64930839]
True reward weights: [-0.94121751 -0.05112011  0.33391067]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.6066
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.99964091  0.00661749 -0.02596636]
True reward weights: [ 0.12519763  0.98080355 -0.14949902]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124025

Running PBIRL with 3 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.2836
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.93678536 -0.11179359  0.33156505]
True reward weights: [-0.95250183  0.27697698  0.12658602]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.197069

Running experiment 36/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.6190
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.73857542  0.3727301   0.56176384]
True reward weights: [-0.05456508 -0.040557    0.99768621]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.2912
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.87905717 -0.15021131  0.45243238]
True reward weights: [-0.53605241  0.82867025 -0.16110068]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173505

Running PBIRL with 3 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.2756
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.84616649 -0.33806877  0.41196089]
True reward weights: [-0.59816223 -0.53569765  0.59601173]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155546

Saving results to files...
Results saved successfully.

Running experiment 37/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6224
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.9789808  -0.0620305   0.19429054]
True reward weights: [-0.75478167 -0.33513042  0.563908  ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.6318
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.91944667 -0.34438158  0.18978711]
True reward weights: [-0.78745332  0.44068344  0.43094707]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124060

Running PBIRL with 3 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.2924
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.1836751   0.33386394  0.92455304]
True reward weights: [0.21738609 0.89899848 0.38019076]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.196534

Running experiment 38/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6242
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.20472855  0.30771582 -0.92919169]
True reward weights: [-0.49248773 -0.22865786  0.83974486]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.6226
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.82239665  0.18266115 -0.53879371]
True reward weights: [-0.44526325  0.19880137  0.87305135]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124120

Running PBIRL with 3 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.2824
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.86274968 -0.21700266  0.45669774]
True reward weights: [-0.49110972 -0.2153438   0.8440606 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.194381

Saving results to files...
Results saved successfully.

Running experiment 39/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.6210
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.28283094 -0.30282851  0.91011074]
True reward weights: [-0.56481614 -0.46100063  0.68444222]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123674

Running PBIRL with 2 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.82120622 -0.44170747  0.36126839]
True reward weights: [-0.526308   -0.74658457  0.40695378]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173919

Running PBIRL with 3 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.2806
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.8267348  -0.17976578  0.53309834]
True reward weights: [-0.80059951  0.05421336  0.59674227]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154812

Running experiment 40/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.6086
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.32895978 -0.29860648  0.89589041]
True reward weights: [-0.6667631  -0.57474284  0.47444455]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.2810
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90254303 -0.13374881  0.40930104]
True reward weights: [ 0.39322651 -0.85589076 -0.33589271]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174101

Running PBIRL with 3 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.2716
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.34626862  0.22246104  0.9113776 ]
True reward weights: [-0.94596749 -0.32419683 -0.00647499]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154772

Saving results to files...
Results saved successfully.

Running experiment 41/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.6150
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.36492028  0.29165878 -0.88417665]
True reward weights: [-0.46247666 -0.0267168   0.88622884]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.6160
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.31161033  0.29902577 -0.9019327 ]
True reward weights: [-0.9817429   0.16217532 -0.09939842]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123915

Running PBIRL with 3 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.2784
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.87191026 -0.15527561  0.46439421]
True reward weights: [-0.4668424   0.07654481  0.8810216 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.196360

Running experiment 42/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.6244
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.23194003 0.5598849  0.79544499]
True reward weights: [-0.71505276 -0.68463496  0.14133129]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123668

Running PBIRL with 2 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.2718
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.4222818   0.29395473  0.8574781 ]
True reward weights: [-0.73094177  0.52923147 -0.4308575 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.172735

Running PBIRL with 3 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.2878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.79341127 -0.19239007  0.57748127]
True reward weights: [-0.77399688  0.25719889  0.57859966]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.154542

Saving results to files...
Results saved successfully.

Running experiment 43/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.6222
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.60487915 -0.25161853  0.75551925]
True reward weights: [-0.94655708 -0.29794542  0.12352418]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123669

Running PBIRL with 2 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.2710
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.67349822 -0.23464706  0.70095714]
True reward weights: [-0.03136523 -0.62334848  0.78131485]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174789

Running PBIRL with 3 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.2778
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.88265153 -0.14999053  0.44545384]
True reward weights: [-0.73004024  0.04459957  0.6819473 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155678

Running experiment 44/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.6212
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.7910792  0.10815643 0.60207631]
True reward weights: [-0.88315502 -0.32822573  0.33511952]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.2818
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.8688814  -0.15798123  0.46913435]
True reward weights: [ 0.32805494 -0.55654716  0.76330545]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174949

Running PBIRL with 3 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.2806
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.81191366 -0.18177039  0.55475737]
True reward weights: [-0.64784935 -0.127964    0.75094369]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155936

Saving results to files...
Results saved successfully.

Running experiment 45/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6248
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.69445201 -0.22523092  0.68337943]
True reward weights: [-0.83381491 -0.37890594  0.40147601]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.6270
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.28846233  0.30178373 -0.9086892 ]
True reward weights: [ 0.30001569  0.83435656 -0.46242807]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123875

Running PBIRL with 3 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.2846
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.89341021 -0.14085741  0.42658808]
True reward weights: [-0.0033917   0.62988576  0.77668039]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.195122

Running experiment 46/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6346
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.54178839  0.26351261 -0.79813937]
True reward weights: [-0.82098277 -0.18440172  0.54035479]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.6226
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.03211959  0.9400761   0.33944847]
True reward weights: [0.47529979 0.87920329 0.0330406 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123792

Running PBIRL with 3 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.2754
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.85355041 -0.16606921  0.49383471]
True reward weights: [-0.80025004 -0.55547404  0.22593909]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.196511

Saving results to files...
Results saved successfully.

Running experiment 47/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.6398
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.81033808 -0.18302726  0.55664461]
True reward weights: [-0.73857451 -0.23755048  0.63093381]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.2826
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.94973359 -0.21266791  0.22973564]
True reward weights: [-0.72167884  0.33366769 -0.6065027 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174485

Running PBIRL with 3 demonstrations for experiment 47
MCMC completed with acceptance ratio: 0.2796
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.25819782  0.37414044  0.89070355]
True reward weights: [-0.29482544  0.41564769  0.86041557]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.156265

Running experiment 48/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.6170
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.98639586  0.05474558 -0.15500364]
True reward weights: [-0.95541883 -0.18852986  0.22722531]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123671

Running PBIRL with 2 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.2884
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.85639071 -0.16404724  0.48957476]
True reward weights: [-0.49391623  0.65900508 -0.5672381 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.174413

Running PBIRL with 3 demonstrations for experiment 48
MCMC completed with acceptance ratio: 0.2778
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.72165595  0.27299384  0.63615017]
True reward weights: [-0.23452464  0.2582489   0.93717965]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155503

Saving results to files...
Results saved successfully.

Running experiment 49/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.6274
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.90593515 -0.13352159  0.40181275]
True reward weights: [-0.98484463 -0.1489005   0.08893646]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.2918
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.80971483 -0.18347703  0.55740298]
True reward weights: [-0.74116331  0.01587804  0.67113697]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173196

Running PBIRL with 3 demonstrations for experiment 49
MCMC completed with acceptance ratio: 0.2836
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.634882    0.05303116  0.77078696]
True reward weights: [-0.548268    0.30174532  0.7799692 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155088

Running experiment 50/50...
Shuffled Demos: [([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 3), (2, 1), (5, None)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 1), (3, 0), (0, 1), (3, 0)]), ([(0, 1), (3, 3), (4, 3), (5, None)], [(0, 3), (1, 1), (4, 3), (5, None)])]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.6198
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.99940033 -0.01034172  0.03304576]
True reward weights: [-0.47840873 -0.41855607  0.77196885]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.2764
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.78738623 -0.29982059  0.53863767]
True reward weights: [-0.17287293  0.06352801 -0.98289325]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.173974

Running PBIRL with 3 demonstrations for experiment 50
MCMC completed with acceptance ratio: 0.2902
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.8683524  -0.15398735  0.4714361 ]
True reward weights: [-0.336333    0.22596926  0.91423083]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.155025

Saving results to files...
Results saved successfully.
