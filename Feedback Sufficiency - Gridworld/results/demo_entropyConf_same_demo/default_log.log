Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Shuffled Demos: [(4, 3), (2, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
Config file loaded successfully.
Feature weights for environment: [-0.97266673 -0.03083248  0.23014951]
Feature weights for environment: [-0.35835056 -0.08647525  0.92957351]
Feature weights for environment: [-0.71046942 -0.64188771  0.28846728]
Feature weights for environment: [-0.89578956 -0.24432685  0.37130237]
Feature weights for environment: [-0.7282924  -0.22722733  0.64649665]
Feature weights for environment: [-0.75022399 -0.57596895  0.32469021]
Feature weights for environment: [-0.70842322 -0.26108332  0.65572254]
Feature weights for environment: [-0.74635145 -0.57484795  0.33542412]
Feature weights for environment: [-0.0505111  -0.01531811  0.99860602]
Feature weights for environment: [-0.76518493 -0.64328377  0.02603879]
Feature weights for environment: [-0.87829137 -0.24439244  0.41094598]
Feature weights for environment: [-0.8497926  -0.50850291  0.13884283]
Feature weights for environment: [-0.34477193 -0.32328333  0.88126058]
Feature weights for environment: [-0.43088031 -0.17122291  0.88601629]
Feature weights for environment: [-0.75433776 -0.02805593  0.65588674]
Feature weights for environment: [-0.92032338 -0.11683579  0.37330186]
Feature weights for environment: [-0.76208548 -0.28664819  0.58056743]
Feature weights for environment: [-0.39174849 -0.30341429  0.86860399]
Feature weights for environment: [-0.92648321 -0.16748955  0.33701054]
Feature weights for environment: [-0.61012931 -0.3129562   0.72787406]
Feature weights for environment: [-0.3738588  -0.3262014   0.86822937]
Feature weights for environment: [-0.70965126 -0.6051058   0.36089066]
Feature weights for environment: [-0.74528522 -0.55533186  0.36899385]
Feature weights for environment: [-0.68144434 -0.29187893  0.6711485 ]
Feature weights for environment: [-0.44714936 -0.30119858  0.84222139]
Feature weights for environment: [-0.7333898  -0.47830978  0.48307263]
Feature weights for environment: [-0.65557811 -0.01374154  0.75500233]
Feature weights for environment: [-0.866301   -0.34967581  0.35672034]
Feature weights for environment: [-0.6535586  -0.23072892  0.72085041]
Feature weights for environment: [-0.81326386 -0.04441834  0.5801973 ]
Feature weights for environment: [-0.27534761 -0.19938474  0.94044108]
Feature weights for environment: [-0.90753571 -0.40234227  0.1204144 ]
Feature weights for environment: [-0.32556312 -0.30396656  0.89532842]
Feature weights for environment: [-0.839382   -0.393236    0.37523766]
Feature weights for environment: [-0.94121751 -0.05112011  0.33391067]
Feature weights for environment: [-0.05456508 -0.040557    0.99768621]
Feature weights for environment: [-0.75478167 -0.33513042  0.563908  ]
Feature weights for environment: [-0.49248773 -0.22865786  0.83974486]
Feature weights for environment: [-0.56481614 -0.46100063  0.68444222]
Feature weights for environment: [-0.6667631  -0.57474284  0.47444455]
Feature weights for environment: [-0.46247666 -0.0267168   0.88622884]
Feature weights for environment: [-0.71505276 -0.68463496  0.14133129]
Feature weights for environment: [-0.94655708 -0.29794542  0.12352418]
Feature weights for environment: [-0.88315502 -0.32822573  0.33511952]
Feature weights for environment: [-0.83381491 -0.37890594  0.40147601]
Feature weights for environment: [-0.82098277 -0.18440172  0.54035479]
Feature weights for environment: [-0.73857451 -0.23755048  0.63093381]
Feature weights for environment: [-0.95541883 -0.18852986  0.22722531]
Feature weights for environment: [-0.98484463 -0.1489005   0.08893646]
Feature weights for environment: [-0.47840873 -0.41855607  0.77196885]
Generated optimal policies for all environments.

Running experiment 1/50...
Same Demos: [(2, 1), (2, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4796
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.2995328  -0.4929467   0.81687432]
True reward weights: [-0.97266673 -0.03083248  0.23014951]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4754
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.67106844  0.0594515   0.7390079 ]
True reward weights: [-0.97209763 -0.23382607  0.01875027]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123646

Running PBIRL with 3 demonstrations for experiment 1
MCMC completed with acceptance ratio: 0.4792
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.81274195 -0.20068924  0.54696834]
True reward weights: [0.59037018 0.50684832 0.62814634]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123726

Running experiment 2/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.65534988  0.26968457  0.70554005]
True reward weights: [-0.35835056 -0.08647525  0.92957351]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123993

Running PBIRL with 2 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2946
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.05495117  0.50493705  0.86140521]
True reward weights: [-0.91315492  0.18454251  0.36344485]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.127765

Running PBIRL with 3 demonstrations for experiment 2
MCMC completed with acceptance ratio: 0.2902
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.91153588 -0.02888561  0.41020478]
True reward weights: [-0.62341899  0.28843553  0.72674184]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127629

Saving results to files...
Results saved successfully.

Running experiment 3/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.88397461 -0.04802143  0.46506218]
True reward weights: [-0.71046942 -0.64188771  0.28846728]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124038

Running PBIRL with 2 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.38290134  0.18003357  0.90607642]
True reward weights: [-0.8434381   0.02673817  0.53656057]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.127681

Running PBIRL with 3 demonstrations for experiment 3
MCMC completed with acceptance ratio: 0.2930
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.91903441 -0.32248724  0.22666654]
True reward weights: [-0.88637644 -0.3815693   0.26218633]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127560

Running experiment 4/50...
Same Demos: [(2, 1), (2, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4864
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.72370686 -0.2602356   0.63916025]
True reward weights: [-0.89578956 -0.24432685  0.37130237]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4882
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.21181758 -0.97172771 -0.1043004 ]
True reward weights: [-0.8606444  -0.50817917 -0.03232882]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123796

Running PBIRL with 3 demonstrations for experiment 4
MCMC completed with acceptance ratio: 0.4736
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.29186968 -0.82211291  0.48881741]
True reward weights: [-0.50196333 -0.13542511  0.85422061]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124209

Saving results to files...
Results saved successfully.

Running experiment 5/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2968
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.60759767  0.06498576  0.79158191]
True reward weights: [-0.7282924  -0.22722733  0.64649665]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124113

Running PBIRL with 2 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.8925222  -0.02974718  0.45002136]
True reward weights: [-0.79045013 -0.45103852  0.41443075]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.128218

Running PBIRL with 3 demonstrations for experiment 5
MCMC completed with acceptance ratio: 0.2964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.61987561  0.17042316  0.76597009]
True reward weights: [-0.65018265 -0.60794785 -0.45569939]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.128526

Running experiment 6/50...
Same Demos: [(2, 1), (2, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4796
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.73800898 -0.05828771  0.67226876]
True reward weights: [-0.75022399 -0.57596895  0.32469021]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4920
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.18483221 -0.65311766  0.73435304]
True reward weights: [-0.17420402 -0.94720634 -0.26917116]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123769

Running PBIRL with 3 demonstrations for experiment 6
MCMC completed with acceptance ratio: 0.4768
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.3581419  -0.86035612 -0.36265925]
True reward weights: [-0.37429932  0.3696571   0.85044321]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123745

Saving results to files...
Results saved successfully.

Running experiment 7/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.2916
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.39266651  0.24519858  0.88639194]
True reward weights: [-0.70842322 -0.26108332  0.65572254]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123869

Running PBIRL with 2 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.3040
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.85506137 -0.17602139  0.48773612]
True reward weights: [-0.32186996  0.125436    0.93843782]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126819

Running PBIRL with 3 demonstrations for experiment 7
MCMC completed with acceptance ratio: 0.2954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.26068338  0.46071129  0.84840396]
True reward weights: [-0.68395809  0.45729312  0.56840508]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.126095

Running experiment 8/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.1005566  -0.48114666  0.87085376]
True reward weights: [-0.74635145 -0.57484795  0.33542412]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.78507253  0.05637796  0.61683276]
True reward weights: [-0.60225388 -0.03484292  0.79754388]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123680

Running PBIRL with 3 demonstrations for experiment 8
MCMC completed with acceptance ratio: 0.4800
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.53584072 -0.81746419  0.2112511 ]
True reward weights: [-0.37323056 -0.15690121  0.91437463]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123737

Saving results to files...
Results saved successfully.

Running experiment 9/50...
Same Demos: [(2, 1), (2, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4886
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.02758353 -0.26961766  0.96257232]
True reward weights: [-0.0505111  -0.01531811  0.99860602]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4860
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.5212499  -0.79397045  0.31290489]
True reward weights: [0.3093754  0.31782593 0.89625529]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124062

Running PBIRL with 3 demonstrations for experiment 9
MCMC completed with acceptance ratio: 0.4878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.75524692 -0.62706723  0.19075843]
True reward weights: [ 0.37811201 -0.77963521  0.4991996 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123895

Running experiment 10/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.84691024 -0.50871782  0.15475537]
True reward weights: [-0.76518493 -0.64328377  0.02603879]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.04292848 -0.61496909  0.78738184]
True reward weights: [-0.87565513 -0.12492266  0.46650019]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123854

Running PBIRL with 3 demonstrations for experiment 10
MCMC completed with acceptance ratio: 0.4802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.18147787 -0.94769203 -0.26257494]
True reward weights: [0.0669379  0.11809437 0.99074368]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123755

Saving results to files...
Results saved successfully.

Running experiment 11/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.3064
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.55512309  0.2010749   0.80709804]
True reward weights: [-0.87829137 -0.24439244  0.41094598]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123987

Running PBIRL with 2 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.2898
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-8.31122405e-01  5.81420622e-04  5.56089210e-01]
True reward weights: [-0.44588061  0.46232542  0.76645005]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126440

Running PBIRL with 3 demonstrations for experiment 11
MCMC completed with acceptance ratio: 0.2924
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.85842055 -0.02126057  0.51250576]
True reward weights: [-0.61310026  0.20884381  0.76190048]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127804

Running experiment 12/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4804
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.26555127 -0.78779211  0.55575724]
True reward weights: [-0.8497926  -0.50850291  0.13884283]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4818
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.54137137 -0.40453246  0.73706887]
True reward weights: [-0.25129027  0.10024099  0.96270709]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123590

Running PBIRL with 3 demonstrations for experiment 12
MCMC completed with acceptance ratio: 0.4750
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.96754502 -0.09440798  0.23440088]
True reward weights: [-0.88085516 -0.35294167  0.31547799]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123548

Saving results to files...
Results saved successfully.

Running experiment 13/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.2996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.83943188 -0.07169224  0.53871545]
True reward weights: [-0.34477193 -0.32328333  0.88126058]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123894

Running PBIRL with 2 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.2964
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.83690675 -0.04636249  0.54537842]
True reward weights: [-0.68616384  0.30358447  0.6610716 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126121

Running PBIRL with 3 demonstrations for experiment 13
MCMC completed with acceptance ratio: 0.2828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.95250567 -0.26679131  0.14681742]
True reward weights: [-0.39062836  0.53387198  0.74992679]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127441

Running experiment 14/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4822
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.123293   -0.83423667  0.53744583]
True reward weights: [-0.43088031 -0.17122291  0.88601629]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4862
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.25257362 -0.68449122  0.68387012]
True reward weights: [-0.43480347 -0.89426588  0.10599282]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123823

Running PBIRL with 3 demonstrations for experiment 14
MCMC completed with acceptance ratio: 0.4750
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.50934167 -0.31753356  0.79983967]
True reward weights: [-0.8337482  -0.18359175  0.52072835]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123756

Saving results to files...
Results saved successfully.

Running experiment 15/50...
Same Demos: [(2, 1), (2, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.4876
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.51620465 0.41156058 0.75109963]
True reward weights: [-0.75433776 -0.02805593  0.65588674]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.4722
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.8493344  -0.05997745  0.52443664]
True reward weights: [0.24996041 0.47412003 0.84423337]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124025

Running PBIRL with 3 demonstrations for experiment 15
MCMC completed with acceptance ratio: 0.4794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.7138778  -0.16809739  0.67979537]
True reward weights: [-0.27861881 -0.16355589  0.94637257]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124229

Running experiment 16/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.2968
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.77059047 -0.08879857  0.63111421]
True reward weights: [-0.92032338 -0.11683579  0.37330186]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124064

Running PBIRL with 2 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.2916
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.06581704  0.46401031  0.88338132]
True reward weights: [-0.6544962  -0.15049801  0.74093527]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.128438

Running PBIRL with 3 demonstrations for experiment 16
MCMC completed with acceptance ratio: 0.3008
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.56240369  0.35527403  0.74664747]
True reward weights: [-0.34661234  0.41108611  0.84312994]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.128429

Saving results to files...
Results saved successfully.

Running experiment 17/50...
Same Demos: [(2, 1), (2, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.4760
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [0.66499644 0.24190088 0.70658594]
True reward weights: [-0.76208548 -0.28664819  0.58056743]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.4790
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90811615  0.0471025   0.41606059]
True reward weights: [0.24580877 0.66860568 0.70181514]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123709

Running PBIRL with 3 demonstrations for experiment 17
MCMC completed with acceptance ratio: 0.4798
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.23111453 -0.42962509  0.8729309 ]
True reward weights: [-0.96326829 -0.26000909  0.06715256]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123686

Running experiment 18/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4760
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.53255654 -0.78854173  0.30754749]
True reward weights: [-0.39174849 -0.30341429  0.86860399]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4876
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.32014291 0.29505026 0.90025211]
True reward weights: [-0.19319368 -0.23749327  0.9519838 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123523

Running PBIRL with 3 demonstrations for experiment 18
MCMC completed with acceptance ratio: 0.4876
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.31456215 0.44480678 0.83856877]
True reward weights: [-0.18846334 -0.85432478  0.48436632]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123834

Saving results to files...
Results saved successfully.

Running experiment 19/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.2968
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.40661832  0.23753523  0.8821783 ]
True reward weights: [-0.92648321 -0.16748955  0.33701054]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124118

Running PBIRL with 2 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.2976
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.72797781  0.16097948  0.66643373]
True reward weights: [-0.91719222 -0.06165606  0.39364574]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.128016

Running PBIRL with 3 demonstrations for experiment 19
MCMC completed with acceptance ratio: 0.2944
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.82408207  0.04015273  0.56504557]
True reward weights: [-0.82431015 -0.55980834  0.08442396]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.128846

Running experiment 20/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.4778
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.42239869 -0.21661003  0.8801474 ]
True reward weights: [-0.61012931 -0.3129562   0.72787406]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.4822
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.56498728  0.19265804  0.80229188]
True reward weights: [-0.01884028 -0.62277281  0.78217586]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123617

Running PBIRL with 3 demonstrations for experiment 20
MCMC completed with acceptance ratio: 0.4808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.30946142 -0.80729675  0.50249934]
True reward weights: [0.26482879 0.05030782 0.96298226]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123904

Saving results to files...
Results saved successfully.

Running experiment 21/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.2938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.03292964  0.50215678  0.86414941]
True reward weights: [-0.3738588  -0.3262014   0.86822937]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123924

Running PBIRL with 2 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.2956
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.90154507 -0.07774288  0.42564366]
True reward weights: [-0.99212435 -0.08975735  0.08736641]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126606

Running PBIRL with 3 demonstrations for experiment 21
MCMC completed with acceptance ratio: 0.2926
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.61473649 -0.00789976  0.788693  ]
True reward weights: [-0.89170629 -0.44491355  0.08313741]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127580

Running experiment 22/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.2858
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.71023603  0.21877384  0.66910596]
True reward weights: [-0.70965126 -0.6051058   0.36089066]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123972

Running PBIRL with 2 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.2966
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.22545056  0.36814908  0.90201901]
True reward weights: [-0.87121099 -0.47640978  0.11842777]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126325

Running PBIRL with 3 demonstrations for experiment 22
MCMC completed with acceptance ratio: 0.2870
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.457967    0.10001525  0.88332507]
True reward weights: [-0.75440945 -0.60588756 -0.25252061]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.128780

Saving results to files...
Results saved successfully.

Running experiment 23/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.4784
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.15181307 -0.24719802  0.9569984 ]
True reward weights: [-0.74528522 -0.55533186  0.36899385]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.4778
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.04628572 -0.91378068  0.40356227]
True reward weights: [0.56935696 0.00975602 0.82203253]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123731

Running PBIRL with 3 demonstrations for experiment 23
MCMC completed with acceptance ratio: 0.4880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.11247426 -0.99261584 -0.04542396]
True reward weights: [0.04629404 0.6560652  0.75328303]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123719

Running experiment 24/50...
Same Demos: [(2, 1), (2, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.4762
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.4123737   0.09344881  0.90620928]
True reward weights: [-0.68144434 -0.29187893  0.6711485 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.4828
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.52980112 -0.79353991 -0.29934126]
True reward weights: [-0.68652987  0.06602489  0.72409768]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123781

Running PBIRL with 3 demonstrations for experiment 24
MCMC completed with acceptance ratio: 0.4794
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.41952826 -0.36293153  0.8320317 ]
True reward weights: [-0.86834701 -0.45523545 -0.19680997]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124172

Saving results to files...
Results saved successfully.

Running experiment 25/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.4754
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.63918174  0.29949735  0.70834175]
True reward weights: [-0.44714936 -0.30119858  0.84222139]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.4848
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.28648187 -0.94035073 -0.18349015]
True reward weights: [-0.60560444 -0.27059732  0.74834508]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123770

Running PBIRL with 3 demonstrations for experiment 25
MCMC completed with acceptance ratio: 0.4798
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.89099052 -0.44851532  0.07049751]
True reward weights: [-0.47048317  0.41455679  0.77896615]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123747

Running experiment 26/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.3082
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.87237011 -0.30001582  0.38595452]
True reward weights: [-0.7333898  -0.47830978  0.48307263]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123949

Running PBIRL with 2 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.2896
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.85988902  0.14177201  0.49039939]
True reward weights: [-0.82388781 -0.43259398  0.36615752]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.125972

Running PBIRL with 3 demonstrations for experiment 26
MCMC completed with acceptance ratio: 0.2918
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.95453771 -0.25079492  0.16112007]
True reward weights: [-0.58811538  0.0705776   0.80569169]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.128016

Saving results to files...
Results saved successfully.

Running experiment 27/50...
Same Demos: [(2, 1), (2, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.4780
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.02388083 -0.96821253  0.24898635]
True reward weights: [-0.65557811 -0.01374154  0.75500233]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.4808
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.48354343 -0.76082918  0.43281024]
True reward weights: [-0.84884961 -0.51244127 -0.12983945]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123658

Running PBIRL with 3 demonstrations for experiment 27
MCMC completed with acceptance ratio: 0.4880
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.84850981 -0.46270438  0.25677959]
True reward weights: [-0.61912348 -0.63399661  0.46339445]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123905

Running experiment 28/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.3066
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.92745082 -0.3376745   0.16065772]
True reward weights: [-0.866301   -0.34967581  0.35672034]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124036

Running PBIRL with 2 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.2962
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.88359936 -0.01111037  0.46811189]
True reward weights: [-0.90930949 -0.21456552  0.35653596]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.127790

Running PBIRL with 3 demonstrations for experiment 28
MCMC completed with acceptance ratio: 0.3046
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.17586954  0.4931912   0.85195795]
True reward weights: [-0.71514676  0.3954521   0.57635297]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127892

Saving results to files...
Results saved successfully.

Running experiment 29/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.2954
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.74070028  0.09426962  0.66518894]
True reward weights: [-0.6535586  -0.23072892  0.72085041]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123785

Running PBIRL with 2 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.2952
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.83591983 -0.04273339  0.54718543]
True reward weights: [-0.92028465  0.16812273  0.35328589]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.125158

Running PBIRL with 3 demonstrations for experiment 29
MCMC completed with acceptance ratio: 0.2932
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.80527822  0.00401151  0.59288354]
True reward weights: [-0.87034053  0.00303309  0.49244102]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127358

Running experiment 30/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.4878
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.23014681 -0.54939676  0.80324072]
True reward weights: [-0.81326386 -0.04441834  0.5801973 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.4842
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.77238516 -0.1003672   0.62717429]
True reward weights: [-0.63144194  0.52762283  0.5682387 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123563

Running PBIRL with 3 demonstrations for experiment 30
MCMC completed with acceptance ratio: 0.4798
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.37781112 -0.42375548  0.82321932]
True reward weights: [-0.24544658 -0.40231383  0.88198614]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123445

Saving results to files...
Results saved successfully.

Running experiment 31/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.2996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.95355059 -0.20866304  0.21725792]
True reward weights: [-0.27534761 -0.19938474  0.94044108]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124084

Running PBIRL with 2 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.2992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.80874045 -0.00148012  0.58816384]
True reward weights: [-0.88193719 -0.45861356 -0.10890544]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.127953

Running PBIRL with 3 demonstrations for experiment 31
MCMC completed with acceptance ratio: 0.2950
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.93941411 -0.05400402  0.33850362]
True reward weights: [0.3670542  0.51430502 0.7750881 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.128269

Running experiment 32/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.2984
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.17738923  0.4807484   0.85872815]
True reward weights: [-0.90753571 -0.40234227  0.1204144 ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123951

Running PBIRL with 2 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.2894
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.89843046 -0.04416529  0.43688917]
True reward weights: [-0.77276957  0.39536998  0.4964975 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.127370

Running PBIRL with 3 demonstrations for experiment 32
MCMC completed with acceptance ratio: 0.2962
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.65655733  0.09693219  0.7480218 ]
True reward weights: [0.33668953 0.58240921 0.73989166]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127558

Saving results to files...
Results saved successfully.

Running experiment 33/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.4814
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.2526688  -0.96168322 -0.10641363]
True reward weights: [-0.32556312 -0.30396656  0.89532842]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.4734
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.25170896 -0.89230034  0.37475686]
True reward weights: [-0.68404479 -0.54737113  0.4821489 ]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123745

Running PBIRL with 3 demonstrations for experiment 33
MCMC completed with acceptance ratio: 0.4830
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.62593542 0.30817942 0.71640093]
True reward weights: [ 0.57729888 -0.30924995  0.75570528]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123896

Running experiment 34/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.4942
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.00718403 -0.73891165  0.67376403]
True reward weights: [-0.839382   -0.393236    0.37523766]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123664

Running PBIRL with 2 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.4802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.30989373 -0.44249027  0.84152732]
True reward weights: [0.4851156  0.25685645 0.83587536]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123708

Running PBIRL with 3 demonstrations for experiment 34
MCMC completed with acceptance ratio: 0.4838
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.29324795 -0.65412909  0.69722361]
True reward weights: [-0.38656512 -0.59122379  0.70782896]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123478

Saving results to files...
Results saved successfully.

Running experiment 35/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.4764
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.46403161 -0.80399142 -0.37185005]
True reward weights: [-0.94121751 -0.05112011  0.33391067]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123667

Running PBIRL with 2 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.4754
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [0.27377493 0.10457973 0.95609119]
True reward weights: [-0.95717787 -0.02944119  0.28799954]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124061

Running PBIRL with 3 demonstrations for experiment 35
MCMC completed with acceptance ratio: 0.4890
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [0.55808622 0.0639528  0.82731482]
True reward weights: [-0.64809023 -0.6010385   0.4676877 ]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124418

Running experiment 36/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.4816
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.54986966 -0.537624    0.63922124]
True reward weights: [-0.05456508 -0.040557    0.99768621]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.4782
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [ 0.66060922 -0.08956653  0.7453679 ]
True reward weights: [ 0.48559181 -0.26285718  0.83373059]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123783

Running PBIRL with 3 demonstrations for experiment 36
MCMC completed with acceptance ratio: 0.4848
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.81151238 -0.56486142  0.14959689]
True reward weights: [-0.49452879 -0.84594178 -0.19955897]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123786

Saving results to files...
Results saved successfully.

Running experiment 37/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.2930
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.84624845 -0.10496265  0.52234701]
True reward weights: [-0.75478167 -0.33513042  0.563908  ]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124047

Running PBIRL with 2 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.2996
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.17945331  0.35254765  0.91842619]
True reward weights: [-0.83493155 -0.42482474  0.34987605]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.128028

Running PBIRL with 3 demonstrations for experiment 37
MCMC completed with acceptance ratio: 0.2998
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.85852789  0.09773084  0.50336721]
True reward weights: [-0.98422129 -0.17184637 -0.04215786]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127881

Running experiment 38/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.2962
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.76159629 -0.12735466  0.63541473]
True reward weights: [-0.49248773 -0.22865786  0.83974486]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123843

Running PBIRL with 2 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.2934
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.13858945  0.44304771  0.885721  ]
True reward weights: [-0.97019463 -0.23868472  0.04185662]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.124949

Running PBIRL with 3 demonstrations for experiment 38
MCMC completed with acceptance ratio: 0.2938
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.95290285 -0.26499449  0.14749261]
True reward weights: [-0.21914696  0.10087694  0.97046301]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.128416

Saving results to files...
Results saved successfully.

Running experiment 39/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.4802
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.59464686 -0.75345734  0.28053014]
True reward weights: [-0.56481614 -0.46100063  0.68444222]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.4776
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.00746244  0.22903828  0.97338881]
True reward weights: [ 0.39419546 -0.7388938   0.54648503]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123746

Running PBIRL with 3 demonstrations for experiment 39
MCMC completed with acceptance ratio: 0.4672
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.01584857 -0.09858273  0.99500265]
True reward weights: [ 0.06581919 -0.22521864  0.97208251]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123885

Running experiment 40/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.2986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.31149257  0.40007638  0.86192301]
True reward weights: [-0.6667631  -0.57474284  0.47444455]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124078

Running PBIRL with 2 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.2970
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.56877837  0.24355909  0.78560177]
True reward weights: [-0.83741859 -0.53000567 -0.13350694]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.128373

Running PBIRL with 3 demonstrations for experiment 40
MCMC completed with acceptance ratio: 0.2950
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.925898   -0.09709439  0.36508296]
True reward weights: [-0.80694311  0.15476279  0.56999236]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.128400

Saving results to files...
Results saved successfully.

Running experiment 41/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.4770
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.78671425 -0.60614161  0.11693175]
True reward weights: [-0.46247666 -0.0267168   0.88622884]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.4814
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.18443793  0.37081285  0.91020903]
True reward weights: [-0.14199919 -0.90356471  0.40423637]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123840

Running PBIRL with 3 demonstrations for experiment 41
MCMC completed with acceptance ratio: 0.4840
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.79019866  0.02708893  0.6122518 ]
True reward weights: [0.39221871 0.38325263 0.83623078]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123695

Running experiment 42/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.2904
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.41028476  0.43071583  0.80383474]
True reward weights: [-0.71505276 -0.68463496  0.14133129]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.124161

Running PBIRL with 2 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.2986
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.20827994  0.38008617  0.90119586]
True reward weights: [-0.63965818 -0.10355527  0.76165197]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.128785

Running PBIRL with 3 demonstrations for experiment 42
MCMC completed with acceptance ratio: 0.3000
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.9626419  -0.13244282  0.23617678]
True reward weights: [0.01304433 0.48505399 0.87438691]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.129648

Saving results to files...
Results saved successfully.

Running experiment 43/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.4748
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.44882075 -0.74937506  0.48682333]
True reward weights: [-0.94655708 -0.29794542  0.12352418]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123666

Running PBIRL with 2 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.4872
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.04409781 -0.76490206  0.64263537]
True reward weights: [-0.76631316  0.21332077  0.60601848]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123918

Running PBIRL with 3 demonstrations for experiment 43
MCMC completed with acceptance ratio: 0.4798
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.89301066 -0.20404335  0.40112127]
True reward weights: [-0.57939505 -0.17034871  0.79704623]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.124092

Running experiment 44/50...
Same Demos: [(2, 1), (2, 1), (2, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.4906
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [ 0.36089691 -0.19110762  0.91281504]
True reward weights: [-0.88315502 -0.32822573  0.33511952]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.4738
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.26848009 -0.76904907  0.58007065]
True reward weights: [-0.86544636  0.30047057  0.40089903]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123623

Running PBIRL with 3 demonstrations for experiment 44
MCMC completed with acceptance ratio: 0.4820
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.35258918 -0.91666706 -0.18815517]
True reward weights: [-0.6342417  -0.42506401  0.64580033]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123909

Saving results to files...
Results saved successfully.

Running experiment 45/50...
Same Demos: [(4, 3), (4, 3), (4, 3)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.4782
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.26616579 -0.95800746  0.1066653 ]
True reward weights: [-0.83381491 -0.37890594  0.40147601]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123665

Running PBIRL with 2 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.4756
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.13194188 -0.26560289  0.95501123]
True reward weights: [-0.30854001 -0.94332323  0.12224706]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.123584

Running PBIRL with 3 demonstrations for experiment 45
MCMC completed with acceptance ratio: 0.4890
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [ 0.24235333 -0.04049916  0.9693424 ]
True reward weights: [ 0.31518087 -0.28341308  0.90572515]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.123986

Running experiment 46/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.2992
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 1
MAP Solution: [-0.95315438 -0.07013383  0.29424134]
True reward weights: [-0.82098277 -0.18440172  0.54035479]
MAP Policy for current environment:
Information gain 1 demonstrations: 0.123934

Running PBIRL with 2 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.2926
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 2
MAP Solution: [-0.8944506  -0.03946997  0.44542143]
True reward weights: [ 0.65369981  0.73897735 -0.16306145]
MAP Policy for current environment:
Information gain 2 demonstrations: 0.126913

Running PBIRL with 3 demonstrations for experiment 46
MCMC completed with acceptance ratio: 0.2990
Using 3250 samples after burn-in.
Stored 3250 MCMC samples for demonstration 3
MAP Solution: [-0.78180025  0.02628192  0.62297482]
True reward weights: [-0.83561942  0.14609946  0.52952349]
MAP Policy for current environment:
Information gain 3 demonstrations: 0.127635

Saving results to files...
Results saved successfully.

Running experiment 47/50...
Same Demos: [(0, 1), (0, 1), (0, 1)]
Maximum entropy: 8.0864

Running PBIRL with 1 demonstrations for experiment 47
